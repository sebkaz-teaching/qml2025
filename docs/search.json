[
  {
    "objectID": "lectures/wyklad5.html",
    "href": "lectures/wyklad5.html",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "",
    "text": "DziedzinÄ… Å‚Ä…czÄ…cÄ… klasyczne uczenie maszynowe i obliczenia kwantowe nazywamy kwantowym uczeniem maszynowym (ang. Quantum Machine Learning).\nQML powstaÅ‚o aby szybciej i sprawniej rozwiÄ…zywaÄ‡ problemu uczenia maszynowego. Do tego celu chcemy wykorzystaÄ‡ procesory kwantowe oraz wÅ‚asnoÅ›ci algorytmÃ³w kwantowych i ich przewagÄ™ nad klasycznymi odpowiednikami.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#pqc-parametrized-quantum-circuits",
    "href": "lectures/wyklad5.html#pqc-parametrized-quantum-circuits",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "PQC â€“ Parametrized Quantum Circuits",
    "text": "PQC â€“ Parametrized Quantum Circuits\nParametryzowane obwody kwantowe (PQC) to podstawowy element wspÃ³Å‚czesnych algorytmÃ³w hybrydowych. Ich dziaÅ‚anie polega na tym, Å¼e stan kwantowy (czyli ukÅ‚ad kubitÃ³w) jest modyfikowany za pomocÄ… bramek kwantowych zaleÅ¼nych od zestawu parametrÃ³w klasycznych â€” np. kÄ…tÃ³w rotacji. ZmieniajÄ…c te parametry, moÅ¼emy dostosowaÄ‡ stan wyjÅ›ciowy obwodu do konkretnego zadania, np. minimalizacji energii lub klasyfikacji danych.\nPQC peÅ‚niÄ… podobnÄ… rolÄ™ jak funkcje z parametrami w klasycznym ML â€” np. wagi w sieciach neuronowych.\nWarto zapamiÄ™taÄ‡, Å¼e nie szukamy tutaj pojedynczego wyniku pomiaru, ale statystycznej wÅ‚aÅ›ciwoÅ›ci stanu kwantowego zaleÅ¼nej od parametrÃ³w.\n\nRX\nRX to parametryzowana jedno-kubitowa bramka realizujÄ…ca obroty w osi X na sferze blocha o kÄ…t \\(\\theta\\).\nUnitarna realizacja:\n\\[\nRX(\\theta) = \\exp\\left(-i \\frac{\\theta}{2} X\\right) =\n\\begin{bmatrix}\n\\cos(\\frac{\\theta}{2}) & -i \\sin(\\frac{\\theta}{2}) \\\\\n-i \\sin(\\frac{\\theta}{2}) & \\cos(\\frac{\\theta}{2})\n\\end{bmatrix}\n\\]\nDla \\(\\theta = \\pi\\), bramka ta realizuje siÄ™ jako bramka X.\n\n\nRZ\nRZ to parametryzowana jedno-kubitowa bramka realizujÄ…ca obroty w osi Z na sferze blocha o kÄ…t \\(\\theta\\).\nUnitarna realizacja:\n\\[\nRZ(\\theta) = \\exp\\left(-i \\frac{\\theta}{2} Z\\right) =\n\\begin{bmatrix}\ne^{-i\\theta/2} & 0 \\\\\n0 & e^{i\\theta/2}\n\\end{bmatrix}\n\\]\n\n\nRY\nRY to parametryzowana jedno-kubitowa bramka realizujÄ…ca obroty w osi Y na sferze blocha o kÄ…t \\(\\theta\\).\nUnitarna realizacja:\n\\[\nRY(\\theta) = \\exp\\left(-i \\frac{\\theta}{2} Y\\right) =\n\\begin{bmatrix}\n\\cos(\\frac{\\theta}{2}) & - \\sin(\\frac{\\theta}{2}) \\\\\n\\sin(\\frac{\\theta}{2}) & \\cos(\\frac{\\theta}{2})\n\\end{bmatrix}\n\\]\n\n\nğŸŒ€ Dowolny obrÃ³t kubitu\nOperator obrotu dla pojedynczego kubitu moÅ¼na zapisaÄ‡ jako:\n\\[\nR(\\phi, \\theta, \\omega) = R_Z(\\omega)  R_Y(\\theta)  R_Z(\\phi)\n\\]\nW postaci macierzowej operator ten przyjmuje formÄ™:\n\\[\nR(\\phi, \\theta, \\omega) =\n\\begin{bmatrix}\ne^{-i(\\phi+\\omega)/2}\\cos(\\theta/2) & -e^{-i(\\phi-\\omega)/2}\\sin(\\theta/2) \\\\\ne^{i(\\phi-\\omega)/2}\\sin(\\theta/2) & e^{i(\\phi+\\omega)/2}\\cos(\\theta/2)\n\\end{bmatrix}\n\\]\n\nğŸ“‹ SzczegÃ³Å‚y\n\nLiczba kubitÃ³w (wires): 1\nLiczba parametrÃ³w: 3\n\n\n\nğŸ§® Przepis na gradient\nPochodna funkcji \\(f\\), ktÃ³ra zaleÅ¼y od operatora \\(R(\\phi, \\theta, \\omega)\\), wzglÄ™dem parametru \\(\\phi\\) jest dana wzorem:\n\\[\n\\frac{d}{d\\phi} f(R(\\phi, \\theta, \\omega)) =\n\\frac{1}{2}\\Big[f(R(\\phi + \\pi/2, \\theta, \\omega)) - f(R(\\phi - \\pi/2, \\theta, \\omega))\\Big]\n\\]\ngdzie \\(f\\) jest wartoÅ›ciÄ… oczekiwanÄ… (expectation value) zaleÅ¼nÄ… od operatora \\(R(\\phi, \\theta, \\omega)\\).\nTen sam przepis na gradient moÅ¼na zastosowaÄ‡ dla kaÅ¼dego z kÄ…tÃ³w: \\(\\phi, \\theta, \\omega\\).\n\n\nğŸ’¡ Komentarz:\nOperator \\(R(\\phi, \\theta, \\omega)\\) opisuje dowolny obrÃ³t pojedynczego kubitu na sferze Blocha. Sekwencja trzech rotacji wokÃ³Å‚ osi Z, Y i ponownie Z pozwala uzyskaÄ‡ dowolny stan kwantowy z bazy obliczeniowej â€” dlatego ten operator jest fundamentem wielu obwodÃ³w kwantowych i bramek parametryzowanych (PQC).\nWiemy juz jak skÅ‚adaÄ‡ bramki w celu utworzenia dowolnego (i o dowolnej gÅ‚Ä™bokoÅ›ci) obwodu. \\[ \\ket{\\psi'} = U_m(\\theta_m)\\dots U_2(\\theta_2) U_1(\\theta_1) \\ket{\\psi} \\]\nCzÄ™Å›Ä‡ indywidualnych bramek (ze zbioru \\((U_i)_{i=1,\\dots,m}\\)) moze byÄ‡ ustalona np. \\(X\\), \\(CNOT\\), czyli ich parametry sÄ… Å›ciÅ›le okreÅ›lone (np. \\(\\pi\\)).\nJednak czÄ™Å›Ä‡ bramek moze zalezeÄ‡ od parametrÃ³w obrotÃ³w wyrazonych najczÄ™Å›ciej jako radiany (w zakresi \\(\\theta \\in [-\\pi, \\pi]\\)).\nPo przygotowaniu stanu \\(\\ket{\\psi'}\\) mozemy zmierzyÄ‡ jeden lub caÅ‚y zestaw kubitÃ³w. Po pomiarze kubity zostajÄ… w stanie bazowym zgodnie z wykorzystanym operatorem. NajczÄ™Å›ciej wybieramy bazÄ™ obliczeniowÄ… pozwalajÄ…cÄ… uzyskaÄ‡ rezultat jako listÄ™ bitÃ³w.\nTak zdefiniowany i dziaÅ‚ajÄ…cy obwÃ³d kwantowy mozna wykorzystaÄ‡ do wielu rzeczy. Dla nas najwazniejszym aspektem jest mozliwoÅ›Ä‡ trenowania parametrÃ³w obwodu.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#kodowanie-danych",
    "href": "lectures/wyklad5.html#kodowanie-danych",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "Kodowanie danych",
    "text": "Kodowanie danych\nğŸ§  Klasyczne potoki przetwarzania cech (Classical Feature Pipelines) 1. Czyszczenie danych: obsÅ‚uga brakujÄ…cych wartoÅ›ci, normalizacja skali. 2. Kodowanie: konwersja kategorii lub tekstu na wektory liczbowe. 3. Skalowanie i Å‚Ä…czenie: skalowanie cech, tworzenie wielomianowych kombinacji lub par interakcyjnych.\n\nâš›ï¸ Kwantowe mapy cech (Quantum Feature Maps)\n\nBasis Encoding: mapowanie binarnych cech bezpoÅ›rednio na stany bazowe kubitÃ³w w bazie obliczeniowej.\nAmplitude Encoding: zakodowanie caÅ‚ego wektora danych w amplitudach n kubitÃ³w.\nAngle Encoding: uÅ¼ycie bramek rotacji \\(R_X(x_i)\\), \\(R_Y(x_i)\\), \\(R_Z(x_i)\\) do zakodowania kaÅ¼dej cechy w fazie kubitu.\n\n\n\nâš™ï¸ Kodowanie w bazie obliczeniowej (Basis Embedding / Encoding)\nOdpowiednim sposobem kodowania danych binarnych jest tzw. â€Basis Embeddingâ€ (kodowanie w bazie obliczeniowej).\nKlasa BasisEmbedding interpretuje ciÄ…g binarny jako stan bazowy kubitÃ³w zgodnie z odwzorowaniem: \\[\nb = (b_0, \\dots, b_{N-1}) ;\\to; \\ket{b_0, \\dots, b_{N-1}}\n\\]\nZaÅ‚Ã³Å¼my, Å¼e nasze cechy (features) zapisane sÄ… jako stan \\[\n\\ket{111} = \\ket{1} \\otimes \\ket{1} \\otimes \\ket{1}.\n\\]\nReprezentujÄ…c stan jako drugi wektor bazowy (standard basis vector), a iloczyn tensorowy jako iloczyn Kroneckera, moÅ¼emy Å‚atwo potwierdziÄ‡ wynik prostym obliczeniem:\n\\[\n\\ket{1} \\otimes \\ket{1} \\otimes \\ket{1} = [0, 0, 0, 0, 0, 0, 0, 1]^T\n\\]\n\n\nâš›ï¸ Kodowanie w amplitudach (Amplitude Embedding / Encoding)\nJak sama nazwa wskazuje, tablica wartoÅ›ci moÅ¼e zostaÄ‡ uÅ¼yta jako amplitudy stanu kwantowego, zgodnie z odwzorowaniem: \\[\n\\alpha = (\\alpha_0, \\dots, \\alpha_{2^N-1}) \\to \\sum_{k=0}^{2^N-1} \\alpha_k \\ket{k}\n\\]\nW ten sposÃ³b kaÅ¼da wartoÅ›Ä‡ w wektorze danych klasycznych odpowiada amplitudzie jednego ze stanÃ³w bazowych ukÅ‚adu kwantowego. Aby stan byÅ‚ fizycznie poprawny, wektor amplitud musi byÄ‡ znormalizowany (tj. suma kwadratÃ³w moduÅ‚Ã³w amplitud rÃ³wna 1).\n\n\nğŸ”„ Kodowanie w kÄ…tach (Angle Embedding / Encoding)\nNajprostszym sposobem kodowania danych rzeczywistych (wartoÅ›ci zmiennoprzecinkowych) jest tzw. â€Angle Embeddingâ€ (kodowanie w kÄ…tach).\nTen rodzaj kodowania przypisuje pojedynczej wartoÅ›ci rzeczywistej x stan kwantowy wedÅ‚ug odwzorowania: \\[\nx \\to R_k(x)\\ket{0} = e^{-i x \\sigma_k /2} \\ket{0}\n\\] gdzie \\(k \\in \\{x, y, z\\}\\) oznacza oÅ› obrotu na sferze Blocha.\nDomyÅ›lnie w klasie AngleEmbedding oÅ› obrotu ustawiona jest na k = x. MoÅ¼na rÃ³wnieÅ¼ wybraÄ‡ k = y, ale naleÅ¼y unikaÄ‡ k = z, poniewaÅ¼ obrÃ³t wokÃ³Å‚ osi Z nie zmienia amplitud stanu, a jedynie fazÄ™ globalnÄ….\nâ¸»\n\n\nğŸ”¸ Uwaga:\nRotacje Pauliego sÄ… okresowe z okresem \\(2\\pi\\) (z dokÅ‚adnoÅ›ciÄ… do fazy globalnej).\nOznacza to, Å¼e dane wejÅ›ciowe warto znormalizowaÄ‡ do przedziaÅ‚u \\([0, \\pi)\\), jeÅ›li to moÅ¼liwe, aby uniknÄ…Ä‡ wieloznacznoÅ›ci reprezentacji stanu.\n\n\nğŸ”¹ Reprezentacja kwantowa danych (Quantum Embedding)\nReprezentacja kwantowa (quantum embedding) przeksztaÅ‚ca dane klasyczne w stany kwantowe w przestrzeni Hilberta za pomocÄ… kwantowej mapy cech (feature map).\nProces ten polega na wziÄ™ciu klasycznego punktu danych \\(x\\) i przetÅ‚umaczeniu go na zestaw parametrÃ³w bramek w obwodzie kwantowym, tworzÄ…c stan:\n\\[\n\\ket{\\psi(x)}\n\\]\nTen etap jest kluczowy przy projektowaniu algorytmÃ³w kwantowych, poniewaÅ¼ sposÃ³b zakodowania danych bezpoÅ›rednio wpÅ‚ywa na moc obliczeniowÄ… i zdolnoÅ›Ä‡ do wykrywania wzorcÃ³w w danych.\n\n\nSchemat kodowania\nRozwazmy N wierszy 8 zmiennych \\(X_1\\dots X_8\\) o wartoÅ›ciach rzeczywistych.\nPotrzebujemy okreÅ›liÄ‡ \\(X_i^{max}\\) oraz \\(X_i^{min}\\).\n\\(\\theta^j_i = \\frac{X^j_i - X_i^{min}}{X_i^{max}-X_i^{min}} \\pi\\)\nKorzystajÄ…c z bramki \\(R_y\\) mozemy zakodowaÄ‡ kazdy kubit z osobnym kÄ…tem. Mozna takze wybraÄ‡ dwie bramki z dwoma kÄ…tami dla jednego kubitu (\\(R_y\\) i \\(R_z\\))",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#kwantowe-algorytmy-wariacyjne",
    "href": "lectures/wyklad5.html#kwantowe-algorytmy-wariacyjne",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "Kwantowe Algorytmy Wariacyjne",
    "text": "Kwantowe Algorytmy Wariacyjne\n\nVariational Quantum Algorithms naleÅ¼Ä… do klasy algorytmÃ³w hybrydowych, w ktÃ³rych czÄ™Å›Ä‡ obliczeÅ„ wykonuje komputer kwantowy, a czÄ™Å›Ä‡ klasyczny procesor. Ich celem jest optymalizacja zestawu parametrÃ³w, ktÃ³re minimalizujÄ… (lub maksymalizujÄ…) pewnÄ… funkcjÄ™ kosztu. Proces polega na naprzemiennym uruchamianiu obwodu kwantowego z rÃ³Å¼nymi wartoÅ›ciami parametrÃ³w i wykorzystaniu klasycznego algorytmu (np. gradient descent) do poprawy tych parametrÃ³w.\nğŸ‘‰ Schemat dziaÅ‚ania:\n\nZdefiniuj obwÃ³d kwantowy zaleÅ¼ny od parametrÃ³w (PQC).\nZmierz jego wynik i policz wartoÅ›Ä‡ funkcji kosztu.\nKlasyczny optymalizator aktualizuje parametry.\nPowtarzaj, aÅ¼ osiÄ…gniesz minimum.\n\nTen model jest fundamentem takich algorytmÃ³w jak VQE czy QAOA.\nVQE to jeden z najwaÅ¼niejszych przykÅ‚adÃ³w algorytmu wariacyjnego. SÅ‚uÅ¼y do znajdowania najniÅ¼szej energii (wartoÅ›ci wÅ‚asnej) dla danego operatora Hamiltona â€” kluczowego w chemii kwantowej i fizyce czÄ…stek.\nğŸ‘‰ VQE Å‚Ä…czy dwa Å›wiaty: - CzÄ™Å›Ä‡ kwantowa: przygotowuje stan kwantowy za pomocÄ… obwodu parametryzowanego. - CzÄ™Å›Ä‡ klasyczna: minimalizuje Å›redniÄ… wartoÅ›Ä‡ energii poprzez optymalizacjÄ™ parametrÃ³w.\nW praktyce pozwala to badaÄ‡ ukÅ‚ady molekularne czy materiaÅ‚y bez potrzeby uÅ¼ycia peÅ‚nego, kosztownego symulatora kwantowego.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#quantum-support-vector-classification-qsvm",
    "href": "lectures/wyklad5.html#quantum-support-vector-classification-qsvm",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "Quantum Support Vector Classification (qSVM)",
    "text": "Quantum Support Vector Classification (qSVM)\nKlasyczny algorytm SVM (Support Vector Machine) znajduje hiperpÅ‚aszczyznÄ™ najlepiej rozdzielajÄ…cÄ… dane dwÃ³ch klas. W wersji kwantowej dane sÄ… najpierw kodowane w stanach kwantowych (np. przez amplitude encoding), a nastÄ™pnie mierzona jest odlegÅ‚oÅ›Ä‡ lub podobieÅ„stwo miÄ™dzy nimi w przestrzeni Hilberta.\nğŸ‘‰ W kwantowym SVM:\n\ndane wejÅ›ciowe â†’ przeksztaÅ‚cane sÄ… w stany kwantowe,\njÄ…dro (kernel) â†’ jest obliczane poprzez pomiary kwantowe,\nklasyfikacja â†’ odbywa siÄ™ klasycznie, ale korzysta z â€kwantowej przestrzeni cechâ€.\n\nKwantowe jÄ…dra mogÄ… umoÅ¼liwiaÄ‡ separacjÄ™ danych nieliniowo rozdzielnych znacznie efektywniej niÅ¼ klasyczne jÄ…dra.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#quantum-neural-networks-qnn",
    "href": "lectures/wyklad5.html#quantum-neural-networks-qnn",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "Quantum Neural Networks (QNN)",
    "text": "Quantum Neural Networks (QNN)\nKwantowe sieci neuronowe to prÃ³ba przeniesienia idei klasycznych sieci (warstw, wag i funkcji aktywacji) na grunt obwodÃ³w kwantowych. Zamiast klasycznych neuronÃ³w, wykorzystuje siÄ™ bramki kwantowe sterowane parametrami, ktÃ³re peÅ‚niÄ… funkcjÄ™ transformacji danych. Proces trenowania QNN przypomina uczenie klasyczne â€” minimalizujemy funkcjÄ™ bÅ‚Ä™du poprzez modyfikacjÄ™ parametrÃ³w obwodu.\n\nğŸ‘‰ GÅ‚Ã³wne cechy QNN:\n\nWagi sieci odpowiadajÄ… parametrom rotacji w bramkach kwantowych.\nFunkcja aktywacji realizowana jest przez pomiar i ponowne przygotowanie stanu.\nDziÄ™ki superpozycji, jedna warstwa moÅ¼e reprezentowaÄ‡ bardzo zÅ‚oÅ¼one zaleÅ¼noÅ›ci danych.\n\nW praktyce QNN sÄ… obecnie w fazie badaÅ„, ale mogÄ… stanowiÄ‡ podstawÄ™ przyszÅ‚ych modeli kwantowych o zdolnoÅ›ci uczenia porÃ³wnywalnej z sieciami klasycznymi.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#ansatz-i-schematy-modelowe",
    "href": "lectures/wyklad5.html#ansatz-i-schematy-modelowe",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "Ansatz i schematy modelowe",
    "text": "Ansatz i schematy modelowe\nParametryzowany kwantowy obwÃ³d wariacyjny reprezentowany przez operator \\(W_\\theta\\) zaleÅ¼ny od wektora parametrÃ³w \\(\\theta\\), ktÃ³ry dziaÅ‚a na wektor stanu przygotowany przez obwÃ³d kodujÄ…cy dane \\(W_\\theta \\ket{\\phi}\\), gdzie \\(\\ket{\\phi}\\) przygotowaliÅ›my za pomocÄ… kodowania danych. Ze wzglÄ™du na bardzo duÅ¼Ä… iloÅ›Ä‡ kombinacji bramek, ktÃ³re moÅ¼na Å‚Ä…czyÄ‡ rÃ³wnolegle i szeregowo trudno jest wskazaÄ‡ jeden konkretny obwÃ³d pozwalajÄ…cy realizowaÄ‡ wszystkie problemy analityczne (nie ma darmowych obiadÃ³w). WybÃ³r odbywa siÄ™ najczÄ™Å›ciej poprzez ustalenie rÃ³Å¼nych schematÃ³w i ich trenowania. PorÃ³wnaÄ‡ moÅ¼na to do klasycznych sieci neuronowych, gdzie iloÅ›Ä‡ warstw ukrytych, liczba neuronÃ³w w kaÅ¼dej warstwie czy funkcje aktywacji sÄ… tzw. hiper-parametrami, ktÃ³re ustala siÄ™ przez doÅ›wiadczenie. Wybrany schemat obwodÃ³w kwantowych okreÅ›la siÄ™ czÄ™sto mianem ansatzu.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad5.html#pomiar-i-interpretacja-wynikÃ³w",
    "href": "lectures/wyklad5.html#pomiar-i-interpretacja-wynikÃ³w",
    "title": "Algorytmy kwantowego uczenia maszynowego",
    "section": "Pomiar i interpretacja wynikÃ³w",
    "text": "Pomiar i interpretacja wynikÃ³w\nNa tym etapie najczÄ™Å›ciej estymuje siÄ™ zbiÃ³r wartoÅ›ci oczekiwanych przyjmujÄ…cy wartoÅ›ci od -1 do 1.\nW zaleÅ¼noÅ›ci od realizowanego procesu moÅ¼na wykonaÄ‡ pomiar jednego lub wiÄ™kszej iloÅ›ci kubitÃ³w. MoÅ¼na rÃ³wnieÅ¼ generowaÄ‡ wynik w postaci amplitud, prawdopodobieÅ„stw czy teÅ¼ wyniku w postaci binarnej. Etap ten czÄ™sto nazywany jest postprocesingiem danych.\n\nInny sposÃ³b pomiaru",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Algorytmy kwantowego uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html",
    "href": "lectures/wyklad1.html",
    "title": "Wprowadzenie do obliczeÅ„ kwantowych",
    "section": "",
    "text": "Nature isnâ€™t classical, dammit, and if you want to make a simulation of Nature, youâ€™d better make it quantum mechanical, and by golly itâ€™s a wonderful problem because it doesnâ€™t look so easy.\nRichard Feynman\nCelem tego wykÅ‚adu jest zrozumienie, czym sÄ…:\nQuntum Machine Learning",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Wprowadzenie do obliczeÅ„ kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#uczenie-maszynowe",
    "href": "lectures/wyklad1.html#uczenie-maszynowe",
    "title": "Wprowadzenie do obliczeÅ„ kwantowych",
    "section": "Uczenie maszynowe",
    "text": "Uczenie maszynowe\nUczenie maszynowe (ale rÃ³wnieÅ¼ AI i uczenie gÅ‚Ä™bokie) to nauka i ,,sztukaâ€™â€™ opisujÄ…ca jak sprawiÄ‡ by komputery mogÅ‚y ,,uczyÄ‡ siÄ™â€™â€™ na podstawie danych, tak by rozwiÄ…zaÄ‡ problemy, ktÃ³rych typowe programowanie nie miaÅ‚oby sensu (lub byÅ‚oby zbyt skomplikowane).\n\nIn 1959, Arthur Samuel:\na field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nDane\n\n\n\nModele\n\n\nUczenie nadzorowane (ang. supervised learning) - posiadajÄ…c oznaczone dane \\((x_i, y_i)\\) szukamy funkcji \\(f(x_i) = y_i\\) tak by uogÃ³lniÄ‡ jÄ… na nowe dane. Np. daÄ‡ kredyt, czy na obrazie jest kot albo pies itp.\nUczenie nienadzorowane (ang. unsupervised learning) - posiadajÄ…c dane \\((x_i)\\) szukamy ukrytych struktur w danych.\nUczenie przez wzmacnianie (ang. reinforcement learning) - agent uczy siÄ™ realizowaÄ‡ zadania w Å›rodowisku na podstawie nagrÃ³d i kar.\n\nJednym z podstawowych celÃ³w uczenia maszynowego (a takze i gÅ‚Ä™bokiego), jest przypisanie klasy (target, labels) dla nowych, nieoznakowanych danych.\nIstniejÄ… dwa gÅ‚Ã³wne typy dla tego zadania:\n\nRegresja â€“ przewidywanie wartoÅ›ci ciÄ…gÅ‚ej,\nklasyfikacja - przewidywanie wartoÅ›ci dyskretnej.\n\n\n\nFunkcja straty\nFunkcja straty mierzy jak przewidywania modelu sÄ… oddalone od rzeczywistych wartoÅ›ci.\n\nPomaga optymalizowaÄ‡ parametry modelu przez mechanizm propagacji wstecz\nPozwala na dopasowanie modelu do danych\nMniejsza wartoÅ›Ä‡ funkcji straty = lepsza jakoÅ›Ä‡ modelu.\n\n\n\n\n\nSieci Neuronowe\n\n\nModel z parametrami do trenowania \\(f(x;\\theta)= \\sigma(Wx+b)\\) gdzie \\(\\theta= \\{W, b\\}\\)\nFunkcja kosztu \\(C = \\sum_{i} ( f(x_i, \\theta)-y_i )^2\\)\nSpadek po gradiencie\n\noblicz gradient funkcji kosztu\nzaktualizuj parametry \\(\\theta^{t+1} = \\theta^{t} - \\eta \\nabla C\\)\n\n\n\nimport torch\nfrom torch.autograd import Variable\n\ndata = torch.tensor([(0. , 1.), (0.1 , 1.1), (0.2 , 1.2)])\n\ndef model(phi, x=None):\n    return x*phi\n\ndef loss(a, b):\n    return torch.abs(a-b) ** 2\n\ndef avg_loss(phi):\n    c = 0 \n    for x, y in data:\n        c += loss(model(phi, x=x), y)\n    return c\n\nphi_ = Variable(torch.tensor(0.1), requires_grad=True)\nopt = torch.optim.Adam([phi_],lr=0.2)\n\nfor i in range(5):\n    l = avg_loss(phi_)\n    print(f\"cost: {l}, for phi: {phi_}\")\n    l.backward()\n    opt.step()\n\ncost: 3.5805001258850098, for phi: 0.10000000149011612\ncost: 3.44450044631958, for phi: 0.29999998211860657\ncost: 3.3168272972106934, for phi: 0.49334585666656494\ncost: 3.1936793327331543, for phi: 0.6854467988014221\ncost: 3.073840856552124, for phi: 0.878169059753418\n\n\n\n\nObliczenia kwantowe opisujÄ… przetwarzenie informacji na urzÄ…dzeniach pracujÄ…cych zgodnie z zasadami mechaniki kwantowej.\n\nUwaga! klasyczne komputery (tranzystory) rÃ³wnieÅ¼ dziaÅ‚ajÄ… zgodnie z zasadami mechaniki kwantowej, ale wykonywane operacje opierajÄ… siÄ™ o logikÄ™ klasycznÄ….\n\nOba kierunki sÄ… istotne w procesie przetwarzania danych obecnie i w niedalekiej przyszÅ‚oÅ›ci. Dlatego naturalnym pytaniem jest jak je ze sobÄ… poÅ‚Ä…czyÄ‡?\nQML to realizowanie metod uczenia maszynowego, ktÃ³re mogÄ… byÄ‡ wykonywane na komputerach kwantowych.\nKwantowe uczenie maszynowe moÅ¼emy okreÅ›liÄ‡ jako uczenie maszynowe realizowane na komputerach kwantowych. Zasadniczym jest pytanie na ile i czy wogÃ³le komputery kwantowe mogÄ… poprawiÄ‡ jakoÅ›Ä‡ modeli uczenia maszynowego i czy pozwalajÄ… zrealizowaÄ‡ coÅ› wiÄ™cej niÅ¼ wykorzystanie klasycznych komputerÃ³w.\n\n\nHistoria MK\nPoczÄ…tek Mechaniki Kwantowej zwiÄ…zane sÄ… z pracami Maxa Plancka (1900) i Alberta Einsteina (1905), ktÃ³rzy wprowadzili pojÄ™cie kwantu - czyli najmniejszej porcji energii. Dalszy rozwÃ³j Mechaniki Kwantowej zwiÄ…zany jest z badaniami takich naukowcÃ³w jak Niels Bohr, Erwin SchrÃ¶dinger, Louis de Broglie, Heisenberg, Dirac, Feynman i wielu innych. PozostaÅ‚e informacje moÅ¼esz znaleÅºÄ‡ w artykule o obliczeniach kwantowych\n\nInformatykÃ³w (najczÄ™Å›ciej) nie interesuje, w jaki sposÃ³b wÅ‚aÅ›ciwoÅ›ci fizyczne ukÅ‚adÃ³w sÄ… wykorzystywane do przechowywania informacji w komputerze klasycznym. Podobnie, nie muszÄ… siÄ™ zastanawiaÄ‡ nad fizycznym mechanizmem, za pomocÄ… ktÃ³rego informacja kwantowa jest realizowana w komputerze kwantowym. Czy prowadzÄ…c samochÃ³d zastanawiasz siÄ™, jak dokÅ‚adnie dziaÅ‚ajÄ… wszystkie jego czÄ™Å›ci? A piszÄ…c kod modelu, zastanawiasz siÄ™, jak zostaÅ‚ on zaimplementowany w bibliotece?â€ Informatycy czÄ™sto nie muszÄ… zagÅ‚Ä™biaÄ‡ siÄ™ w szczegÃ³Å‚y fizycznej realizacji, skupiajÄ…c siÄ™ za to na wydajnym wykorzystaniu technologii komputerowych.\n\n\n\nHistoria obliczeÅ„ kwantowych\n\n1936 Alan Turing opublikowaÅ‚ pracÄ™ On Computable Numbers, ktÃ³ra stanowiÅ‚a istotny krok w kierunku teoretycznych podstaw obliczeÅ„ (Hilbert Problems) - universal computing machine local\n1976 Roman S. Ingarden - Quantum Information Theory Roman S. Ingarden wprowadziÅ‚ pojÄ™cie teorii informacji kwantowej, co miaÅ‚o kluczowe znaczenie dla rozwoju komputerÃ³w kwantowych.\n1980 Paul Benioff - Paul Benioff przedstawiÅ‚ teoretycznÄ… koncepcjÄ™ komputerÃ³w kwantowych jako fizycznych systemÃ³w, otwierajÄ…c drzwi do praktycznych implementacji.\n1981 Richard Feynman - zwrÃ³ciÅ‚ uwagÄ™ na to, Å¼e klasyczne komputery nie sÄ… w stanie efektywnie symulowaÄ‡ procesÃ³w kwantowych.\n1985 David Deutsch opracowaÅ‚ pierwszy opis kwantowej maszyny Turinga i algorytmÃ³w przeznaczonych do uruchamiania na komputerach kwantowych, w tym bramek kwantowych.\n1994 Peter Shor opracowaÅ‚ algorytm faktoryzacji liczb w czasie wielomianowym, co miaÅ‚o znaczenie dla kryptografii i bezpieczeÅ„stwa informacji.\n1996 Lov Grover - Lov Grover stworzyÅ‚ algorytm Groverâ€™a, ktÃ³ry okazaÅ‚ siÄ™ wyjÄ…tkowo efektywny w przeszukiwaniu stanÃ³w kwantowych.\n2000 ZostaÅ‚ zbudowany pierwszy komputer kwantowy (5 qubitÃ³w) oparty na nuklearnym rezonansie magnetycznym, co stanowiÅ‚o waÅ¼ny krok w rozwoju fizycznych platform komputerÃ³w kwantowych.\n2001 Demonstracja algorytmu Shora potwierdziÅ‚a praktycznoÅ›Ä‡ i znaczenie algorytmÃ³w kwantowych.\n2007 Firma D-Wave dokonaÅ‚a pierwszej sprzedaÅ¼y komercyjnego komputera kwantowego, co miaÅ‚o wpÅ‚yw na rozwÃ³j technologii komputerÃ³w kwantowych w sektorze prywatnym.\nFirma IBM dokonaÅ‚a znaczÄ…cego przeÅ‚omu, pokazujÄ…c, Å¼e klasyczne superkomputery nie sÄ… w stanie efektywnie symulowaÄ‡ systemÃ³w zawierajÄ…cych wiÄ™cej niÅ¼ 56 kubitÃ³w, co jest znane jako â€œquantum supremacy.â€\n23 paÅºdziernika 2019: Google ogÅ‚osiÅ‚ uzyskanie tzw. quantum supremacy na 53 kubitach.\n2020 ZespÃ³Å‚ Jian-Wei Pana z University of Science and Technology of China dokonaÅ‚ przeÅ‚omu, realizujÄ…c 76 fotonowych kubitÃ³w na komputerze Jiuzhang.\n2023 Pierwszy logiczny qubit?\n2025 Google Quantum Echoes\n\nOd okoÅ‚o 1990 roku fizycy i informatycy pracujÄ… nad fizycznÄ… realizacjÄ… komputerÃ³w kwantowych. Jednym z popularnych modeli obliczeÅ„ na komputerach kwantowych jest model oparty na kwantowych obwodach (ang. quantum circuit), ktÃ³ry wykorzystuje qubity zamiast klasycznych bitÃ³w. Podobnie jak w przypadku obwodÃ³w klasycznych, w modelu kwantowym definiuje siÄ™ bramki kwantowe (ang. quantum gates), ktÃ³re pozwalajÄ… na wykonywanie operacji na qubitach.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Wprowadzenie do obliczeÅ„ kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#dlaczego-chcemy-uÅ¼ywaÄ‡-komputerÃ³w-kwantowych",
    "href": "lectures/wyklad1.html#dlaczego-chcemy-uÅ¼ywaÄ‡-komputerÃ³w-kwantowych",
    "title": "Wprowadzenie do obliczeÅ„ kwantowych",
    "section": "Dlaczego chcemy uÅ¼ywaÄ‡ komputerÃ³w kwantowych?",
    "text": "Dlaczego chcemy uÅ¼ywaÄ‡ komputerÃ³w kwantowych?\n\nKwantowa zÅ‚oÅ¼onoÅ›Ä‡ (Quantum Complexity)\nNowy paradygmat wykorzystuje unikalne cechy interferencji, superpozycji i splÄ…tania w celu wykonywania obliczeÅ„.\nObecnie realizowany jest w trzech modelach:\n\nQuantum Circuits (Obwody Kwantowe) - oparty na modelu bramkowym, pozwala realizowaÄ‡ algorytmy typu QAOA, VQA, oraz metody hybrydowe.\nAdiabatyczne Obliczenia Kwantowe (D-Wave) - polegajÄ…ce na minimalizacji energii, z wykorzystaniem optymalizacji QUBO i analogii do modelu Isinga.\nTopologiczne Komputery Kwantowe - oparte na topologicznych kubitach.\n\nProblemy, ktÃ³re uwaÅ¼amy za trudne do rozwiÄ…zania klasycznie, takie jak optymalizacja, stajÄ… siÄ™ Å‚atwiejsze dla komputerÃ³w kwantowych. PrzykÅ‚adem moze byÄ‡ faktoryzacja liczb. Klasyczne komputery nie sÄ… w stanie efektywnie symulowaÄ‡ dziaÅ‚ania kwantowych komputerÃ³w. Koszt najlepszych symulatorÃ³w roÅ›nie wykÅ‚adniczo wraz z liczbÄ… kubitÃ³w. MoÅ¼liwoÅ›ci komputerÃ³w kwantowych sÄ… potencjalnie ogromne, ale obecnie istniejÄ… pewne ograniczenia link. Kwantowy komputer moÅ¼e byÄ‡ uÅ¼ywany do efektywnej symulacji niemal dowolnego procesu fizycznego zachodzÄ…cego w przyrodzie, choÄ‡ nie zawsze jesteÅ›my pewni, czy taka symulacja jest moÅ¼liwa.\nPodstawowym faktem przewagi komputerÃ³w kwantowych nad klasycznymi jest tzw. parallelizm. Ze wzglÄ™du, iÅ¼ kubity moga znajdowac siÄ™ w superpozycji stanÃ³w, komputer kwantowy moÅ¼e przeprowadzic obliczenia jednoczeÅ›nie na wszystkich stanach. Co dokÅ‚adnie to oznacza, poznamy w dalszej czesci wykÅ‚adu. RozwaÅ¼my sytuacjÄ™ w ktÃ³rej chcemy poznac dziaÅ‚anie funkcji \\(f(x)\\) dla pewnego argumentu \\(x\\) (dla pewnej liczby). Aby znaleÅºc wynik dla dwÃ³ch liczb (np. \\(x=0\\) i \\(x=1\\)) klasyczny komputer musi wykonac dwie operacje. Komputer kwantowy moÅ¼e uzuskac ten wynik przeprowadzajac obliczenia jednoczeÅ›nie dla obu waroÅ›ci. Do wykonania takiej operacji wystarczy jeden kubit. NastÄ™pnie jeÅ¼eli bÄ™dziemy chcieli obliczyc nasza funkcjÄ™ dla kolejnych liczb \\(x=2\\) (ktÃ³ra binarnie reprezentowana jest jako \\(10\\)) oraz liczby \\(x=3\\) (binarnie \\(11\\)) musimy dodac kolejny (jeden!) kubit. Dwa kubity moga posÅ‚uÅ¼yc do realizacji czterech rÃ³wnolegÅ‚ych operacji. JeÅ›li rozwaÅ¼ymy 3 kubity znowu mozemy podwoic iloÅ›c operacji (3 kubity maja 8 stanÃ³w bazowych). Dodanie kubitu do komputera kwantowego pozwala podwoic liczbÄ™ obliczeÅ„. W przypadku klasycznego komputera aby uzyskac taki efekt, potrzeba podwoic rownieÅ¼ liczbÄ™ bitÃ³w. n-kubitÃ³w moze realizowac \\(2^n\\) rÃ³wnolegÅ‚ych obliczeÅ„.\n\n\nKwantowa korekcja bÅ‚Ä™dÃ³w (Quantum Error Correction)\nDekoherencja, czyli oddziaÅ‚ywanie z otoczeniem, niszczy stan komputera kwantowego i wprowadza bÅ‚Ä™dy obliczeniowe. Istnieje potrzeba zabezpieczenia przed tym zjawiskiem. Obliczenia kwantowe wymagajÄ… tzw. korekcji bÅ‚Ä™dÃ³w, ktÃ³ra pomaga w utrzymaniu integralnoÅ›ci obliczeÅ„ na komputerach kwantowych. Aktualnie mÃ³wimy o erze Noisy Intermediate-Scale Quantum (NISQ), co oznacza, Å¼e komputery kwantowe wciÄ…Å¼ potrzebujÄ… rozwoju w zakresie korekcji bÅ‚Ä™dÃ³w i stabilnoÅ›ci.\n\n\nRealizacja fizyczna komputerÃ³w kwantowych\nprocesory kwantowe\n\n\n\nProces obliczeÅ„ kwantowych\nWykonanie obliczeÅ„ zwiÄ…zane jest z pojÄ™ciem fizycznego doÅ›wiadczenia. BÄ™dzie siÄ™ ono skÅ‚adaÄ‡ z trzech czÄ™Å›ci:\n\nprzygotowanie (przygotuj stan kwantowy kubitÃ³w),\newolucja (przeprowadÅº transformacjÄ™ za pomocÄ… bramek kwantowych),\npomiar i interpretacja wynikÃ³w.\n\n\nPodobnie w informatyce i w analizach danych wykonujemy obliczenia klasyczne. przygotowujemy dane (stan poczÄ…tkowy); nastÄ™pnie wykonujemy program (ewolucja) i odczytujemy wyniki (pomiar).\n\nNie obserwujemy tych etapÃ³w podczas codziennej interakcji z komputerem, wiÄ™c nie zauwaÅ¼amy w sposÃ³b Å›wiadomy powyÅ¼szego schematu dziaÅ‚ania. Piotr Gawron, Oscar SÅ‚owik - Rewolucja Stanu, Fantastyczne wprowadzenie do informatyki kwantowej.\n\nKaÅ¼dy komputer kwantowy (koprocesor) musi komunikowaÄ‡ siÄ™ z podukÅ‚adem klasycznym.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Wprowadzenie do obliczeÅ„ kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#quantum-machine-learning",
    "href": "lectures/wyklad1.html#quantum-machine-learning",
    "title": "Wprowadzenie do obliczeÅ„ kwantowych",
    "section": "Quantum Machine Learning",
    "text": "Quantum Machine Learning\n\nKlasyczne dane w kwantowym uczeniu maszynowym\n\n\nCC - Classical data using classical computers, algorytmy inspirowane obliczeniami kwantowymi\nQC - Quantum data using classical (ML) computers. link1, link2, link3\nCQ - Classical data on qunatum computers. Na tym chcemy siÄ™ skupiÄ‡.\nQQ - Quantum data on quantum computers. Who knows?\n\n\n\nDostÄ™p do obliczeÅ„ kwantowych w chmurze\n\nIBM Quantum z wykorzystaniem biblioteki qiskit.\nPennylane z wykorzystaniem biblioteki pennylane.\nCirq Google z wykorzystaniem biblioteki cirq.\nD-Wave - Python\nXanadu - Pennylane Python library\nAmazon braket - AWS Python, Julia",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Wprowadzenie do obliczeÅ„ kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html",
    "href": "lectures/wyklad2.html",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "",
    "text": "\\[\n\\newcommand{\\bra}[1]{\\left \\langle #1 \\right \\rvert}\n\\newcommand{\\ket}[1]{\\left \\rvert #1 \\right \\rangle}\n\\newcommand{\\braket}[2]{\\left \\langle #1 \\middle \\rvert #2 \\right \\rangle}\n\\]\nObliczenia (przetwarzanie) wykonywane przez komputer moÅ¼emy zdefiniowaÄ‡ jako transformacje jednego stanu pamiÄ™ci na inny. Z matematycznego punktu widzenia oznacza to, Å¼e obliczenia to funkcje, ktÃ³re przeksztaÅ‚cajÄ… informacje.\nW przypadku klasycznych komputerÃ³w podstawowÄ… jednostkÄ… pamiÄ™ci jest bit (ang. binary digit). Funkcje, ktÃ³re operujÄ… na bitach nazywamy bramkami logicznymi (ang. logic gates).",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#monety",
    "href": "lectures/wyklad2.html#monety",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "Monety",
    "text": "Monety\nRozwaÅ¼my monetÄ™. Posiada ona dwie strony (odrÃ³Å¼nialne od siebie). KÅ‚adÄ…c monetÄ™ na podÅ‚oÅ¼u moÅ¼emy okreÅ›liÄ‡ dostÄ™pne stany jako: \\(\\ket{O}\\), \\(\\ket{R}\\).\nPosiadajÄ…c dwie takie monety moÅ¼emy okreÅ›liÄ‡ cztery moÅ¼liwe stany: \\[\\ket{OO}, \\ket{OR}, \\ket{RO}, \\ket{RR}\\]\nPoniewaÅ¼ pierwsza moneta ma dwa moÅ¼liwe stany i druga moneta rÃ³wnieÅ¼ ma dwie moÅ¼liwoÅ›ci istniejÄ… \\(2\\times 2= 2^{2} =4\\) moÅ¼liwe stany dla dwÃ³ch monet.\nW przypadku trzech monet moÅ¼emy okreÅ›liÄ‡ osiem moÅ¼liwych stanÃ³w: \\[\\ket{OOO}, \\ket{OOR}, \\ket{ORO}, \\ket{ORR}, \\ket{ROO}, \\ket{ROR}, \\ket{RRO}, \\ket{RRR}\\] Co daje nam \\(2 \\times 2 \\times 2 = 2^{3}=8\\) moÅ¼liwych stanÃ³w.\ndla \\(n\\) monet moÅ¼emy okreÅ›liÄ‡ \\(2^{n}\\) moÅ¼liwych stanÃ³w.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#koÅ›ci-do-gry",
    "href": "lectures/wyklad2.html#koÅ›ci-do-gry",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "KoÅ›ci do gry",
    "text": "KoÅ›ci do gry\nKoÅ›ci do gry posiadajÄ… szeÅ›Ä‡ stron. KaÅ¼dej moÅ¼emy przypisaÄ‡ liczbÄ™ od 1 do 6. Dwie koÅ›ci do gry posiadajÄ… \\(6 \\times 6 = 36\\) moÅ¼liwych stanÃ³w. IdÄ…c dalej, dla trzech koÅ›ci do gry mamy \\(6 \\times 6 \\times 6 = 6^{3} = 216\\) moÅ¼liwych stanÃ³w. Dla n koÅ›ci do gry mamy \\(6^{n}\\) moÅ¼liwych stanÃ³w.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#kodowanie-informacji",
    "href": "lectures/wyklad2.html#kodowanie-informacji",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "Kodowanie informacji",
    "text": "Kodowanie informacji\n\nIle informacji moÅ¼emy zakodowaÄ‡ za pomocÄ… monet i koÅ›ci do gry?\n\nZaÅ‚Ã³Å¼my, Å¼e chcemy przekazaÄ‡ informacje o jednym kolorze tÄ™czy. Dla przypomnienia mamy 7 kolorÃ³w (czerwony, pomaraÅ„czowy, Å¼Ã³Å‚ty, zielony, niebieski, indygo, fioletowy). Te 7 kolorÃ³w moÅ¼na zakodowaÄ‡ (albo zareprezetowaÄ‡) poprzez rÃ³Å¼ne konfiguracje monet i koÅ›ci.\n\n\n\nkolor\nmonety\nkoÅ›ci\n\n\n\n\nczerwony\nOOO\n11\n\n\npomaraÅ„czowy\nOOR\n12\n\n\nÅ¼Ã³Å‚ty\nORO\n13\n\n\nzielony\nORR\n14\n\n\nniebieski\nROO\n15\n\n\nindygo\nROR\n16\n\n\nfioletowy\nRRO\n21\n\n\n\nPozostaÅ‚e konfiguracje nie sÄ… uÅ¼ywane (albo moÅ¼na w nich powtÃ³rzyÄ‡ elementy do zakodowania).\n\nMonety niosÄ… mniej informacji niÅ¼ koÅ›ci do gry. Co prowadzi do waÅ¼nego wniosku: obiekt o dwÃ³ch stanach niesie najmniejszÄ… (moÅ¼liwÄ…) iloÅ›Ä‡ informacji. A co za tym idzie bit jest najmniejszÄ… jednostkÄ… klasycznej informacji.\n\n\nZadanie: Angielski alfabet posiada 26 liter. Ile monet i koÅ›ci potrzeba aby zakodowaÄ‡ informacje o jednej literze?",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#fizyczna-realizacja-klasycznych-bitÃ³w",
    "href": "lectures/wyklad2.html#fizyczna-realizacja-klasycznych-bitÃ³w",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "Fizyczna realizacja klasycznych bitÃ³w",
    "text": "Fizyczna realizacja klasycznych bitÃ³w\nObiekty fizyczne, ktÃ³re mogÄ… przyjmowaÄ‡ dwa stany:\n\nprzeÅ‚Ä…cznik wÅ‚Ä…czony/wyÅ‚Ä…czony\nmoneta\nPÅ‚yty CD, DVD (ang. pits - wypalone laserem dziury, lands - nie wypalone dziury)\nobwÃ³d elektryczny - moÅ¼e mieÄ‡ dowolny prÄ…d - ale wybieramy np. 0 lub 5V.\n\n\njak sprowadziÄ‡ klasyfikacjÄ™ dla wielu klas do 2 klasowej?\n\nZazwyczaj oznaczamy te stany jako \\(0\\) i \\(1\\).\nZastosujmy nowe oznaczenia dla tych stanÃ³w: \\[0 = \\ket{0} \\] oraz \\[1 = \\ket{1}\\]",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#reprezentacja-binarna",
    "href": "lectures/wyklad2.html#reprezentacja-binarna",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "Reprezentacja binarna",
    "text": "Reprezentacja binarna\nW przypadku trzech monet mieliÅ›my nastÄ™pujÄ…ce stany: \\[OOO, OOR, ORO, ORR, ROO, ROR, RRO, RRR.\\]\nMoÅ¼emy je zapisaÄ‡ w postaci binarnej jako: \\[000, 001, 010, 011, 100, 101, 110, 111\\] Dla przyzwyczajenia z zapisem moÅ¼emy napisaÄ‡ rÃ³wnieÅ¼: \\[\\ket{000}, \\ket{001}, \\ket{010}, \\ket{011}, \\ket{100}, \\ket{101}, \\ket{110}, \\ket{111}\\]\nCo oznacza, Å¼e moÅ¼emy te stany zapisaÄ‡ matematycznie - Å‚atwo nam nimi operowaÄ‡.\n\nliczby tego typu nazywamy liczbami binarnymi, albo liczbami o podstawie 2.\n\nJak zapisujemy liczby w systemie dziesiÄ™tnym? i dlaczego nazywa siÄ™ on dziesiÄ™tnym?\n\\[6174 = 6*1000 + 1*100 + 7*10 + 4*1 = 6*10^{3} + 1*10^{2} + 7*10^{1} +4*10^{0}\\]\nNatomiast: \\[11010_{2} = 1*2^{4} + 1*2^{3} + 0*2^{2} + 1*2^{1} + 0*2^{0} \\] co w systemie dziesiÄ™tnym daje nam: \\(26\\).\n\nZadanie: \\(10111_{2}= ?_{10}\\)\nZadanie: \\(42= ?_{2}\\)\n\nIstnieje jeszcze wygodny system 16-tkowy (szesnastkowy) - gdzie wspÃ³Å‚czynniki zapisujemy jako \\(0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F\\).\n\nZadanie: \\(F2A_{16}= ?_{10}\\)\n\n\nJak zliczamy liczby w systemie dziesiÄ™tnym? a jak w dwÃ³jkowym?\n\n\nJak zapisaÄ‡ liczby ujemne i zmiennoprzecinkowe binarnie?\n\n\nASCII American Standard Code for Information Interchange\nKomputer przechowuje informacje jako bity, jednak w naszym Å›wiecie uÅ¼ywamy tekstu zÅ‚oÅ¼onego z liter. System ASCII przechowuje informacje o literach w postaci 7 bitowych liczb binarnych. W ten sposÃ³b moÅ¼emy zapisaÄ‡ 128 znakÃ³w (liter, cyfr, znakÃ³w specjalnych) \\(0000000\\) do \\(1111111\\).\n\nZnajdÅº w internecie tablicÄ™ ASCII i zapisz w kodzie ASCII swoje imiÄ™.\n\nStandard UTF-8 (Unicode Transformation Format) przechowuje informacje o literach w postaci liczb binarnych do 32 bitÃ³w. co daje nam moÅ¼liwoÅ›Ä‡ zapisania \\(2^{32}=4294967296\\) stanÃ³w. 128 pierwszych bitowych napisÃ³w w UTF-8 to kod ASCII.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#bramki-logiczne",
    "href": "lectures/wyklad2.html#bramki-logiczne",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "Bramki logiczne",
    "text": "Bramki logiczne\nBramki logiczne to funkcje Boolowskie, ktÃ³re moÅ¼emy skÅ‚adaÄ‡ w bardziej zÅ‚oÅ¼one ukÅ‚ady (ang. circuits). Stworzone przez Georgeâ€™a Booleâ€™a w 1854 roku, algebra boola jest matematycznÄ… strukturÄ…, ktÃ³ra opisuje zachowanie siÄ™ obiektÃ³w, ktÃ³re mogÄ… przyjmowaÄ‡ tylko jednÄ… z dwÃ³ch wartoÅ›ci: prawda lub faÅ‚sz. Zdolne sÄ… one do wykonywania np. dodawania, mnoÅ¼enia czy teÅ¼ innych bardziej skomplikowanych operacji.\nW latach trzydziestych XX wieku Claude Shannon zastosowaÅ‚ algebrÄ™ boola do analizy i projektowania ukÅ‚adÃ³w elektrycznych. Co oznacza, Å¼e zareprezentowaÅ‚ on funkcje boolowskie za pomocÄ… przeÅ‚Ä…cznikÃ³w elektrycznych. Dlatego teÅ¼ komponenty elektroniczne odpowiadajÄ…ce funkcjom boolowskim nazywamy bramkami logicznymi.\n\nCiekawostka. Richard Feynman wykÅ‚adaÅ‚ teoriÄ™ obliczeÅ„ na Kalifornijskim Instytucie Technologii. WykÅ‚ad ten prezentowany jest obecnie jako Feynmana wykÅ‚ady o obliczeniach (ang. Feynman Lectures on Computation).\n\n\nZ pozoru obliczenia przedstawione w ten sposÃ³b wyglÄ…dajÄ… jako abstrakcjny matematyczny koncept. Jednak jego realizacja zawsze wymaga jakiegoÅ› ukÅ‚adu fizycznego realizujÄ…cego wykonywanie funkcji. Nie ma znaczenia jak ten ukÅ‚ad zostanie zrealizowany: kule bilardowe, przeÅ‚Ä…czniki elektroniczne, tranzystory, czy cokolwiek innego.\n\nLogika obliczeÅ„ jest niezaleÅ¼na od realizacji bramek logicznych.\n\nZ punktu widzenia realizacji zawsze chodzi nam o kontrolowany sposÃ³b zmiany stanu ukÅ‚adu.\nNa wykÅ‚adzie postaramy siÄ™ wskazaÄ‡ jak i kiedy logika klasycznych obliczeÅ„ moÅ¼e byÄ‡ uogÃ³lniona przez logikÄ™ obliczeÅ„ kwantowych. Jasne jest, Å¼e przypadek klasyczny powinien byÄ‡ szczegÃ³lnym przypadkiem kwantowego.\nobwody klasyczne\nZobaczmy jakie bramki moÅ¼emy okreÅ›liÄ‡ dla jednego bitu.\n\nBramki logiczne dla jednego bitu\nIle bramek mamy gdy input = 1 bit, output = 1 bit? Ile funkcji moÅ¼emy zdefiniowaÄ‡ dla odwzorowania jednego bitu w jeden bit?\nWszystkie cztery operatory dziaÅ‚ajÄ…ce na jednym bicie moÅ¼emy okreÅ›liÄ‡ jako:\n\nIdentycznoÅ›Ä‡ (ang. identity) - \\(I(0)=0\\), \\(I(1)=1\\)\nNegacja (ang. negation, NOT, filps) - \\(NOT(0)=1\\), \\(NOT(1)=0\\)\nStaÅ‚e zero \\(ZERO(0)=0\\), \\(ZERO(1)=0\\)\nStaÅ‚e jeden \\(ONE(0)=1\\), \\(ONE(1)=1\\)\n\nPo zastosowaniu operatora \\(I\\) oraz \\(NOT\\), z otrzymanego wyniku moÅ¼emy wyznaczyc wartoÅ›ci poczÄ…tkowe. Jednak po zastosowaniu dwÃ³ch pozostaÅ‚ych operacji \\(ZERO\\) i \\(ONE\\) nie jesteÅ›my w stanie okreÅ›lic jaki byÅ‚ stan poczÄ…tkowy, ktÃ³ry wygenerowaÅ‚ okreÅ›lony wynik.\nTe dwie wÅ‚asnoÅ›ci pozwalajÄ… nam sklasyfikowac operatory jako:\n\nOdwracalne - moÅ¼emy odtworzyc wartoÅ›c poczÄ…tkowÄ… z wartoÅ›ci koÅ„cowej\nNieodwracalne - NIE moÅ¼emy odtworzyc wartoÅ›ci poczÄ…tkowej z wartoÅ›ci koÅ„cowej.\n\nJak pokaÅ¼emy pÃ³Åºniej, wszystkie operatory reprezentujÄ…ce kwantowe bramki bÄ™dÄ… odwracalne.\n\n\nInne bramki i operacje logiczne\n\nZrÃ³bmy krÃ³tkie przedstawienie niektÃ³rych, klasycznych bramek logicznych.\nBramka logiczna jest implementacjÄ… funkcji boolowskiej. OperacjÄ… logicznÄ… przeprowadzanÄ… na jednym lub kilku binarnych wejÅ›ciach produkujÄ…cÄ… jednÄ… binarnÄ… wartoÅ›Ä‡ wyjÅ›ciowÄ…. \\[f: \\{0,1\\}^{n} \\to \\{0,1\\} \\]\nKaÅ¼dy element algebry boola (Boolean Statements) musi byÄ‡ okreÅ›lony jako prawda albo faÅ‚sz.\nBramki logiczne moÅ¼emy wyraziÄ‡ za pomocÄ… tablicy prawdy (ang. truth table). Tablica ta posiada jednÄ… kolumnÄ™ dla kaÅ¼dej zmiennej wejÅ›ciowej oraz jednÄ… kolumnÄ™ dla zmiennej wyjÅ›ciowej. Kolumna wyjÅ›ciowa przedstawia wszystkie moÅ¼liwe wyniki przedstawianej logicznej operacji reprezentowanej przez tablicÄ™. KaÅ¼dy wiersz tablicy prawdy reprezentuje jednÄ… moÅ¼liwÄ… kombinacjÄ™ (konfiguracje) danych wejÅ›ciowych oraz wyniku.\nPodstawowe bramki, ktÃ³re znasz to:\n\nAND - koniunkcja\nOR - alternatywa\nNOT - negacja\nNAND - not and\nXOR - alternatywa wykluczajÄ…ca (Exclusive OR) - dodawanie modulo 2\n\n\nZadanie: zapisz tablicÄ™ prawdy dla kaÅ¼dej bramki.\n\n\nZadanie: PorÃ³wnaj AND oraz OR z potocznym znaczeniem tych sÅ‚Ã³w.\n\n\nZadanie: Dlaczego Algebra boola nazywana jest algebrÄ… zbiorÃ³w?\n\n\nZadanie: Czy skÅ‚adanie podzbiorÃ³w zbioru rÃ³wnieÅ¼ generuje algebrÄ™ boola ?\n\n\nZadanie: ile bramek logicznych moÅ¼emy stworzyÄ‡ dla jednego bitu, dwÃ³ch bitÃ³w, trzech bitÃ³w?\n\n\n\nUniweralne bramki logiczne - NAND\nTak jak widzieliÅ›my dla 1-bitu informacji mieliÅ›my 4 bramki logiczne. Dla 2-bitÃ³w mieliÅ›my 16 bramek logicznych. Dla 3-bitÃ³w mamy juÅ¼ 256 moÅ¼liwoÅ›ci.\nW przypadku 2-bitÃ³w nie wypisaliÅ›my wszystkich bramek, dlaczego?\nCzy musimy realizowaÄ‡ wszystkie?\n\nNa szczÄ™Å›cie odpowiedÅº jest negatywna.\n\nIstniejÄ… tzw. zbiory bramek uniwersalnych dziÄ™ki ktÃ³rym moÅ¼emy zrealizowaÄ‡ dowolnÄ… funkcjÄ™ boolowskÄ….\n\nNOT, AND, OR\nNAND, AND\nNAND\nNOT, OR\nNOR",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#szyfrowanie-z-wykorzystaniem-bramki-xor",
    "href": "lectures/wyklad2.html#szyfrowanie-z-wykorzystaniem-bramki-xor",
    "title": "Klasyczne bramki logiczne - Algebra Boola",
    "section": "Szyfrowanie z wykorzystaniem bramki XOR",
    "text": "Szyfrowanie z wykorzystaniem bramki XOR\nWeÅºmy dwie sekwencje bitÃ³w:\nsekwencja A - przedstawiajÄ…ca naszÄ… zakodowanÄ… wiadomoÅ›c: \\[1 0 1 1 0 1 1 1 0 0 0\\]\nlosowa sekwencja B: \\[0 1 1 0 1 1 0 1 1 0 1\\]\ni obliczmy XOR miÄ™dzy dwoma sekwencjami (dla poszczegÃ³lnych kolumn) \\(A\\) XOR \\(B\\).\nZgodnie z tablicÄ… prawdy dla XOR otrzymujemy: \\[1 1 0 1 1 0 1 0 1 0 1\\]\nNa otrzymanym wyniku jeszcze raz zastosuj bramkÄ™ XOR.\nCo moÅ¼esz zauwaÅ¼yÄ‡?\n\nOblicz A XOR B XOR B.\n\nZamieÅ„ wiadomoÅ›Ä‡, ktÃ³rÄ… chcesz zaszyfrowaÄ‡ na binarnÄ… postaÄ‡ (czyli jako sekwencjÄ™ zer i jedynek).\nWeÅº losowÄ… sekwencjÄ™ bitÃ³w (klucz szyfrujÄ…cy), ktÃ³rÄ… zna tylko nadawca i odbiorca.\n\\(1001011010...\\) - wiadomoÅ›Ä‡\n\\(0110101010...\\) - klucz szyfrujÄ…cy, czyli losowa sekwencja bitÃ³w\nZaszyfruj wiadomoÅ›Ä‡ wykonujÄ…c operacjÄ™ XOR na kaÅ¼dym bicie wiadomoÅ›ci i klucza szyfrujÄ…cego. Tak otrzymanÄ… wiadomoÅ›Ä‡ (zaszyfrowanÄ…) wyÅ›lij do odbiorcy.\n\ndecyrpting message\nOdbiorca otrzymuje zaszyfrowanÄ… wiadomoÅ›Ä‡ (posiada klucz szyfrujÄ…cy).\nMessage XOR SecretSequence = EncryptedMessage\nMessage XOR SecretSequence XOR SecretSequence = Message\nNie powinniÅ›my uÅ¼ywaÄ‡ SecretSequence wiÄ™cej niÅ¼ raz. JeÅ›li uÅ¼yjemy jej wiÄ™cej niÅ¼ raz, to Å‚atwo jest zÅ‚amaÄ‡ szyfrowanie.\n\nDlaczego nie powinno siÄ™ uÅ¼ywaÄ‡ szyfru wiÄ™cej niÅ¼ jeden raz? PodpowiedÅº: Zakodowana wiadomoÅ›Ä‡ przestaje byÄ‡ losowa.\n\n\nimport random\n\ndef text_to_bits(text):\n    return ' '.join(format(ord(c), '08b') for c in text)\n\ndef xor_encrypt_decrypt(text, key):\n    return ''.join(chr(ord(c) ^ key) for c in text)\n\n# ====== DANE ======\nplaintext = \"Quantum ML\"\nkey = random.randint(0, 255)  # losowy 8-bitowy klucz\n\n# ====== SZYFROWANIE ======\nciphertext = xor_encrypt_decrypt(plaintext, key)\ndecrypted = xor_encrypt_decrypt(ciphertext, key)\n\n# ====== WYNIKI ======\nprint(f\"Oryginalny tekst:     {plaintext}\")\nprint(f\"Klucz (8-bit):        {key} ({format(key, '08b')})\\n\")\n\nprint(f\"Tekst binarnie:       {text_to_bits(plaintext)}\")\nprint(f\"Zaszyfrowany binarnie:{text_to_bits(ciphertext)}\\n\")\n\nprint(f\"Zaszyfrowany tekst:   {ciphertext}\")\nprint(f\"Odszyfrowany tekst:   {decrypted}\")\n\nprint(f\"\\nCzy poprawnie odszyfrowano? {'TAK' if decrypted == plaintext else 'NIE'}\")\n\nOryginalny tekst:     Quantum ML\nKlucz (8-bit):        153 (10011001)\n\nTekst binarnie:       01010001 01110101 01100001 01101110 01110100 01110101 01101101 00100000 01001101 01001100\nZaszyfrowany binarnie:11001000 11101100 11111000 11110111 11101101 11101100 11110100 10111001 11010100 11010101\n\nZaszyfrowany tekst:   ÃˆÃ¬Ã¸Ã·Ã­Ã¬Ã´Â¹Ã”Ã•\nOdszyfrowany tekst:   Quantum ML\n\nCzy poprawnie odszyfrowano? TAK\n\n\n\nciphertext = xor_encrypt_decrypt(\"Quantum ML\", 77)  # zaszyfruj znanym kluczem\nprint(\"Szyfrogram:\", ciphertext)\n\nprint(\"\\nPrÃ³ba zÅ‚amania:\")\nfor k in range(256):\n    candidate = xor_encrypt_decrypt(ciphertext, k)\n    if candidate.isprintable():\n        print(f\"Klucz {k:3d} -&gt; {candidate}\")\n\nSzyfrogram: \u001c8,#98 m\u0001\n\nPrÃ³ba zÅ‚amania:\nKlucz  64 -&gt; \\xlcyx`-@A\nKlucz  65 -&gt; ]ymbxya,A@\nKlucz  66 -&gt; ^zna{zb/BC\nKlucz  67 -&gt; _{o`z{c.CB\nKlucz  68 -&gt; X|hg}|d)DE\nKlucz  69 -&gt; Y}if|}e(ED\nKlucz  72 -&gt; Tpdkqph%HI\nKlucz  73 -&gt; Uqejpqi$IH\nKlucz  74 -&gt; Vrfisrj'JK\nKlucz  75 -&gt; Wsghrsk&KJ\nKlucz  76 -&gt; Pt`outl!LM\nKlucz  77 -&gt; Quantum ML\nKlucz  78 -&gt; Rvbmwvn#NO\nKlucz  79 -&gt; Swclvwo\"ON\nKlucz  80 -&gt; Lh|sihp=PQ\nKlucz  81 -&gt; Mi}rhiq&lt;QP\nKlucz  82 -&gt; Nj~qkjr?RS\nKlucz  84 -&gt; Hlxwmlt9TU\nKlucz  85 -&gt; Imyvlmu8UT\nKlucz  86 -&gt; Jnzuonv;VW\nKlucz  87 -&gt; Ko{tnow:WV\nKlucz  88 -&gt; D`t{a`x5XY\nKlucz  89 -&gt; Eauz`ay4YX\nKlucz  90 -&gt; Fbvycbz7Z[\nKlucz  91 -&gt; Gcwxbc{6[Z\nKlucz  93 -&gt; Aeq~de}0]\\\nKlucz  94 -&gt; Bfr}gf~3^_\nKlucz 193 -&gt; ÃÃ¹Ã­Ã¢Ã¸Ã¹Ã¡Â¬ÃÃ€\nKlucz 194 -&gt; ÃÃºÃ®Ã¡Ã»ÃºÃ¢Â¯Ã‚Ãƒ\nKlucz 195 -&gt; ÃŸÃ»Ã¯Ã ÃºÃ»Ã£Â®ÃƒÃ‚\nKlucz 196 -&gt; Ã˜Ã¼Ã¨Ã§Ã½Ã¼Ã¤Â©Ã„Ã…\nKlucz 197 -&gt; Ã™Ã½Ã©Ã¦Ã¼Ã½Ã¥Â¨Ã…Ã„\nKlucz 198 -&gt; ÃšÃ¾ÃªÃ¥Ã¿Ã¾Ã¦Â«Ã†Ã‡\nKlucz 199 -&gt; Ã›Ã¿Ã«Ã¤Ã¾Ã¿Ã§ÂªÃ‡Ã†\nKlucz 200 -&gt; Ã”Ã°Ã¤Ã«Ã±Ã°Ã¨Â¥ÃˆÃ‰\nKlucz 201 -&gt; Ã•Ã±Ã¥ÃªÃ°Ã±Ã©Â¤Ã‰Ãˆ\nKlucz 202 -&gt; Ã–Ã²Ã¦Ã©Ã³Ã²ÃªÂ§ÃŠÃ‹\nKlucz 203 -&gt; Ã—Ã³Ã§Ã¨Ã²Ã³Ã«Â¦Ã‹ÃŠ\nKlucz 204 -&gt; ÃÃ´Ã Ã¯ÃµÃ´Ã¬Â¡ÃŒÃ\nKlucz 206 -&gt; Ã’Ã¶Ã¢Ã­Ã·Ã¶Ã®Â£ÃÃ\nKlucz 207 -&gt; Ã“Ã·Ã£Ã¬Ã¶Ã·Ã¯Â¢ÃÃ\nKlucz 208 -&gt; ÃŒÃ¨Ã¼Ã³Ã©Ã¨Ã°Â½ÃÃ‘\nKlucz 209 -&gt; ÃÃ©Ã½Ã²Ã¨Ã©Ã±Â¼Ã‘Ã\nKlucz 210 -&gt; ÃÃªÃ¾Ã±Ã«ÃªÃ²Â¿Ã’Ã“\nKlucz 211 -&gt; ÃÃ«Ã¿Ã°ÃªÃ«Ã³Â¾Ã“Ã’\nKlucz 212 -&gt; ÃˆÃ¬Ã¸Ã·Ã­Ã¬Ã´Â¹Ã”Ã•\nKlucz 213 -&gt; Ã‰Ã­Ã¹Ã¶Ã¬Ã­ÃµÂ¸Ã•Ã”\nKlucz 214 -&gt; ÃŠÃ®ÃºÃµÃ¯Ã®Ã¶Â»Ã–Ã—\nKlucz 215 -&gt; Ã‹Ã¯Ã»Ã´Ã®Ã¯Ã·ÂºÃ—Ã–\nKlucz 216 -&gt; Ã„Ã Ã´Ã»Ã¡Ã Ã¸ÂµÃ˜Ã™\nKlucz 217 -&gt; Ã…Ã¡ÃµÃºÃ Ã¡Ã¹Â´Ã™Ã˜\nKlucz 218 -&gt; Ã†Ã¢Ã¶Ã¹Ã£Ã¢ÃºÂ·ÃšÃ›\nKlucz 219 -&gt; Ã‡Ã£Ã·Ã¸Ã¢Ã£Ã»Â¶Ã›Ãš\nKlucz 220 -&gt; Ã€Ã¤Ã°Ã¿Ã¥Ã¤Ã¼Â±ÃœÃ\nKlucz 221 -&gt; ÃÃ¥Ã±Ã¾Ã¤Ã¥Ã½Â°ÃÃœ\nKlucz 222 -&gt; Ã‚Ã¦Ã²Ã½Ã§Ã¦Ã¾Â³ÃÃŸ\nKlucz 223 -&gt; ÃƒÃ§Ã³Ã¼Ã¦Ã§Ã¿Â²ÃŸÃ",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Klasyczne bramki logiczne - Algebra Boola"
    ]
  },
  {
    "objectID": "cwiczenia/cw1.html",
    "href": "cwiczenia/cw1.html",
    "title": "Modele uczenia maszynowego",
    "section": "",
    "text": "RozwÃ³j technologii i powszechna cyfryzacja przyczyniÅ‚y siÄ™ do powstania nowego zasobu, jakim sÄ… dane. Dane te sÄ… generowane i przetwarzane zarÃ³wno w sposÃ³b ustrukturyzowany, jak i nieustrukturyzowany. Strukturyzacja danych doprowadziÅ‚a do rozwoju wielu modeli, ktÃ³re dziÅ› ogÃ³lnie okreÅ›lamy jako modele uczenia maszynowego (ang. machine learning, ML). Natomiast przetwarzanie danych nieustrukturyzowanych takich jak tekst, obrazy czy wideo, przyczyniÅ‚o siÄ™ do rozwoju uczenia gÅ‚Ä™bokiego (ang. deep learning, DL). Oba te podejÅ›cia czÄ™sto okreÅ›lane zbiorczo jako sztuczna inteligencja (ang. artificial inteligence, AI), zostaÅ‚y stworzone gÅ‚Ã³wnie do rozpoznawania wzorcÃ³w. Jednak coraz czÄ™Å›ciej wykorzystywane sÄ… rÃ³wnieÅ¼ do modelowania i generowania nowych danych. Klasyczny model sztucznej inteligencji moÅ¼emy wyraziÄ‡ jako funkcjÄ™ \\(f(X,\\theta)\\), ktÃ³ra zaleÅ¼y zarÃ³wno od danych reprezentowanych przez ustrukturyzowanÄ… macierz \\(X\\), jak i od parametrÃ³w \\(\\theta\\), ktÃ³rych wartoÅ›ci zostajÄ… ustalone w procesie uczenia.\nW uczeniu nadzorowanym posiadamy wartoÅ›ci zmiennej celu dla wygenerowanych danych treningowych. Dwa podstawowe modele nadzorowanego uczenia maszynowego moÅ¼emy zrealizowaÄ‡ jako proste sieci neuronowe.\nDo wygenerowania kodÃ³w uÅ¼yjemy biblioteki PyTorch",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "cwiczenia/cw1.html#regresja-liniowa",
    "href": "cwiczenia/cw1.html#regresja-liniowa",
    "title": "Modele uczenia maszynowego",
    "section": "Regresja liniowa",
    "text": "Regresja liniowa\nWygenerujemy niezaszumione dane na podstawie wzoru \\(y = 2 x - 1\\). Na podstawie zbioru danych postaramy siÄ™ oszacowaÄ‡ nieznane parametry czyli wyraz przy \\(x\\) (\\(\\alpha_1 = 2\\)) i wyraz wolny (\\(\\alpha_0 = -1\\)).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# zbior danych\nx = range(11)\ny = [2*xi - 1 for xi in x]\nplt.plot(x, y, 'go', label='True data', alpha=0.5)\n\n\n\n\n\n\n\n\nModel regresji liniowej dla jednej zmiennej moÅ¼na zrealizowaÄ‡ jako prostÄ… jednowarstwowÄ… sieÄ‡ neuronowÄ…. CaÅ‚y proces moÅ¼na zrealizowaÄ‡ za pomocÄ… obiektu torch.nn.Linear\n\nimport torch\n\nclass LinearRegression(torch.nn.Module):\n\n    def __init__(self, inputSize, outputSize):\n        super(LinearRegression, self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(inputSize, outputSize)\n        ) \n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nm = torch.nn.Linear(1,1)\n\n\nm\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nm.weight, m.bias\n\n(Parameter containing:\n tensor([[-0.1154]], requires_grad=True),\n Parameter containing:\n tensor([0.4927], requires_grad=True))\n\n\nAby nasze dane mogÅ‚ybyÄ‡ przeliczane przez bibliotekÄ™ PyTorch musimy je przetworzyÄ‡ na tensory - czyli obiekty z biblioteki PyTorch.\n\n# dostosowanie do pytorch\nx = np.array(x, dtype=np.float32)\ny = np.array(y, dtype=np.float32)\n\nX_train = torch.from_numpy(x).view(-1,1)\ny_train = torch.from_numpy(y).view(-1,1)\n\nUwaga - poniewaÅ¼ mamy jednÄ… zmiennÄ… zawierajÄ…cÄ… 10 przypadkÃ³w - potrzebujemy listy skÅ‚adajÄ…cej siÄ™ z 10 list jednoelementowych.\nMozna tez wykorzystac obiektowe programowanie.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass LinearDataset(Dataset):\n    def __init__(self, X_train, y_train):\n        self.X_train = X_train # tensor typu torch\n        self.y_train = y_train\n\n    def __len__(self):\n        return len(self.y_train)\n\n    def __getitem__(self, idx):\n        return self.X_train[idx], self.y_train[idx]\n\n\ndataset = LinearDataset(X_train=X_train, y_train=y_train)\n\n\ndataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n\nMoÅ¼emy utworzyÄ‡ model i wybraÄ‡ optymalizator z funkcjÄ… kosztu.\n\n# obiekt liniowej regresji w wersji sieci nn\nlr_model = LinearRegression(1,1)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)\n\nMoÅ¼emy sprawdziÄ‡, Å¼e nasz model bÄ™dzie dostrajaÅ‚ 2 parametry.\n\nnum_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)\nprint(f\"liczba trenowalnych parametrÃ³w: {num_params}\")\n\nliczba trenowalnych parametrÃ³w: 2\n\n\nParametry te w poczÄ…tkowej inicjalizacji majÄ… nastÄ™pujÄ…ce wartoÅ›ci:\n\nfor layer in lr_model.layers:\n    if isinstance(layer, torch.nn.Linear):\n        print(f\"weight: {layer.state_dict()['weight']}\")\n        print(f\"bias: {layer.state_dict()['bias']}\")\n\nweight: tensor([[0.7394]])\nbias: tensor([0.1832])\n\n\n\nepochs = 400\n# petla uczaca \nfor epoch in range(epochs):\n    lr_model.train() # etap trenowania \n\n    y_pred = lr_model(X_train)\n    loss = criterion(y_pred, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.4f}')\n \n    lr_model.eval() # etap ewaluacji modelu\n\n# po treningu jeszcze raz generujemy predykcje\nlr_model.eval()\nwith torch.no_grad():\n    predicted = lr_model(X_train)\n\nepoch: 050, loss = 0.2946\nepoch: 100, loss = 0.1681\nepoch: 150, loss = 0.0959\nepoch: 200, loss = 0.0547\nepoch: 250, loss = 0.0312\nepoch: 300, loss = 0.0178\nepoch: 350, loss = 0.0101\nepoch: 400, loss = 0.0058\n\n\nMozna tez wykorzystac obiekt dataloader\n\nfor epoch in range(50):\n    for X_batch, y_batch in dataloader:\n        preds = lr_model(X_batch)\n        loss = criterion(preds, y_batch)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, loss = {loss.item():.4f}\")\n\nEpoch 0, loss = 0.0060\nEpoch 10, loss = 0.0046\nEpoch 20, loss = 0.0000\nEpoch 30, loss = 0.0014\nEpoch 40, loss = 0.0001\n\n\nOtrzymane parametry po uczeniu\n\nprint(f\"po procesie uczenia waga: {lr_model.layers[0].weight} oraz bias {lr_model.layers[0].bias}\")\n\npo procesie uczenia waga: Parameter containing:\ntensor([[1.9952]], requires_grad=True) oraz bias Parameter containing:\ntensor([-0.9755], requires_grad=True)\n\n\nDopasowanie modelu do danych moÅ¼na przedstawiÄ‡ na wykresie\n\nplt.clf()\nplt.plot(X_train, y_train, 'go', label='True data', alpha=0.5)\nplt.plot(X_train, predicted, '--', label='Predictions', alpha=0.5)\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport torch\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\ntorch.manual_seed(1234)\n\n# DANE \nx = torch.linspace(0,10,500).view(-1,1)\ny = torch.sin(x)\ny = y + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x,y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nclass SinusEstimator(torch.nn.Module):\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int):\n        super(SinusEstimator,self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(N_INPUT, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,16),\n            torch.nn.Tanh(),\n            torch.nn.Linear(16,N_OUTPUT)\n        )\n\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nmodel = SinusEstimator(1,1)\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\nlosses = []\n\n\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\n\n\ndef train(X,Y, model, optimiser, epochs, lossfn, callback = None):\n    for epoch in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\n\n\nx_train = x.requires_grad_(False)\n\ntrain(x_train, y, model, optimiser, 300, criterion, callback)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    return torch.mean((y-y_pred)**2)\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n\nmodel2 = SinusEstimator(1,1)\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model2.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\nlosses = []\n\ntrain(x_train, y, model2, optimiser, 200, special_loss_fn, callback)",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "cwiczenia/cw1.html#regresja-logistyczna",
    "href": "cwiczenia/cw1.html#regresja-logistyczna",
    "title": "Modele uczenia maszynowego",
    "section": "Regresja logistyczna",
    "text": "Regresja logistyczna\nW przypadku procesu klasyfikacji danych do numerycznego wyniku musimy dodaÄ‡ funkcjÄ™ aktywacji - sigmoid \\(\\sigma\\), ktÃ³ra pozwoli nam wygenerowaÄ‡ prawdopodobieÅ„stwo otrzymania klasy 1.\nDane wygenerujemy na podstawie pakietu scikit-learn\n\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# prepare dataset\nX, y = make_classification(n_samples=10**4, n_features=10 ,random_state=42)\n\n\nimport torch\n\nclass LogisticRegression(torch.nn.Module):\n\n    def __init__(self, inputSize, outputSize):\n        super(LogisticRegression, self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(inputSize, outputSize),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\nPodobnie jak w przypadku regresji liniowej musimy przetworzyÄ‡ nasze dane do obiektÃ³w torch.\n\nX_train = torch.from_numpy(X.astype(np.float32))\ny_train = torch.from_numpy(y.astype(np.float32))\ny_train = y_train.view(y_train.shape[0], 1)\n\n\nmodel = LogisticRegression(X_train.shape[1], y_train.shape[1])\n\nlearningRate = 0.01\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n\n# petla uczaca \nnum_epochs = 500\n\nfor epoch in range(num_epochs):\n    # forward pass and loss\n    model.train()\n    y_predicted = model(X_train)\n    loss = criterion(y_predicted, y_train)\n    \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    model.eval()\n\n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    y_predicted = model(X_train)  # no need to call model.forward()\n    y_predicted_cls = y_predicted.round()   # round off to nearest class\n    acc = y_predicted_cls.eq(y_train).sum() / float(y_train.shape[0])  # accuracy\n    print(f'accuracy = {acc:.4f}')\n    print(f\"predykcja dla wiersza 0:{y_predicted[0]}, wartosc prawdziwa: {y_train[0]}\")\n\nepoch: 50, loss = 0.6307\nepoch: 100, loss = 0.5104\nepoch: 150, loss = 0.4487\nepoch: 200, loss = 0.4120\nepoch: 250, loss = 0.3878\nepoch: 300, loss = 0.3708\nepoch: 350, loss = 0.3581\nepoch: 400, loss = 0.3484\nepoch: 450, loss = 0.3406\nepoch: 500, loss = 0.3343\naccuracy = 0.8830\npredykcja dla wiersza 0:tensor([0.8189]), wartosc prawdziwa: tensor([1.])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "cwiczenia/cw3.html",
    "href": "cwiczenia/cw3.html",
    "title": "Bramki kwantowe",
    "section": "",
    "text": "Bramki dwukubitowe\nZobaczmy jak dziaÅ‚a bramka CNOT (CX) na stany bazy obliczeniowej.\nimport pennylane as qml\nimport pennylane.numpy as np\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef circ(stan='0'):\n    if stan == '1':\n        qml.X(wires=0)\n    qml.CNOT(wires=[0,1])\n    # qml.CNOT(wires=[1,0])\n    return qml.state()\n\n\nstate = circ()\nprint(state)\n\n[1.+0.j 0.+0.j 0.+0.j 0.+0.j]\nstate = circ('1')\nprint(state)\n\n[0.+0.j 0.+0.j 0.+0.j 1.+0.j]\nqml.draw_mpl(circ)()\nqml.draw_mpl(circ)('1')\nRozpatrzmy teraz co siÄ™ dzieje kiedy stan kontrolny jest superpozycjÄ… stanÃ³w bazowych.\n@qml.qnode(dev)\ndef circ2():\n    qml.H(wires=0)\n    qml.CNOT(wires=[0,1])\n    return qml.state()\n\n\nstate = circ2()\nprint(state)\n\n[0.70710678+0.j 0.        +0.j 0.        +0.j 0.70710678+0.j]\nqml.draw_mpl(circ2)()\nOtrzymany stan koÅ„cowy nazywamy stanem Bella.\nimport pennylane as qml\nimport matplotlib.pyplot as plt\n\nSHOTS = 10000\nALL_STATES = ['00', '01', '10', '11']\ndev = qml.device(\"default.qubit\", wires=2, shots=SHOTS)\n\ndef plot_counts_histogram(counts, title):\n    \"\"\"Generuje histogram, wyÅ›wietlajÄ…c wszystkie 4 stany (00, 01, 10, 11).\"\"\"\n    full_counts = {state: counts.get(state, 0) for state in ALL_STATES}\n    probabilities = [full_counts[state] / SHOTS for state in ALL_STATES]\n    \n    plt.figure(figsize=(7, 5))\n    plt.bar(ALL_STATES, probabilities, color='teal')\n    plt.xlabel(\"Stan KoÅ„cowy ($|q_1 q_0 \\\\rangle$)\")\n    plt.ylabel(\"PrawdopodobieÅ„stwo\")\n    plt.title(title + f\" ({SHOTS} shots)\")\n    plt.ylim(0, 1.1)\n    plt.grid(axis='y', alpha=0.5)\n\n    for i, prob in enumerate(probabilities):\n        plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=10)\n\n    plt.show()\n\n\n@qml.qnode(dev)\ndef circ_bell():\n    \"\"\"Stan Bella: splÄ…tanie, wynik powinien byÄ‡ ~50% '00' i ~50% '11'\"\"\"\n    qml.H(wires=0)\n    qml.CNOT(wires=[0, 1])\n    return qml.counts()\n\n@qml.qnode(dev)\ndef circ_h_h():\n    \"\"\"Hadamard na obu: superpozycja, wynik ~25% dla kaÅ¼dego stanu\"\"\"\n    qml.H(wires=0)\n    qml.H(wires=1)\n    return qml.counts()\n\n@qml.qnode(dev)\ndef circ_zero():\n    \"\"\"Stan zerowy: wynik 100% '00'\"\"\"\n    # Brak bramek oznacza stan poczÄ…tkowy |00&gt;\n    return qml.counts()\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n# WywoÅ‚anie dla Stanu Bella\nprint(\"Symulacja Stanu Bella...\")\ncounts_bell = circ_bell() \nplot_counts_histogram(counts_bell, \"Stan Bella ($|\\\\Phi^+ \\\\rangle$)\")\n\nSymulacja Stanu Bella...\n# WywoÅ‚anie dla H-H (RÃ³wnomierna superpozycja)\nprint(\"Symulacja H-H...\")\ncounts_hh = circ_h_h()\nplot_counts_histogram(counts_hh, \"Superpozycja H-H\")\n\nSymulacja H-H...\n# WywoÅ‚anie dla Stanu Zerowego\nprint(\"Symulacja Stanu Zerowego...\")\ncounts_zero = circ_zero()\nplot_counts_histogram(counts_zero, \"Stan Bazowy $|00\\\\rangle$\")\n\nSymulacja Stanu Zerowego...",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Bramki kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3.html#kopiowanie-kubitu",
    "href": "cwiczenia/cw3.html#kopiowanie-kubitu",
    "title": "Bramki kwantowe",
    "section": "Kopiowanie Kubitu",
    "text": "Kopiowanie Kubitu\nKlasyczne komputery bardzo czÄ™sto wykorzystujÄ…c operacjÄ™ kopiowania.\nZobaczmy jak taka operacja wyglÄ…da dla kubitÃ³w.\nRozwazmy obwod z operatorem C, ktÃ³ry w dziaÅ‚aniu na dwa kubity kopiuje wartoÅ›Ä‡ pierwszego kubitu na wynik drugiego. Drugi kubit mozna na poczÄ…tku ustawiÄ‡ w dowolnym stanie.\nChcemy skopiowaÄ‡ stan \\(\\ket{\\psi_0} = a\\ket{0} + b\\ket{1}\\)\nStan poczÄ…tkowy ukÅ‚adu: \\(\\ket{\\psi_0} \\otimes \\ket{0}\\)\nChcemy przeksztaÅ‚ciÄ‡ na \\(\\ket{\\psi_0} \\otimes \\ket{\\psi_0}\\) czyli\n\\[\nC \\left(\\ket{\\psi_0} \\otimes \\ket{0}\\right) = \\ket{\\psi_0} \\otimes \\ket{\\psi_0}\n\\]\nLewa strona\n\\[\nC \\left(\\ket{\\psi_0} \\otimes \\ket{0}\\right) = C\\left(   (a\\ket{0} + b\\ket{1} )  \\otimes \\ket{0} \\right)\n\\] \\[\nC\\left( a\\ket{0} \\otimes \\ket{0} + b\\ket{1}\\otimes \\ket{0} \\right) = a C \\left(\\ket{0} \\otimes \\ket{0}\\right) + b C \\left( \\ket{1}\\otimes \\ket{0}\\right)\n\\] \\[\na \\ket{00} + b \\ket{11}\n\\]\nPrawa strona \\[\n\\ket{\\psi_0} \\otimes \\ket{\\psi_0}  = a^2 \\ket{00} + ab\\ket{01} + ab\\ket{10} + b^2\\ket{11}\n\\]",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Bramki kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3.html#zadanie",
    "href": "cwiczenia/cw3.html#zadanie",
    "title": "Bramki kwantowe",
    "section": "Zadanie",
    "text": "Zadanie\n\nNapisz operator 1+1 na ukÅ‚adzie 4 kubitÃ³w\n\\[\n0+0 = 00\n\\] \\[\n0+1 = 01\n\\] \\[\n1+0 = 01\n\\] \\[\n1+1 = 10\n\\]\nzauwaz, ze mamy dwa typy rozwiÄ…zaÅ„:\n\ndwa bity wejsciowe sÄ… takie same (00, 11) i dajÄ… na prawym bicie odpowiedzi 0.\ndwa bity wejsciowe sÄ… rÃ³zne (10,01) i dajÄ… na prawym bicie odpowiedzi 1.\n\nAby napisaÄ‡ prawidÅ‚owe rozwiÄ…zanie musimy stworzyÄ‡ bramki, ktÃ³re bÄ™dÄ… rozpoznawaÄ‡ czy dwa kubity sÄ… takie same czy tez rÃ³zne. Dla przypomnienia - klasycznie rolÄ™ takÄ… peÅ‚ni bramka XOR.\n\n\n\nInput 1\nInput 2\nXOR\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n\nPodobnie dziaÅ‚a bramka CNOT\n\ndev = qml.device('default.qubit', wires=4, shots=1)\n\n@qml.qnode(dev)\ndef qc(input='00'):\n    if input[0]=='1':\n        qml.X(wires=0)\n    if input[1]=='1':\n        qml.X(wires=1)\n    qml.CNOT(wires=[0,3])\n    qml.CNOT(wires=[1,3])\n    #return qml.state()\n    return qml.counts(wires=[2,3])\n\nqc()\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\n{np.str_('00'): np.int64(1)}\n\n\n\nfig, ax = qml.draw_mpl(qc)()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor input in ['00','01','10','11']:\n    print(f\"wartosci poczatkowe: {input} : wynik {qc(input)}\")\n\nwartosci poczatkowe: 00 : wynik {np.str_('00'): np.int64(1)}\nwartosci poczatkowe: 01 : wynik {np.str_('01'): np.int64(1)}\nwartosci poczatkowe: 10 : wynik {np.str_('01'): np.int64(1)}\nwartosci poczatkowe: 11 : wynik {np.str_('00'): np.int64(1)}\n\n\nZastosowanie dwÃ³ch CNOT do inputÃ³w rozwiÄ…zuje nam problem prawego bitu odpowiedzi.\nCo z pierszym bitem odpowiedzi otrzymywanym po pomiarzze q3 ?\n\njego wartoÅ›Ä‡ dla pierwszych trzech rÃ³wnaÅ„ zawsze wynosi 0.\n\nJednak dla rÃ³wnania 1+1 powinniÅ›my otrzymaÄ‡ 1.\nDo rozwiÄ…zania tego problemu mozna wykorzystaÄ‡ bramkÄ™ operujÄ…cÄ… na 3 kubitach. Bramka ta to bramka Toffoli.\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=4, shots=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.X(wires=0)\n    qml.X(wires=1)\n    qml.CNOT([0,1])\n    qml.CNOT([0,2])\n    qml.Toffoli([0,1,3])\n    return qml.counts(wires=[2,3])\n\nqc()\n\nprint(\"wynik 1+1 =\",int('10', 2))\n\nwynik 1+1 = 2\n\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\n\ndev = qml.device('default.qubit', wires=4, shots=1)\n\n@qml.qnode(dev)\ndef qc(input='00'):\n    if input[0]=='1':\n        qml.X(wires=0)\n    if input[1]=='1':\n        qml.X(wires=1)\n    qml.CNOT(wires=[0,3])\n    qml.CNOT(wires=[1,3])\n    qml.Toffoli(wires=[0,1,2])\n    #return qml.state()\n    return qml.counts(wires=[2,3])\n\n\nfor input in ['00','01','10','11']:\n    print(f\"wartosci poczatkowe: {input} : wynik {qc(input)}\")\n\nwartosci poczatkowe: 00 : wynik {np.str_('00'): np.int64(1)}\nwartosci poczatkowe: 01 : wynik {np.str_('01'): np.int64(1)}\nwartosci poczatkowe: 10 : wynik {np.str_('01'): np.int64(1)}\nwartosci poczatkowe: 11 : wynik {np.str_('10'): np.int64(1)}\n\n\n\n# 2. Stan Phi Minus (|Î¦-âŸ©) -&gt; 1/âˆš2 (|00âŸ© - |11âŸ©)\n# Przy pomiarze counts wyglÄ…da tak samo jak |Î¦+âŸ©, rÃ³Å¼ni siÄ™ fazÄ… (minus).\n@qml.qnode(dev)\ndef circ_phi_minus():\n    qml.H(wires=0)\n    qml.CNOT(wires=[0, 1])\n    qml.X(wires=0)    \n    return qml.counts()\n\nfig, ax = qml.draw_mpl(circ_phi_minus)()\nplt.show()\n\n# Rysowanie Phi Minus\ncounts_phi_minus = circ_phi_minus()\nplot_counts_histogram(counts_phi_minus, \"Stan Bella $|\\Phi^- \\\\rangle$\")\n\n&lt;&gt;:15: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:15: SyntaxWarning: invalid escape sequence '\\P'\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_46886/2338609265.py:15: SyntaxWarning: invalid escape sequence '\\P'\n  plot_counts_histogram(counts_phi_minus, \"Stan Bella $|\\Phi^- \\\\rangle$\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@qml.qnode(dev)\ndef circ_psi_plus():\n    qml.H(wires=0)\n    qml.CNOT(wires=[0, 1])\n    qml.Z(wires=1)          # Zmiana stanu wejÅ›ciowego na |01âŸ©\n    return qml.counts()\n\nfig, ax = qml.draw_mpl(circ_psi_plus)()\nplt.show()\n\n# Rysowanie Psi Plus\ncounts_psi_plus = circ_psi_plus()\nplot_counts_histogram(counts_psi_plus, \"Stan Bella $|\\Psi^+ \\\\rangle$\")\n\n&lt;&gt;:13: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:13: SyntaxWarning: invalid escape sequence '\\P'\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_46886/1188501442.py:13: SyntaxWarning: invalid escape sequence '\\P'\n  plot_counts_histogram(counts_psi_plus, \"Stan Bella $|\\Psi^+ \\\\rangle$\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@qml.qnode(dev)\ndef circ_psi_minus():\n    qml.Hadamard(wires=0)\n    qml.CNOT(wires=[0,1])\n    qml.X(wires=1)\n    qml.Z(wires=1)\n    return qml.counts()\n\nfig, ax = qml.draw_mpl(circ_psi_minus)()\nplt.show()\n\n# Rysowanie Psi Minus\ncounts_psi_minus = circ_psi_minus()\nplot_counts_histogram(counts_psi_minus, \"Stan Bella $|\\Psi^- \\\\rangle$\")\n\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\P'\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_46886/2492143563.py:14: SyntaxWarning: invalid escape sequence '\\P'\n  plot_counts_histogram(counts_psi_minus, \"Stan Bella $|\\Psi^- \\\\rangle$\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@qml.qnode(dev)\ndef circ3(theta):\n    qml.H(wires=0)\n    qml.CNOT(wires=[0,1])\n    qml.RZ(theta, wires=0)\n    return qml.probs(wires=[0,1])\n\n\nstate = circ3(np.pi)\nprint(state)\n\n[0.5026 0.     0.     0.4974]\n\n\n\nqml.draw_mpl(circ3)(np.pi)\n\n\n\n\n\n\n\n\nBramka (i operator) Z, w bazie obliczeniowej dany jest macierzÄ…: \\[\n\\textbf{Z} = \\begin{bmatrix} 1 \\,\\,\\,\\,\\,\\,\\,\\, 0 \\\\ 0 \\,\\, -1 \\end{bmatrix}\n\\]\nOperator ten mierzy rÃ³Å¼nicÄ™ pomiÄ™dzy prawdopodobieÅ„stwem, Å¼e kubit jest w stanie \\(\\ket{0}\\) a prawdopodobieÅ„stwem, Å¼e jest w stanie \\(\\ket{1}\\)\nW ogÃ³lnoÅ›ci wartoÅ›Ä‡ oczekiwana (wartoÅ›Ä‡ Å›rednia wyniku pomiaru w bazie operatora Z) dana jest wzorem: \\[\n\\textbf{&lt;Z&gt;} = \\bra{\\psi} \\textbf{Z} \\ket{\\psi}\n\\]\nNiech \\[\n\\ket{\\psi} = \\alpha\\ket{0} + \\beta\\ket{1}\n\\] wtedy \\[\n\\bra{\\psi} = \\alpha^*\\bra{0} + \\beta^*\\bra{1}\n\\]\nMoÅ¼emy obliczyÄ‡: \\[\n\\bra{\\psi} \\textbf{Z} \\ket{\\psi}  = (\\alpha^*\\bra{0} + \\beta^*\\bra{1} ) \\,\\,\\, Z \\,\\,\\,(\\alpha\\ket{0} + \\beta\\ket{1}) = |\\alpha|^2 - |\\beta|^2\n\\] Czyli dla kubitu w stanie \\(\\ket{0}\\) \\[\n\\textbf{&lt;Z&gt;} = 1  \n\\] Dla kubitu w stanie \\(\\ket{1}\\) \\[\n\\textbf{&lt;Z&gt;} = -1  \n\\] Dla kubitu w superpozycji \\(\\ket{0} +\\ket{1}\\) \\[\n\\textbf{&lt;Z&gt;} = 0  \n\\]\n\n# wartosc oczekiwana \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef qc(test='0'):\n    if test == '1':\n        qml.X(wires=0)\n    return qml.expval(qml.PauliZ(wires=0))\n\n\nqml.draw_mpl(qc)()\n\n\n\n\n\n\n\n\n\nqml.draw_mpl(qc)('1')\n\n\n\n\n\n\n\n\n\nqc(), qc('1')\n\n(np.float64(1.0), np.float64(-1.0))\n\n\n\ndev = qml.device('default.qubit', wires=1)\n@qml.qnode(dev)\ndef qn(theta):\n    qml.X(wires=0)\n    qml.RY(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ntheta = 3.14 # tak nie !!!\ntheta = np.array([3.13], requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\nfor epoch in range(epochs):\n    theta, cost = opt.step_and_cost(qn, theta)\n    if epoch % 10 == 0:\n        print(f\"Theta: {theta}, Cost: {cost}\")\n\nTheta: [3.12884076], Cost: [0.99993281]\nTheta: [3.10851975], Cost: [0.99954803]\nTheta: [3.0558488], Cost: [0.99696296]\nTheta: [2.91986733], Cost: [0.97972892]\nTheta: [2.57778269], Cost: [0.87049361]\nTheta: [1.8364975], Cost: [0.35163303]\nTheta: [0.88932495], Cost: [-0.56371304]\nTheta: [0.33249582], Cost: [-0.93286178]\nTheta: [0.11704127], Cost: [-0.99156122]\nTheta: [0.04085768], Cost: [-0.99896979]",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Bramki kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3.html#wyjaÅ›nienie-dziaÅ‚ania-funkcji-kosztu-w-vqa-mini-eksperyment",
    "href": "cwiczenia/cw3.html#wyjaÅ›nienie-dziaÅ‚ania-funkcji-kosztu-w-vqa-mini-eksperyment",
    "title": "Bramki kwantowe",
    "section": "ğŸ§ WyjaÅ›nienie DziaÅ‚ania Funkcji Kosztu w VQA (Mini-Eksperyment)",
    "text": "ğŸ§ WyjaÅ›nienie DziaÅ‚ania Funkcji Kosztu w VQA (Mini-Eksperyment)\nW tym prostym eksperymencie z Wariacyjnym Algorytmem Kwantowym (VQA), optymalizator (Gradient Descent) minimalizuje funkcjÄ™ kosztu, choÄ‡ nie zostaÅ‚a ona jawnie zdefiniowana jako np. Mean Squared Error (MSE). Dzieje siÄ™ tak, poniewaÅ¼ w bibliotekach takich jak PennyLane, optymalizator domyÅ›lnie przyjmuje, Å¼e to, co zwraca \\(\\mathbf{QNode}\\) jest funkcjÄ… celu, ktÃ³rÄ… naleÅ¼y minimalizowaÄ‡.\n\n1. Definicja Kosztu Przez Operator PauliZ\nTwÃ³j obwÃ³d qn(theta) jest zdefiniowany nastÄ™pujÄ…co:\n\\[\\mathbf{qn}(\\theta) = \\bra{\\psi(\\theta)} \\mathbf{Z} \\ket{\\psi(\\theta)} = \\langle \\mathbf{Z} \\rangle\\]\nPoniewaÅ¼ \\(\\mathbf{qn}(\\theta)\\) zwraca WartoÅ›Ä‡ OczekiwanÄ… operatora PauliZ (\\(\\langle \\mathbf{Z} \\rangle \\in [-1, 1]\\)), optymalizator traktuje ten wynik jako funkcjÄ™ kosztu \\(\\mathbf{C}(\\theta)\\), ktÃ³rÄ… naleÅ¼y sprowadziÄ‡ do najniÅ¼szej moÅ¼liwej wartoÅ›ci, czyli \\(-1\\).\n\\[\\text{Optymalizator dÄ…Å¼y do minimalizacji: } \\mathbf{C}(\\theta) = \\langle \\mathbf{Z} \\rangle\\]\n\n\n2. Cel Obwodu Kwantowego\nW tym kontekÅ›cie, obwÃ³d VQA jest uÅ¼ywany do znalezienia stanu wÅ‚asnego operatora \\(\\mathbf{Z}\\) odpowiadajÄ…cego najniÅ¼szej wartoÅ›ci wÅ‚asnej (\\(\\lambda = -1\\)). Jest to analogiczne do algorytmu Variational Quantum Eigensolver (VQE), ktÃ³ry sÅ‚uÅ¼y do znajdowania energii stanu podstawowego (najniÅ¼szej wartoÅ›ci wÅ‚asnej Hamiltonianu).\n\nPoÅ¼Ä…dany Stan: Stan \\(\\ket{1}\\), dla ktÃ³rego \\(\\mathbf{Z}\\ket{1} = -1\\ket{1}\\) (minimalna wartoÅ›Ä‡ oczekiwana).\nArchitektura Obwodu:\n\nqml.X(wires=0): Ustawia kubit w stan \\(\\ket{1}\\).\nqml.RY(theta, wires=0): Wprowadza parametr \\(\\theta\\), ktÃ³ry perturbuje stan \\(\\ket{1}\\).\n\nDziaÅ‚anie Optymalizatora: ZaczynajÄ…c z \\(\\theta\\) bliskim zera (gdzie \\(\\langle \\mathbf{Z} \\rangle \\approx -1\\)), optymalizator bardzo szybko dÄ…Å¼y do kÄ…ta \\(\\theta\\), ktÃ³ry minimalizuje perturbacjÄ™, czyli \\(\\theta \\to 0\\) lub \\(\\theta \\to 2\\pi\\). W tych punktach stan wraca do \\(\\ket{1}\\), a koszt osiÄ…ga swoje minimum \\(-1\\).\n\n\n\n\nğŸ’¡ Podsumowanie\nEksperyment ten demonstruje podstawowÄ… funkcjonalnoÅ›Ä‡ wariacyjnego uczenia maszynowego w kwantowym kontekÅ›cie, gdzie sam obwÃ³d kwantowy moÅ¼e definiowaÄ‡ funkcjÄ™ celu (Hamiltonian), a klasyczny optymalizator znajduje parametry \\(\\theta\\), ktÃ³re minimalizujÄ… tÄ™ funkcjÄ™. Jest to archetyp podejÅ›cia hybrydowego (klasyczny optymalizator + kwantowe przetwarzanie).\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Definicja UrzÄ…dzenia i Obwodu Kwantowego\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef qn(theta):\n    qml.X(wires=0)       # Ustawia stan na |1&gt; (expval Z = -1)\n    qml.RY(theta, wires=0) # ObrÃ³t wokÃ³Å‚ Y (zmienia expval Z)\n    return qml.expval(qml.PauliZ(0)) # Funkcja celu: minimalizujemy expval Z\n\n# 2. Inicjalizacja ParametrÃ³w i Optymalizatora\n# Zaczynamy z theta, gdzie koszt nie jest minimalny, aby zobaczyÄ‡ proces optymalizacji.\n# Np. theta = pi/2, gdzie RY(pi/2) na |1&gt; daje 1/sqrt(2)(|1&gt; - |0&gt;), a expval Z = 0.\ntheta_init = np.array([np.pi-0.01], requires_grad=True) \nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\n# Listy do przechowywania historii treningu\ncost_history = []\ntheta_history = []\n\n# 3. PÄ™tla Treningowa\ntheta = theta_init # Ustawienie poczÄ…tkowej wartoÅ›ci theta\n\nprint(\"--- RozpoczÄ™cie treningu ---\")\nfor epoch in range(epochs):\n    # Wykonaj krok optymalizatora i oblicz koszt\n    theta, cost = opt.step_and_cost(qn, theta)\n    \n    # Zapisz historiÄ™\n    cost_history.append(cost)\n    theta_history.append(theta[0]) # theta jest tablicÄ… numpy, bierzemy tylko wartoÅ›Ä‡\n\n    if epoch % 10 == 0 or epoch == epochs - 1:\n        print(f\"Epoka: {epoch:3d} | KÄ…t theta: {theta.item():.4f} | Koszt: {cost.item():.4f}\")\n\nprint(\"\\n--- Trening zakoÅ„czony âœ… ---\")\nprint(f\"KoÅ„cowy kÄ…t theta: {theta.item():.4f}\") \nprint(f\"KoÅ„cowy koszt: {cost.item():.4f}\")\n\n# 4. Wizualizacja Historii Treningu\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Wykres kosztu\naxes[0].plot(range(epochs), cost_history, color='blue', linewidth=2)\naxes[0].set_title('Ewolucja Funkcji Kosztu')\naxes[0].set_xlabel('Epoka')\n# POPRAWIONA LINIA: Poprawna skÅ‚adnia LaTeX dla wartoÅ›ci oczekiwanej\naxes[0].set_ylabel('Koszt ($\\langle \\\\mathbf{Z} \\\\rangle$)') \naxes[0].grid(True, linestyle='--', alpha=0.6)\naxes[0].axhline(-1, color='red', linestyle=':', label='Minimum teoretyczne (-1)', alpha=0.7)\naxes[0].legend()\n\n\n# Wykres kÄ…ta theta\naxes[1].plot(range(epochs), theta_history, color='green', linewidth=2)\naxes[1].set_title('Ewolucja KÄ…ta $\\\\theta$')\naxes[1].set_xlabel('Epoka')\naxes[1].set_ylabel('KÄ…t $\\\\theta$ (Radiany)')\naxes[1].grid(True, linestyle='--', alpha=0.6)\naxes[1].axhline(0, color='red', linestyle=':', label='WartoÅ›Ä‡ optymalna (0 lub $2\\\\pi$)', alpha=0.7)\naxes[1].axhline(2 * np.pi, color='red', linestyle=':', alpha=0.7)\naxes[1].legend()\n\nplt.tight_layout() # Teraz powinno dziaÅ‚aÄ‡ poprawnie\nplt.show()\n\n--- RozpoczÄ™cie treningu ---\nEpoka:   0 | KÄ…t theta: 3.1306 | Koszt: 1.0000\nEpoka:  10 | KÄ…t theta: 3.1131 | Koszt: 0.9997\nEpoka:  20 | KÄ…t theta: 3.0676 | Koszt: 0.9977\nEpoka:  30 | KÄ…t theta: 2.9502 | Koszt: 0.9849\nEpoka:  40 | KÄ…t theta: 2.6524 | Koszt: 0.9022\nEpoka:  50 | KÄ…t theta: 1.9802 | Koszt: 0.4770\nEpoka:  60 | KÄ…t theta: 1.0194 | Koszt: -0.4456\nEpoka:  70 | KÄ…t theta: 0.3899 | Koszt: -0.9082\nEpoka:  80 | KÄ…t theta: 0.1377 | Koszt: -0.9883\nEpoka:  90 | KÄ…t theta: 0.0481 | Koszt: -0.9986\nEpoka:  99 | KÄ…t theta: 0.0186 | Koszt: -0.9998\n\n--- Trening zakoÅ„czony âœ… ---\nKoÅ„cowy kÄ…t theta: 0.0186\nKoÅ„cowy koszt: -0.9998\n\n\n&lt;&gt;:53: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:53: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_46886/4293797474.py:53: SyntaxWarning: invalid escape sequence '\\l'\n  axes[0].set_ylabel('Koszt ($\\langle \\\\mathbf{Z} \\\\rangle$)')\n\n\n\n\n\n\n\n\n\n\\[\\mathbf{R_X}(\\theta) \\ket{0} = \\cos\\left(\\frac{\\theta}{2}\\right) \\ket{0} - i \\sin\\left(\\frac{\\theta}{2}\\right) \\ket{1} \\] Wtedy wartoÅ›Ä‡ oczekiwana operatora \\(\\mathbf{Z}\\) jest rÃ³wna: \\[\\langle \\mathbf{Z} \\rangle = \\cos(\\theta) \\] PoniewaÅ¼ funkcja \\(\\cos(\\theta)\\) jest ciÄ…gÅ‚a i przyjmuje wartoÅ›ci w zakresie \\([-1, 1]\\), jej wykres idealnie demonstruje to, co chcesz osiÄ…gnÄ…Ä‡.\n\nimport pennylane as qml \nimport pennylane.numpy as np \nimport matplotlib.pyplot as plt \n# 1. Definicja UrzÄ…dzenia (1 kubit) \ndev = qml.device('default.qubit', wires=1) \n# 2. Poprawiony ObwÃ³d Kwantowy z ParametrycznÄ… BramkÄ… RX \n@qml.qnode(dev) \ndef continuous_expval(theta): \n    \"\"\" ObwÃ³d stosuje rotacjÄ™ RX z kÄ…tem theta. Zwraca wartoÅ›Ä‡ oczekiwanÄ… operatora PauliZ, ktÃ³ra jest rÃ³wna cos(theta). \"\"\" \n    # Zastosowanie bramki obrotu RX \n    qml.RX(theta, wires=0) \n    return qml.expval(qml.PauliZ(wires=0)) \n# 3. Generowanie Danych \n# # Tworzymy wektor kÄ…tÃ³w theta od 0 do 2*pi (peÅ‚ny obrÃ³t) \ntheta_vals = np.linspace(0, 2 * np.pi, 100) \n# Obliczamy wartoÅ›Ä‡ oczekiwanÄ… dla kaÅ¼dego kÄ…ta \nexpval_vals = [continuous_expval(theta) for theta in theta_vals] \n# 4. Wizualizacja WynikÃ³w \nplt.figure(figsize=(8, 5)) \nplt.plot(theta_vals, expval_vals, label=r'$\\langle \\mathbf{Z} \\rangle = \\cos(\\theta)$', color='darkred', linewidth=3)\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.8) \n# Linia y=0 \nplt.axhline(1, color='green', linestyle=':', linewidth=0.8) # Linia y=1 \nplt.axhline(-1, color='green', linestyle=':', linewidth=0.8) # Linia y=-1 \n# Oznaczenia \nplt.title(r'WartoÅ›Ä‡ oczekiwana $\\langle \\mathbf{Z} \\rangle$ jako funkcja kÄ…ta obrotu $\\theta$') \nplt.xlabel(r'KÄ…t obrotu $\\theta$ (Radiany)') \nplt.ylabel(r'WartoÅ›Ä‡ Oczekiwana $\\langle \\mathbf{Z} \\rangle \\in [-1, 1]$') \nplt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], [r'$0$', r'$\\pi/2$', r'$\\pi$', r'$3\\pi/2$', r'$2\\pi$']) \nplt.grid(True, linestyle='-', alpha=0.3) \nplt.legend() \nplt.show()\n\n\n\n\n\n\n\n\n\n# --- Dodanie Oznaczenia dla Bramki X (theta = pi) ---\npi_angle = np.pi\npi_expval = continuous_expval(pi_angle) # WartoÅ›Ä‡ oczekiwana = -1.0\n\n# Pionowa linia w theta = pi\nplt.axvline(pi_angle, color='blue', linestyle='-.', linewidth=1.5, alpha=0.7)\n# Punkt na wykresie\nplt.plot(pi_angle, pi_expval, marker='o', color='blue', markersize=8, label=r'Punkt $\\mathbf{X} = \\mathbf{R_X}(\\pi)$')\n\n# Oznaczenia\nplt.title(r'WartoÅ›Ä‡ oczekiwana $\\langle \\mathbf{Z} \\rangle$ jako funkcja kÄ…ta obrotu $\\theta$')\nplt.xlabel(r'KÄ…t obrotu $\\theta$ (Radiany)')\nplt.ylabel(r'WartoÅ›Ä‡ Oczekiwana $\\langle \\mathbf{Z} \\rangle \\in [-1, 1]$')\nplt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], \n           [r'$0$', r'$\\pi/2$', r'$\\pi$', r'$3\\pi/2$', r'$2\\pi$'])\nplt.grid(True, linestyle='-', alpha=0.3)\nplt.legend(loc='lower left')\nplt.show()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Bramki kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw6.html",
    "href": "cwiczenia/cw6.html",
    "title": "Kwantowe sieci neuronowe w modelowaniu regresji",
    "section": "",
    "text": "import pennylane as qml\nimport pennylane.numpy as np\n\nsim = 'default.qubit'\ninputs = np.array([np.pi*0.7, np.pi/3], requires_grad=False)\n\nweights = np.array([1.2, 0.4, 0.3] , requires_grad=True)\ndev1 = qml.device(sim, wires=2, shots=1000)\n\n@qml.qnode(dev1)\ndef vqa(inputs, weights):\n    qml.RY(inputs[0], wires=0)\n    qml.RY(inputs[1], wires=1)\n    qml.Barrier()\n\n    qml.RX(weights[0], wires=0)\n    qml.RY(weights[1], wires=1)\n    qml.CZ(wires=[0,1])\n    qml.RY(weights[2], wires=1)\n    qml.Barrier()\n    return qml.probs(wires=range(2))\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\nqml.draw_mpl(vqa, scale=0.7, style='pennylane', decimals=2, level='device')(inputs, weights)\nvqa(inputs, weights), vqa(inputs, weights), vqa(inputs, weights)\n\n(array([0.16 , 0.238, 0.433, 0.169]),\n array([0.173, 0.236, 0.43 , 0.161]),\n array([0.163, 0.235, 0.432, 0.17 ]))\nqml.gradients.param_shift(vqa)(inputs, weights)\n\narray([[ 0.115 , -0.201 , -0.1855],\n       [ 0.1625,  0.1945,  0.191 ],\n       [-0.1895, -0.272 ,  0.284 ],\n       [-0.088 ,  0.2785, -0.2895]])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Kwantowe sieci neuronowe w modelowaniu regresji"
    ]
  },
  {
    "objectID": "cwiczenia/cw6.html#proste-modele-regresyjne",
    "href": "cwiczenia/cw6.html#proste-modele-regresyjne",
    "title": "Kwantowe sieci neuronowe w modelowaniu regresji",
    "section": "proste modele regresyjne",
    "text": "proste modele regresyjne\n\nX = np.linspace(0, 2*np.pi, 5) \nX.requires_grad = False\nY = np.sin(X) \n\nX_test = np.linspace(0.2, 2*np.pi+0.2, 5)\nY_test = np.sin(X_test)\n\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(X, Y, s=30, c='b', marker=\"s\", label='Train outputs')\nax1.scatter(X_test,Y_test, s=60, c='r', marker=\"o\", label='Test outputs')\nplt.xlabel(\"Inputs\")\nplt.ylabel(\"Outputs\")\n\nplt.legend(loc='upper right');\nplt.show()\n\n\n\n\n\n\n\n\n\ndev = qml.device('default.qubit', wires=1)\n@qml.qnode(dev)\ndef qreg(datapoint, params):\n    qml.RX(datapoint, wires=0)\n    qml.RY(params, wires=0)\n    #qml.Rot(params[0], params[1], params[2], wires=0)\n    return qml.expval(qml.PauliZ(wires=0))\n\n\nqml.draw_mpl(qreg)([0.1],[0.2])\n\n\n\n\n\n\n\n\n\n[qreg(x, 0.5)  for x in X] \n\n[tensor(0.87758256, requires_grad=True),\n tensor(1.11022302e-16, requires_grad=True),\n tensor(-0.87758256, requires_grad=True),\n tensor(-1.11022302e-16, requires_grad=True),\n tensor(0.87758256, requires_grad=True)]\n\n\n\ndef loss_func(predictions):\n \n    total_losses = 0\n    for i in range(len(Y)):\n        output = Y[i]\n        prediction = predictions[i]\n        loss = (prediction - output)**2\n        total_losses += loss\n    return total_losses\n\n\ndef cost_fn(params):\n    predictions = [qreg(x, params)  for x in X]\n    cost = loss_func(predictions)\n    return cost\n\n\nopt = qml.GradientDescentOptimizer(stepsize=0.2)\n\nparams = np.array([0.4], requires_grad=True)\n\nfor i in range(100):\n    params, prev_cost = opt.step_and_cost(cost_fn, params)\n    if i%10 == 0:\n        print(f'Step = {i} Cost = {cost_fn(params)}')\n\nStep = 0 Cost = [3.36513589]\nStep = 10 Cost = [2.]\nStep = 20 Cost = [2.]\nStep = 30 Cost = [2.]\nStep = 40 Cost = [2.]\nStep = 50 Cost = [2.]\nStep = 60 Cost = [2.]\nStep = 70 Cost = [2.]\nStep = 80 Cost = [2.]\nStep = 90 Cost = [2.]\n\n\n\ntest_predictions = []\nfor x_test in X_test:\n    prediction = qreg(x_test, params)\n    test_predictions.append(prediction)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(X, Y, s=30, c='b', marker=\"s\", label='Train outputs')\nax1.scatter(X_test,Y_test, s=60, c='r', marker=\"o\", label='Test outputs')\nax1.scatter(X_test,test_predictions, s=30, c='k', marker=\"x\", label='Test predicitons')\nplt.xlabel(\"Inputs\")\nplt.ylabel(\"Outputs\")\nplt.title(\"QML results\")\n\nplt.legend(loc='upper right');\nplt.show()\n\n\n\n\n\n\n\n\n\nQNN template\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\nn_qubits = 3\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef quantum_regression(datapoint, params):\n    qml.AngleEmbedding(datapoint, wires=range(n_qubits))\n    qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n\n\nshape = qml.StronglyEntanglingLayers.shape(n_layers=2, n_wires=n_qubits)\nshape\n\n(2, 3, 3)\n\n\n\ninputs = [0.3]\nparams = np.random.random(size=shape)\n\nprint(inputs, params)\n\n[0.3] [[[0.11282513 0.72564779 0.21595164]\n  [0.98078925 0.204768   0.712829  ]\n  [0.35530699 0.42168993 0.61578077]]\n\n [[0.51455643 0.24840436 0.76422136]\n  [0.07528059 0.41880629 0.36329945]\n  [0.47308219 0.26640078 0.63166918]]]\n\n\n\nimport matplotlib.pyplot as plt\nqml.drawer.use_style(\"pennylane_sketch\")\nfig, ax = qml.draw_mpl(quantum_regression, decimals=2,level='device')(inputs, params)\nplt.show()\n\nMatplotlib is building the font cache; this may take a moment.\n\n\n\n\n\n\n\n\n\n\ndef cost_fn(params):\n    predictions = [quantum_regression([x], params)  for x in X]\n    cost = loss_func(predictions)\n    return cost\n\n\nfor i in range (100):\n    params, prev_cost = opt.step_and_cost(cost_fn, params)\n    if i%10 == 0:\n        print(f'Step = {i} Cost = {cost_fn(params)}')\n\nStep = 0 Cost = 1.6750134901195395\nStep = 10 Cost = 0.010468152498413093\nStep = 20 Cost = 0.0024648568256249092\nStep = 30 Cost = 0.0010817200905668901\nStep = 40 Cost = 0.0006070350983908127\nStep = 50 Cost = 0.0003888980331095931\nStep = 60 Cost = 0.0002706936774535423\nStep = 70 Cost = 0.0001994395953619091\nStep = 80 Cost = 0.0001531545104738453\nStep = 90 Cost = 0.0001213765221163838\n\n\n\ntest_predictions = []\nfor x_test in X_test:\n    prediction = quantum_regression([x_test],params)\n    test_predictions.append(prediction)\n\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(X, Y, s=30, c='b', marker=\"s\", label='Train outputs')\nax1.scatter(X_test,Y_test, s=60, c='r', marker=\"o\", label='Test outputs')\nax1.scatter(X_test,test_predictions, s=30, c='k', marker=\"x\", label='Test predicitons')\nplt.xlabel(\"Inputs\")\nplt.ylabel(\"Outputs\")\nplt.title(\"QML results\")\n\nplt.legend(loc='upper right')\nplt.show()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Kwantowe sieci neuronowe w modelowaniu regresji"
    ]
  },
  {
    "objectID": "cwiczenia/cw6.html#architektury-modeli-qtsa",
    "href": "cwiczenia/cw6.html#architektury-modeli-qtsa",
    "title": "Kwantowe sieci neuronowe w modelowaniu regresji",
    "section": "Architektury modeli QTSA",
    "text": "Architektury modeli QTSA\nKwantowa analiza szeregÃ³w czasowych (QTSA) opiera siÄ™ na wariacyjnych obwodach kwantowych zaprojektowanych do aproksymowania funkcji mapujÄ…cych wejÅ›ciowe dane zaleÅ¼ne od czasu na przewidywane wyniki. KaÅ¼dy model QTSA jest rozszerzeniem generycznej Kwantowej Sieci Neuronowej (QNN), ktÃ³rÄ… moÅ¼na wyraziÄ‡ jako sparametryzowany obwÃ³d kwantowy:\n\\[\nU(x,\\theta)=U_{ansatz}(\\theta)â‹…U_{enc}(x),\n\\] gdzie \\(x\\) oznacza klasyczne dane wejÅ›ciowe, \\(\\theta\\) to trenowalne parametry, \\(U_{enc}\\) koduje klasyczne dane w stany kwantowe, a \\(U_{ansatz}\\) stosuje trenowalne transformacje.\nPrzewidywanie uzyskuje siÄ™ poprzez pomiar wartoÅ›ci oczekiwanej obserwabli \\(M\\):\n\\[\ny = \\bra{0}^{\\otimes q} U^\\dagger M U \\ket{0}^{\\otimes q},\n\\]\ngdzie \\(q\\) jest liczbÄ… kubitÃ³w, a \\(M=\\sum_{j=1}^{q} Z_j\\), chyba Å¼e zaznaczono inaczej.\n\nModele dopasowania krzywych\nModele QTSA dopasowania krzywych aproksymujÄ… funkcjÄ™ ciÄ…gÅ‚Ä… \\(f:R\\to R\\), przewidujÄ…c wartoÅ›ci szeregu czasowego w pojedynczych krokach czasowych.\nCharakteryzujÄ… siÄ™ one wielokrotnym ponownym Å‚adowaniem danych wejÅ›ciowych \\(\\{x_i\\}\\) do obwodu w celu uchwycenia struktury sekwencyjnej.\nRozwaÅ¼ane sÄ… trzy warianty:\n\nSzeregowe Ponowne Åadowanie (PQFT-Serial)\n\nDla pojedynczego kubitu, model naprzemiennie stosuje sparametryzowane unitarne ansatze i powtarzane unitarne kodowania. Unitarne kodowanie jest wybierane jako rotacja wokÃ³Å‚ osi \\(X\\), a kaÅ¼de unitarne ansatze jest uniwersalnÄ… bramkÄ… jednokubitowÄ… \\(U(\\theta)\\) sparametryzowanÄ… trzema trenowalnymi kÄ…tami.\nNiech \\(x \\in \\mathbb{R}\\) bÄ™dzie skalarnÄ… wartoÅ›ciÄ… wejÅ›ciowÄ… (np. pojedynczy krok czasowy), a \\(n\\) oznacza liczbÄ™ warstw ponownego Å‚adowania danych. KaÅ¼da bramka ansatze \\(U(\\theta_k)\\) jest zdefiniowana przez trzy parametry \\(\\theta_k=(\\theta_k^{(1)},\\theta_k^{(2)},\\theta_k^{3})\\), a peÅ‚ny zbiÃ³r parametrÃ³w to \\(\\theta = \\{\\theta_1, \\dots, \\theta_{n+1} \\}\\).\nWynikowy stan kwantowy po przetworzeniu wejÅ›cia x to: \\[\n\\ket{\\psi(x, \\theta)} = ( \\prod_{j=1}^{n} [U(\\theta_j) \\cdot R_X(x)]) \\cdot U(\\theta_{n+1})\\ket{0}\n\\] gdzie \\(R_X(x) = exp (âˆ’i x X/2)\\).\n\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# ==== 1. Dane ====\nx_all = np.linspace(-6, 6, 90)\ny_all = (np.sin(5.0 * x_all) + 0.5 * np.sin(8.0 * x_all)) / 4 + 0.5\n\n# ==== 2. PodziaÅ‚ ====\nx_train = x_all[:70]\ny_train = y_all[:70]\n\nx_test = x_all[70:]\ny_test = y_all[70:]\n\n# ==== 3. Wykres ====\nplt.figure(figsize=(10, 5))\nplt.plot(x_all, y_all, label=\"Target function\", color=\"gray\", linestyle=\"--\")\nplt.scatter(x_train, y_train, label=\"Train\", color=\"blue\", s=25)\nplt.scatter(x_test, y_test, label=\"Test\", color=\"orange\", s=25)\n\n# Linia podziaÅ‚u\nplt.axvline(x=x_train[-1], color=\"black\", linestyle=\":\", label=\"Train/Test split\")\n\n#plt.legend()\nplt.title(\"Synthetic Time Series â€“ Train/Test Split\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\nscaling = 1\n\ndef S(x):\n    \"\"\" kodowanie danych \n    x - wartosc skalarna jednej zmiennej - do przewidywania szeregow czasowych \n    czyli najczÄ™Å›ciej to bedzie reprezentowaÄ‡ oÅ› czasu.\n    scaling - trochÄ™ nie wiem po co ale zakÅ‚adam, ze pozwoli ustaliÄ‡ zakres od 0 do 2pi bo kÄ…ty siÄ™ powtarzajÄ…\n    \"\"\"\n    qml.RX(scaling * x, wires=0)\n\ndef W(theta):\n    \"\"\"\n    trenowalny blok zalezacy od 3 katÃ³w \n    wymusza aby theta bylo przynajmniej listÄ… (krotkÄ…) posiadajÄ…cÄ… 3 elementy\n    minimalny model z zaladowaniem danych [[1.2,0.3,4.5],[1.2,0.3,4.5]] - musi zawierac przynajmniej dwie listy\n    \"\"\"\n    qml.Rot(theta[0], theta[1], theta[2], wires=0)\n\n@qml.qnode(dev)\ndef serial_quantum_model(weights, x):\n    for theta in weights[:-1]:\n        W(theta)\n        S(x)\n    W(weights[-1])\n\n    return qml.expval(qml.PauliZ(wires=0))\n\n\nr = 4\nweights = (np.random.random(size=(r+1, 3), requires_grad=True))\n\n\nweights\n\ntensor([[0.22811526, 0.36255741, 0.67154863],\n        [0.97108591, 0.36983219, 0.41593129],\n        [0.17374648, 0.56058029, 0.77977962],\n        [0.00867151, 0.80233344, 0.33836025],\n        [0.68861766, 0.19787254, 0.30560425]], requires_grad=True)\n\n\n\nqml.draw_mpl(serial_quantum_model, decimals=2, level=\"device\", style='sketch')(weights, 1.0)\n\n\n\n\n\n\n\n\n\nX = np.linspace(-6,6, 200)\nresults = [serial_quantum_model(weights=weights, x=x_) for x_ in X]\n\n\nplt.plot(X,results)\n\n\n\n\n\n\n\n\n\n# Koszt: MSE\ndef cost(weights, x_data, y_data):\n    preds = np.array([serial_quantum_model(weights, x) for x in x_data])\n    return np.mean((preds - y_data) ** 2)\n\nr = 20\nweights = (np.random.random(size=(r+1, 3), requires_grad=True))\n\n# Optymalizacja\nopt = qml.AdamOptimizer(stepsize=0.1)\nepochs = 100\n\nfor epoch in range(epochs):\n    weights = opt.step(lambda w: cost(w, x_train, y_train), weights)\n    if epoch % 10 == 0:\n        loss = cost(weights, x_train, y_train)\n        print(f\"Epoch {epoch}: loss = {loss:.6f}\")\n\nEpoch 0: loss = 0.210831\nEpoch 10: loss = 0.023146\nEpoch 20: loss = 0.009356\nEpoch 30: loss = 0.005972\nEpoch 40: loss = 0.003265\nEpoch 50: loss = 0.001862\nEpoch 60: loss = 0.001238\nEpoch 70: loss = 0.000985\nEpoch 80: loss = 0.000760\nEpoch 90: loss = 0.000569\n\n\n\n# Predykcje\ny_pred_train = np.array([serial_quantum_model(weights, x ) for x in x_train])\ny_pred_test = np.array([serial_quantum_model(weights, x) for x in x_test])\n\n# Wykres\nplt.figure(figsize=(10, 5))\nplt.plot(x_all, y_all, label=\"Ground Truth\", color=\"black\", linewidth=1)\nplt.scatter(x_train, y_pred_train, color=\"blue\", label=\"Train prediction\", s=12)\nplt.scatter(x_test, y_pred_test, color=\"red\", label=\"Test prediction\", s=12)\nplt.axvline(x=x_train[-1], color=\"gray\", linestyle=\"--\", label=\"Train/Test split\")\nplt.title(\"Serial Model\")\nplt.grid(True)\nplt.legend()\nplt.show()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Kwantowe sieci neuronowe w modelowaniu regresji"
    ]
  },
  {
    "objectID": "cwiczenia/cw6.html#parallel",
    "href": "cwiczenia/cw6.html#parallel",
    "title": "Kwantowe sieci neuronowe w modelowaniu regresji",
    "section": "Parallel",
    "text": "Parallel\n\n# Parametry systemu\nn_qubits = 3\nn_ansatz_layers = 1\nm_ansatz_layers = 2\n\ndef S(x):\n    # RX(x) na wszystkich qubitach\n    for i in range(n_qubits):\n        qml.RX(x, wires=i)\n\ndef U_ent(weights):\n    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\ndef U_q(weights):\n    for idx, w in enumerate(weights):\n        qml.Rot(w[0], w[1], w[2], wires=idx)\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n\n@qml.qnode(device=dev)\ndef parralel_quantum_model(weights, x):\n    \n    U_ent(weights[:n_ansatz_layers])\n\n    qml.Barrier()\n    \n    U_q(weights[n_ansatz_layers+1])\n    \n    qml.Barrier()\n    \n    S(x)\n    \n    qml.Barrier()\n\n    U_ent(weights[n_ansatz_layers+1:-1])\n\n    qml.Barrier()\n    \n    U_q(weights[-1])\n\n    obs = qml.prod(*[qml.PauliZ(i) for i in range(n_qubits)])\n\n    return qml.expval(obs)\n\n\nweights = (np.random.random(size=(n_ansatz_layers+1+m_ansatz_layers+1, n_qubits, 3), requires_grad=True))\n\n\nprint(qml.draw_mpl(parralel_quantum_model, level=\"device\", decimals=2)(weights, np.pi))\n\n(&lt;Figure size 1800x400 with 1 Axes&gt;, &lt;Axes: &gt;)\n\n\n\n\n\n\n\n\n\n\nX = np.linspace(-6,6, 200)\nresults = [parralel_quantum_model(weights, x_) for x_ in X]\n\n\nplt.plot(X,results)\n\n\n\n\n\n\n\n\n\n# Koszt: MSE\ndef cost(weights, x_data, y_data):\n    preds = np.array([parralel_quantum_model(weights, x) for x in x_data])\n    return np.mean((preds - y_data) ** 2)\n\n\n# Optymalizacja\nopt = qml.AdamOptimizer(stepsize=0.1)\nepochs = 100\n\nfor epoch in range(epochs):\n    weights = opt.step(lambda w: cost(w, x_train, y_train), weights)\n    if epoch % 10 == 0:\n        loss = cost(weights, x_train, y_train)\n        print(f\"Epoch {epoch}: loss = {loss:.6f}\")\n\nEpoch 0: loss = 0.048043\nEpoch 10: loss = 0.038311\nEpoch 20: loss = 0.038768\nEpoch 30: loss = 0.038308\nEpoch 40: loss = 0.037880\nEpoch 50: loss = 0.037783\nEpoch 60: loss = 0.037730\nEpoch 70: loss = 0.037712\nEpoch 80: loss = 0.037714\nEpoch 90: loss = 0.037711\n\n\n\n# Predykcje\ny_pred_train = np.array([parralel_quantum_model(weights, x ) for x in x_train])\ny_pred_test = np.array([parralel_quantum_model(weights, x) for x in x_test])\n\n# Wykres\nplt.figure(figsize=(10, 5))\nplt.plot(x_all, y_all, label=\"Ground Truth\", color=\"black\", linewidth=1)\nplt.scatter(x_train, y_pred_train, color=\"blue\", label=\"Train prediction\", s=12)\nplt.scatter(x_test, y_pred_test, color=\"red\", label=\"Test prediction\", s=12)\nplt.axvline(x=x_train[-1], color=\"gray\", linestyle=\"--\", label=\"Train/Test split\")\nplt.title(\"Serial Model\")\nplt.grid(True)\nplt.legend()\nplt.show()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Kwantowe sieci neuronowe w modelowaniu regresji"
    ]
  },
  {
    "objectID": "cwiczenia/cw3z.html",
    "href": "cwiczenia/cw3z.html",
    "title": "pierwsze modele kwantowe",
    "section": "",
    "text": "from sklearn.datasets import make_blobs\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nX, y = make_blobs(n_samples=100, centers=2, random_state=6)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nsvm_clf = Pipeline(\n    [\n        ('scaler', StandardScaler()),\n        (\"linear_svc\", LinearSVC(C=1, loss='hinge'))\n    ]\n)\n\nsvm_clf.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('linear_svc', LinearSVC(C=1, loss='hinge'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('scaler', ...), ('linear_svc', ...)]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    LinearSVC?Documentation for LinearSVC\n        \n            \n                Parameters\n                \n\n\n\n\npenaltyÂ \n'l2'\n\n\n\nlossÂ \n'hinge'\n\n\n\ndualÂ \n'auto'\n\n\n\ntolÂ \n0.0001\n\n\n\nCÂ \n1\n\n\n\nmulti_classÂ \n'ovr'\n\n\n\nfit_interceptÂ \nTrue\n\n\n\nintercept_scalingÂ \n1\n\n\n\nclass_weightÂ \nNone\n\n\n\nverboseÂ \n0\n\n\n\nrandom_stateÂ \nNone\n\n\n\nmax_iterÂ \n1000\nprint(f\"Test acc: {svm_clf.score(X_test, y_test):.2f}\")\n\nTest acc: 1.00\ndef plot_svm_pipeline(pipeline, X, y):\n    scaler = pipeline.named_steps['scaler']\n    svc = pipeline.named_steps['linear_svc']\n    X_scaled = scaler.transform(X)\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='autumn')\n    ax = plt.gca()\n\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = svc.decision_function(xy).reshape(XX.shape)\n\n    ax.contour(XX, YY, Z, color='k', levels=[-1,0,1], linestyles=['--','-','--'])\n\n    plt.title(\"SVM Decision Boundry\")\n    plt.show()\n\nplot_svm_pipeline(svm_clf, X, y)\n\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_31970/3794771256.py:17: UserWarning: The following kwargs were not used by contour: 'color'\n  ax.contour(XX, YY, Z, color='k', levels=[-1,0,1], linestyles=['--','-','--'])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "pierwsze modele kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3z.html#kernel-trick",
    "href": "cwiczenia/cw3z.html#kernel-trick",
    "title": "pierwsze modele kwantowe",
    "section": "Kernel trick",
    "text": "Kernel trick\nDla prawdziwych danych trudno oczekiwaÄ‡ aby byÅ‚y one liniowo separowalne.\nDlatego jednym z rozwiÄ…zaÅ„ jest stworzenie odwzorowania do wyÅ¼ej wymiarowej przestrzeni tak by dane w niej byÅ‚y juÅ¼ liniowo separowalne. Obliczenie takiej transformacji dla dowolnych danych jest bardzo trudne, dlatego moÅ¼emy zastosowaÄ‡ tzw kernel trick. Potrzebujemy tylko obliczyÄ‡ iloczyn skalarny: \\[ K(x,x') = &lt;\\phi(x), \\phi(x')&gt;\\] bez jawnego wyznaczania \\(\\phi\\).\n\nx, xâ€™ wektory wejÅ›ciowe z oryginalnej przestrzeni\n\\(\\phi(x)\\) odwzorowanie do przestrzeni o wyÅ¼szym wymiarze\n\\(K(x, x')\\) funkcja jÄ…drowa - kernel function - oblicza iloczy skalarny w zadanej przestrzeni.\n\n\nLinear - \\(K(x, x') = x^{T}x'\\)\nPolynomial - \\(K(x,x') = (x^{T}x' +c)^d\\)\nRBF - \\(K(x,x') = exp(-\\gamma \\, |x-x'|^2)\\)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_moons\nfrom sklearn.svm import SVC\n\nX,y = make_moons(n_samples=200, noise=0.2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=44)\n\npoly_svm_clf = Pipeline([\n  #  ('polu_features', PolynomialFeatures(degree=3)),\n    ('scaler', StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=1, loss='hinge'))\n])\n\npoly_svm_clf.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('linear_svc', LinearSVC(C=1, loss='hinge'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('scaler', ...), ('linear_svc', ...)]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    LinearSVC?Documentation for LinearSVC\n        \n            \n                Parameters\n                \n\n\n\n\npenaltyÂ \n'l2'\n\n\n\nlossÂ \n'hinge'\n\n\n\ndualÂ \n'auto'\n\n\n\ntolÂ \n0.0001\n\n\n\nCÂ \n1\n\n\n\nmulti_classÂ \n'ovr'\n\n\n\nfit_interceptÂ \nTrue\n\n\n\nintercept_scalingÂ \n1\n\n\n\nclass_weightÂ \nNone\n\n\n\nverboseÂ \n0\n\n\n\nrandom_stateÂ \nNone\n\n\n\nmax_iterÂ \n1000\n\n\n\n\n            \n        \n    \n\n\n\nprint(f\"Test acc: {poly_svm_clf.score(X_test, y_test):.2f}\")\n\nTest acc: 0.82\n\n\n\nlinear_svm = SVC(kernel='linear', C=1)\nlinear_svm.fit(X_train, y_train)\ny_pred_linear = linear_svm.predict(X_test)\nacc_linear = accuracy_score(y_test, y_pred_linear)\n\n\nrbf_svm = SVC(kernel='rbf', C=1, gamma='scale')\nrbf_svm.fit(X_train, y_train)\ny_pred_rbf = rbf_svm.predict(X_test)\nacc_rbf = accuracy_score(y_test, y_pred_rbf)\n\n\ndef plot_decision_boundry(model, X, y, title):\n    h = 0.02\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()+1\n    y_min, y_max = X[:, 1].min(), X[:, 1].max()+1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()\n\n\nplot_decision_boundry(linear_svm, X_test, y_test, \"linear SVM\")\nplot_decision_boundry(rbf_svm, X_test, y_test, \"RBF SVM\")",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "pierwsze modele kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3z.html#idea-swap-testu",
    "href": "cwiczenia/cw3z.html#idea-swap-testu",
    "title": "pierwsze modele kwantowe",
    "section": "Idea swap testu",
    "text": "Idea swap testu\nSwap test sÅ‚uÅ¼y do obliczania wartoÅ›ci\n\\[ |\\langle \\psi |\\phi \\rangle |^2 \\]\nczyli kwadratu moduÅ‚u iloczynu skalarnego dwÃ³ch stanÃ³w kwantowych \\(|\\psi \\rangle\\) i \\(|\\phi \\rangle\\) .\n\nğŸ”§ ObwÃ³d swap testu\nSwap test uÅ¼ywa dodatkowego kubitu kontrolnego oraz bramki SWAP\nKontrolny kubit realizowany jest w stanie \\(|0\\rangle\\).\n\\[ \\ket{\\psi_0} = \\ket{0} \\otimes \\ket{\\psi} \\otimes \\ket{\\phi} \\]\n\n\nğŸ›ï¸ Jak to dziaÅ‚a\n\nZastosuj Hadamarda (zamiana bazy) na kontrolny (ancilla) kubit \\[ \\ket{\\psi_1} = (\\ket{0} + \\ket{1}) \\otimes \\ket{\\psi} \\otimes \\ket{\\phi} \\]\nZastosuj CSWAP (3 kubitowa bramka - controll = ancilla)\nZastosuj Hadamarda (powrÃ³t do bazy)\nPomiar ancilla kubitu.\n\nPrawdopodobieÅ„stwo, Å¼e kontrolny kubit da wynik \\(0\\), wynosi: \\[P(0)=\\frac{1+|\\langle \\psi |\\phi \\rangle |^2}{2}\\]\nPrawdopodobieÅ„stwo, Å¼e kontrolny kubit da wynik 1, wynosi: \\[P(1)=\\frac{1-|\\langle \\psi |\\phi \\rangle |^2}{2}\\]\nDziÄ™ki temu, mierzÄ…c kontrolny kubit, moÅ¼emy wyznaczyÄ‡ overlap miÄ™dzy stanami.\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\ndev_test = qml.device('default.qubit', wires=['ancilla','phi','psi'], shots=5000)\n\n@qml.qnode(dev_test)\ndef swap_test():\n    qml.Hadamard(wires='ancilla')\n    \n    qml.X(wires=['phi'])\n    qml.Hadamard(wires=['psi'])\n\n    qml.CSWAP(wires=['ancilla', 'phi', 'psi'])\n    qml.Hadamard(wires='ancilla')\n    return qml.sample(wires='ancilla')\n\nres = swap_test()\n\nprint(f\"P(0) = {np.mean(res==0)}, P(1) = {np.mean(res == 1)}\")\nprint(f\"{2*np.mean(res==0) - 1}\")\n\nP(0) = 0.746, P(1) = 0.254\n0.492\n\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\nsprawdzenie\n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef phi():\n    qml.X(wires=0)\n    return qml.state()\n\n@qml.qnode(dev)\ndef psi():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\ndef theory(phi, psi):\n    inner = np.vdot(phi, psi)\n    return float(np.abs(inner)**2)\n\ntheory(psi(), phi())\n\n0.4999999999999999",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "pierwsze modele kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3z.html#quantum-embedding",
    "href": "cwiczenia/cw3z.html#quantum-embedding",
    "title": "pierwsze modele kwantowe",
    "section": "Quantum Embedding",
    "text": "Quantum Embedding\nKwantowy Embedding reprezentuje klasyczne dane jako stan (wektor) w przestrzeni Hilberta. Odwzorowanie, ktÃ³re generuje embedding nazywamy quantum feature map.\nFeature map: \\(\\phi: X \\to F\\) gdzie \\(F\\) to nowa przestrzeÅ„ Hilberta stanÃ³w. \\[ x \\to \\ket{\\phi(x)} \\]\nW naszym przypadku to odwzorowanie realizujÄ… \\(U_{\\phi}(x)\\) macierze kodowania kÄ…towego. \\[ \\ket{0} \\to U_{\\phi}(x)\\ket{0} \\]\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, np.pi))\nX_scaled = scaler.fit_transform(X)\n\ny_scaled = 2 * y -1 \n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n\nRozwaÅ¼my model kwantowy w postaci: \\[\nf(x) = \\braket{\\phi(x) | M | \\phi{x} }\n\\]\nModel moÅ¼e byÄ‡ realizowany jako wariacyjny obwÃ³d kwantowy.\nZamiast jednak trenowaÄ‡ parametry dla takiego obwodu moÅ¼emy wykorzystaÄ‡ kwantowy kernel ktÃ³ry realizuje siÄ™ przez SWAP test.\nZamiast SWAP testu moÅ¼emy wykorzystaÄ‡ inny obwÃ³d SzczegÃ³Å‚y tutaj\n\nfrom pennylane.templates import AngleEmbedding\n\n\nn_qubits = 2\ndev_kernel = qml.device('lightning.qubit', wires= n_qubits)\n\n\n\nprojector = np.zeros((2 ** n_qubits, 2 ** n_qubits))\nprojector[0, 0] = 1\n\n\n@qml.qnode(dev_kernel)\ndef kernel(x1, x2):\n    AngleEmbedding(x1, wires=range(n_qubits))\n    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))\n\n\nX_train = np.array(X_train, requires_grad=False)\nX_test = np.array(X_test, requires_grad=False)\n\ny_train = np.array(y_train, requires_grad=False)\ny_test = np.array(y_test, requires_grad=False)\n\n\nkernel(X_train[0], X_train[0]), kernel(X_test[0], X_test[1])\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n(array(1.), array(0.84505579))\n\n\n\ndef kernel_matrix(A, B):\n    return np.array([[kernel(a,b) for b in B] for a in A])\n\nsvm = SVC(kernel=kernel_matrix).fit(X_train, y_train)\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n\npredictions = svm.predict(X_test)\n\n\nprint(f\"model qsvm {accuracy_score(predictions, y_test):.4f}\")\n\nmodel qsvm 0.8600\n\n\n\nsvm.predict(X_test[:4]), y_test[:4]\n\n\nZadanie\nWygeneruj kernel oraz wykonaj model QSVM dla poprzenich danych :\ndef zz_feature_map(x, wires=[0,1]):\n    for i in range(len(wires)):\n        qml.Hadamard(wires=wires[i])\n\n    for i in range(len(wires)):\n        qml.RX(2 * x[i], wires=wires[i])\n\n    theta = (np.pi - x[0])* (np.pi - x[i])\n    qml.CNOT(wires=[wires[0], wires[1]])\n    qml.RZ(2 * theta, wires=wires[1])\n    qml.CNOT(wires=[wires[0], wires[1]])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "pierwsze modele kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3z.html#kwantowa-regresja",
    "href": "cwiczenia/cw3z.html#kwantowa-regresja",
    "title": "pierwsze modele kwantowe",
    "section": "Kwantowa regresja",
    "text": "Kwantowa regresja\n\nX = np.linspace(0, 2*np.pi, 5) \nX.requires_grad = False\nY = np.sin(X) \n\nX_test = np.linspace(0.2, 2*np.pi+0.2, 5)\nY_test = np.sin(X_test)\n\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(X, Y, s=30, c='b', marker=\"s\", label='Train outputs')\nax1.scatter(X_test,Y_test, s=60, c='r', marker=\"o\", label='Test outputs')\nplt.xlabel(\"Inputs\")\nplt.ylabel(\"Outputs\")\n\nplt.legend(loc='upper right');\nplt.show()\n\n\n\n\n\n\n\n\n\ndev = qml.device('default.qubit', wires=1)\n@qml.qnode(dev)\ndef qreg(datapoint, params):\n    qml.RX(datapoint, wires=0)\n    qml.RY(params, wires=0)\n    #qml.Rot(params[0], params[1], params[2], wires=0)\n    return qml.expval(qml.PauliZ(wires=0))\n\n\nqml.draw_mpl(qreg)([0.1],[0.2])\n\n\n\n\n\n\n\n\n\n[qreg(x, 0.5)  for x in X] \n\n[tensor(0.87758256, requires_grad=True),\n tensor(1.11022302e-16, requires_grad=True),\n tensor(-0.87758256, requires_grad=True),\n tensor(-1.11022302e-16, requires_grad=True),\n tensor(0.87758256, requires_grad=True)]\n\n\n\ndef loss_func(predictions):\n \n    total_losses = 0\n    for i in range(len(Y)):\n        output = Y[i]\n        prediction = predictions[i]\n        loss = (prediction - output)**2\n        total_losses += loss\n    return total_losses\n\n\ndef cost_fn(params):\n    predictions = [qreg(x, params)  for x in X]\n    cost = loss_func(predictions)\n    return cost\n\n\nopt = qml.GradientDescentOptimizer(stepsize=0.2)\n\nparams = np.array([0.4], requires_grad=True)\n\nfor i in range(100):\n    params, prev_cost = opt.step_and_cost(cost_fn, params)\n    if i%10 == 0:\n        print(f'Step = {i} Cost = {cost_fn(params)}')\n\nStep = 0 Cost = [3.36513589]\nStep = 10 Cost = [2.]\nStep = 20 Cost = [2.]\nStep = 30 Cost = [2.]\nStep = 40 Cost = [2.]\nStep = 50 Cost = [2.]\nStep = 60 Cost = [2.]\nStep = 70 Cost = [2.]\nStep = 80 Cost = [2.]\nStep = 90 Cost = [2.]\n\n\n\ntest_predictions = []\nfor x_test in X_test:\n    prediction = qreg(x_test, params)\n    test_predictions.append(prediction)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(X, Y, s=30, c='b', marker=\"s\", label='Train outputs')\nax1.scatter(X_test,Y_test, s=60, c='r', marker=\"o\", label='Test outputs')\nax1.scatter(X_test,test_predictions, s=30, c='k', marker=\"x\", label='Test predicitons')\nplt.xlabel(\"Inputs\")\nplt.ylabel(\"Outputs\")\nplt.title(\"QML results\")\n\nplt.legend(loc='upper right');\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\nn_qubits = 3\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef quantum_regression(datapoint, params):\n    qml.AngleEmbedding(datapoint, wires=range(n_qubits))\n    qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n\n\nshape = qml.StronglyEntanglingLayers.shape(n_layers=2, n_wires=n_qubits)\nshape\n\n(2, 3, 3)\n\n\n\ninputs = [0.3]\nparams = np.random.random(size=shape)\n\nprint(inputs, params)\n\n[0.3] [[[0.98800917 0.56995002 0.59715563]\n  [0.28215755 0.92825046 0.79220626]\n  [0.71667081 0.34879264 0.79418002]]\n\n [[0.45923874 0.2484918  0.7642648 ]\n  [0.34818393 0.77342551 0.71944554]\n  [0.79462278 0.84043035 0.13915954]]]\n\n\n\nimport matplotlib.pyplot as plt\nqml.drawer.use_style(\"pennylane_sketch\")\nfig, ax = qml.draw_mpl(quantum_regression, decimals=2,level='device')(inputs, params)\nplt.show()\n\nMatplotlib is building the font cache; this may take a moment.\n\n\n\n\n\n\n\n\n\n\ndef cost_fn(params):\n    predictions = [quantum_regression([x], params)  for x in X]\n    cost = loss_func(predictions)\n    return cost\n\n\nfor i in range (100):\n    params, prev_cost = opt.step_and_cost(cost_fn, params)\n    if i%10 == 0:\n        print(f'Step = {i} Cost = {cost_fn(params)}')\n\nStep = 0 Cost = 1.976972461237902\nStep = 10 Cost = 0.018757250387467495\nStep = 20 Cost = 0.003508288809442019\nStep = 30 Cost = 0.0014923572177364891\nStep = 40 Cost = 0.0008485647250635812\nStep = 50 Cost = 0.0005593337594326065\nStep = 60 Cost = 0.0004030880858351285\nStep = 70 Cost = 0.0003082942511157639\nStep = 80 Cost = 0.0002459964786782721\nStep = 90 Cost = 0.00020259043801405815\n\n\n\ntest_predictions = []\nfor x_test in X_test:\n    prediction = quantum_regression([x_test],params)\n    test_predictions.append(prediction)\n\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(X, Y, s=30, c='b', marker=\"s\", label='Train outputs')\nax1.scatter(X_test,Y_test, s=60, c='r', marker=\"o\", label='Test outputs')\nax1.scatter(X_test,test_predictions, s=30, c='k', marker=\"x\", label='Test predicitons')\nplt.xlabel(\"Inputs\")\nplt.ylabel(\"Outputs\")\nplt.title(\"QML results\")\n\nplt.legend(loc='upper right')\nplt.show()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "pierwsze modele kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw3z.html#klasyfikacja",
    "href": "cwiczenia/cw3z.html#klasyfikacja",
    "title": "pierwsze modele kwantowe",
    "section": "klasyfikacja",
    "text": "klasyfikacja\n\nimport torch\nimport pennylane.numpy as np\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt \n\ntorch.manual_seed(123)\n\n\nX, y = make_moons(n_samples=200, noise=0.1)\n\n# create torch\n\nX = torch.from_numpy(X).to(torch.float32)\n\ny_ = torch.from_numpy(y).view(-1,1)\n\n\nc = [\"#1f77b4\" if y_ == 0 else \"#ff7f0e\" for y_ in y]  # kolorowanie\nplt.axis(\"off\")\nplt.scatter(X[:, 0], X[:, 1], c=c)\nplt.show()\n\n\ny_hot = torch.scatter(torch.zeros((200, 2)), 1, y_, 1).to(torch.float32)\n\n# X = X.clone().detach().requires_grad_(True)\n\n\n\n\n\n\n\n\n\ny_hot[:10]\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 0.],\n        [1., 0.],\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\nimport pennylane as qml\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\nn_layers = 4\n\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\n\nclass QN2(torch.nn.Module):\n    '''Classical -&gt; Quantum -&gt; Classical'''\n\n    def __init__(self, quanutm_layer):\n        super().__init__()\n\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(2, 2),\n            quanutm_layer,\n            torch.nn.Linear(2, 2),\n            torch.nn.Softmax(dim=1)\n        )\n        \n\n    def forward(self, x):\n        return  self.layers(x)\n\n\nqclassifier = QN2(qlayer)\nprint(qclassifier)\n\nopt = torch.optim.SGD(qclassifier.parameters(), lr=0.2)\nloss = torch.nn.L1Loss()\n\nQN2(\n  (layers): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): &lt;Quantum Torch Layer: func=qnode&gt;\n    (2): Linear(in_features=2, out_features=2, bias=True)\n    (3): Softmax(dim=1)\n  )\n)\n\n\n\nbatch_size = 5\nbatches = 200 // batch_size\n\ndata_loader = torch.utils.data.DataLoader(\n    list(zip(X, y_hot)), batch_size=5, shuffle=True, drop_last=True\n)\n\nepochs = 6\n\nfor epoch in range(epochs):\n\n    running_loss = 0\n\n    for xs, ys in data_loader:\n        opt.zero_grad()\n\n        loss_evaluated = loss(qclassifier(xs), ys)\n        loss_evaluated.backward()\n\n        opt.step()\n\n        running_loss += loss_evaluated\n\n    avg_loss = running_loss / batches\n    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n\ny_pred = qclassifier(X)\npredictions = torch.argmax(y_pred, axis=1).detach().numpy()\n\ncorrect = [1 if p == p_true else 0 for p, p_true in zip(predictions, y)]\naccuracy = sum(correct) / len(correct)\nprint(f\"Accuracy: {accuracy * 100}%\")\n\nAverage loss over epoch 1: 0.4244\nAverage loss over epoch 2: 0.2587\nAverage loss over epoch 3: 0.1951\nAverage loss over epoch 4: 0.1682\nAverage loss over epoch 5: 0.1543\nAverage loss over epoch 6: 0.1416\nAccuracy: 86.5%",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "pierwsze modele kwantowe"
    ]
  },
  {
    "objectID": "cwiczenia/cw1z.html",
    "href": "cwiczenia/cw1z.html",
    "title": "Laboratorium 1",
    "section": "",
    "text": "Na zajÄ™ciach wykorzystamy Å›rodowisko z jÄ™zykiem python3.11.\nÅšrodowisko to moÅ¼esz zainstalowaÄ‡ na swoim komputerze i uzupeÅ‚niÄ‡ o wymagane biblioteki (torch, pennylane). Mozesz rÃ³wnieÅ¼ wykorzystaÄ‡ Å›rodowisko na serwerach SGH.\n\nprzejdÅº pod adres https://sv-pb-jupiter.sgh.waw.pl/j1## w miejsce ## wstaw przypisany w zadaniu indywidualny numer Å›rodowiska.\nHasÅ‚o: root1## gdzie za ## wprowadÅº numer swojego Å›rodowiska.\nUruchom nowy notatnik. Wpisz w komÃ³rkÄ™ print(\"hello world\") i sprawdÅº czy wszystko dziaÅ‚a.\n\npobierz plik zad1.ipynb i zrealizuj Ä‡wiczenia.",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 1"
    ]
  },
  {
    "objectID": "cwiczenia/cw1z.html#kwantowa-sieÄ‡-neuronowa",
    "href": "cwiczenia/cw1z.html#kwantowa-sieÄ‡-neuronowa",
    "title": "Laboratorium 1",
    "section": "Kwantowa sieÄ‡ neuronowa",
    "text": "Kwantowa sieÄ‡ neuronowa\nZdefiniujmy nowÄ… strukturÄ™ siec - wymieniajÄ…c warstÄ™ ukrytÄ… na obwÃ³d kwantowy.\nclass QN(nn.Module):\n    '''Classical -&gt; Quantum -&gt; Classical'''\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int, Q_NODE, N_QUBITS):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            # input layer\n            nn.Linear(N_INPUT, N_QUBITS),\n            # 1st hidden layer as a quantum circuit\n            Q_NODE,\n            # output layer\n            nn.Linear(N_QUBITS, N_OUTPUT)\n        )\n        \n\n    def forward(self, x):\n        return  self.layers(x)\nJak mozesz zauwazyc po warstwie wejsciowej umiescilismy obietk Q_NODE, ktÃ³rego funkcjÄ™ podstawimy jako trzeci parametr naszej sieci.\nBez wiÄ™kszego wchodzenia w definicjÄ™ tego obiektu nasz obwÃ³d kwantowy musi pobraÄ‡ dane z warstwy poprzedniej i wypuÅ›ciÄ‡ jakieÅ› wyniki do warstwy wynikowej. OczywiÅ›cie takÄ… operacjÄ™ musi realizowaÄ‡ jakaÅ› funkcja (obiekt) w pythonie.\n# NASZ kwantowy PQC - parametryzowany obwÃ³d kwantowy dla jednej warstwy ukrytej\nimport pennylane as qml\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\n\n\nn_layers = 5\n\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\nUruchomienie sieci mozesz zrealizowac ponizszym kodem:\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    # oblicz Å›redniÄ… z roznnicy y i y_pred podniesionej do kwadratu\n    return ...\n\n\n#########################\n#   utworz zmienna qmodel z parametrami (1,1, qlayer, n_qubits)\n#   Twoj kod\n#  \nqmodel = ...\n#####\n\nprint(qmodel)\n\nx = x.requires_grad_(True)\nx_train = x.requires_grad_(True)\n\nlearning_rate=1e-3\noptimiser = torch.optim.Adam(qmodel.parameters(), lr=learning_rate)\n\nlosses = []\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n    \n\ntrain(x_train, y, qmodel, optimiser, 500, special_loss_fn, callback)\nSprawdz wyniki kodem:\ndef plot_result(x,y,x_data,y_data,yh, title=None):\n\n    plt.figure(figsize=(8,4))\n    plt.title(title)\n    plt.plot(x,y, color=\"tab:grey\", alpha=0.6, label=\"Exact solution\")\n    plt.plot(x,yh, color=\"tab:green\", label=\"Neural network prediction\")\n    plt.scatter(x_data, y_data, alpha=0.3, label='Training data')\n    l = plt.legend(loc='best')\n\nplot_result(\n    x.detach(),\n    torch.sin(x).detach(),\n    x.detach(),\n    y.detach(),\n    qmodel(x).detach(),\n    title='Training of PINN'\n    )\n\nprint(mse(qmodel(x), torch.sin(x)))",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 1"
    ]
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.13 Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3 -m venv venv\nUruchomienie w systemach linux i mac os:\nsource venv/bin/activate\n# . venv/bin/activate\n\n(venv)$ ~\nW systemie windows Å›rodowisko pythona uruchomisz poprzez:\nvenv/Scripts/activate\nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "NarzÄ™dzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeÅ›li nie odnaleziono komendy uruchom polecenie:\npython3\nZwrÃ³Ä‡ uwagÄ™, aby Twoja wersja nie byÅ‚a niÅ¼sza niÅ¼ 3.13 Aby wyjÅ›Ä‡ z powÅ‚oki pythona uÅ¼yj funkcji exit()\nPython 3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3 -m venv venv\nUruchomienie w systemach linux i mac os:\nsource venv/bin/activate\n# . venv/bin/activate\n\n(venv)$ ~\nW systemie windows Å›rodowisko pythona uruchomisz poprzez:\nvenv/Scripts/activate\nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeÅ›li masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglÄ…darce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdÅº do katalogu w ktÃ³rym utworzyÅ‚eÅ› Å›rodowisko, nastÄ™pnie uruchom Å›rodowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwÃ³rz konto na Kaggle, przejdÅº do zakÅ‚adki Courses i przerÃ³b caÅ‚y moduÅ‚ Pythona. Zawiera on:\n\nwyraÅ¼enia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npÄ™tle\nstringi i sÅ‚owniki\ndodawanie i uÅ¼ywanie zewnÄ™trznych bibliotek"
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "Chris Bernhardt, Obliczenia kwantowe dla kaÅ¼dego. PWN 2020\n\n\nWyjaÅ›nienie jak to dziaÅ‚a w obliczeniach kwantowych.\n\n\nMichel Le Bellac, WstÄ™p do informatyki kwantowej. PWN 2011\n\n\nTrudniejsza, duÅ¼o matematyki i fizyki.\n\n\nThomas G. Wong, Introduction to Classical and Quantum Computing. Rooted Grove 2022.\n\n\nBardzo dobra!, duÅ¼o przykÅ‚adÃ³w, duÅ¼o ciekawych informacji wyjaÅ›nianych bardzo szczegÃ³Å‚owo.\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nP. Gawron, M. Cholewa, â€¦ Rewolucja stanu. Fantastyczne wprowadzenie do informatyki kwantowej. Quantumz.io 2021\nA. Saxena, J. Mancilla, I. Montalban, C. Pere, Financial Modeling Using Quantum Computing. Packt 2023\nM. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Springer 2021.\nNielsen and Chuang Quantum Computation and Quantum Information, Cambridge, 2018.\nR. S. Sutor Dancing with Qubits, Second Edition, Packt, 2024\n\n\n\n\n\n\n\nL. Moroney, Sztuczna inteligencja i uczenie maszynowe dla programistÃ³w. Praktyczny przewodnik po sztucznej inteligencji. Helion 2021. Zobacz opis lub Kup e-book\nBruce, Bruce, Gedeck, Statystyka praktyczna w data science. Wydanie II. Helion. 2021."
  },
  {
    "objectID": "ksiazki.html#ksiÄ…Å¼ki",
    "href": "ksiazki.html#ksiÄ…Å¼ki",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "",
    "text": "Chris Bernhardt, Obliczenia kwantowe dla kaÅ¼dego. PWN 2020\n\n\nWyjaÅ›nienie jak to dziaÅ‚a w obliczeniach kwantowych.\n\n\nMichel Le Bellac, WstÄ™p do informatyki kwantowej. PWN 2011\n\n\nTrudniejsza, duÅ¼o matematyki i fizyki.\n\n\nThomas G. Wong, Introduction to Classical and Quantum Computing. Rooted Grove 2022.\n\n\nBardzo dobra!, duÅ¼o przykÅ‚adÃ³w, duÅ¼o ciekawych informacji wyjaÅ›nianych bardzo szczegÃ³Å‚owo.\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nP. Gawron, M. Cholewa, â€¦ Rewolucja stanu. Fantastyczne wprowadzenie do informatyki kwantowej. Quantumz.io 2021\nA. Saxena, J. Mancilla, I. Montalban, C. Pere, Financial Modeling Using Quantum Computing. Packt 2023\nM. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Springer 2021.\nNielsen and Chuang Quantum Computation and Quantum Information, Cambridge, 2018.\nR. S. Sutor Dancing with Qubits, Second Edition, Packt, 2024\n\n\n\n\n\n\n\nL. Moroney, Sztuczna inteligencja i uczenie maszynowe dla programistÃ³w. Praktyczny przewodnik po sztucznej inteligencji. Helion 2021. Zobacz opis lub Kup e-book\nBruce, Bruce, Gedeck, Statystyka praktyczna w data science. Wydanie II. Helion. 2021."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "KsiÄ…Å¼ki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nPeter Shor WykÅ‚ad\n\n\nPakiety Python\n\nPennyLane\n\n\n\nEdytory tekstu\n\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nFilmy\n\nWprowadzenie do obliczeÅ„ kwantowych\nQPoland, Bronze, Warsztaty z programowania komputerÃ³w kwantowych 2023"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Sylabus",
    "section": "",
    "text": "Nazwa przedmiotu: WstÄ™p do kwantowego uczenia maszynowego\nJednostka: SGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nKod przedmiotu: 232530-D, 232530-S\nPunkty ECTS: 3\nJÄ™zyk prowadzenia: polski\nPoziom przedmiotu: Å›rednio-zaawansowany\nProwadzÄ…cy: Sebastian ZajÄ…c, sebastian.zajac at sgh.waw.pl"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Sylabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nJeszcze do niedawna rozwÃ³j technologiczny oparty byÅ‚ na zmniejszaniu rozmiaru tranzystorÃ³w i zwiÄ™kszaniu mocy obliczeniowej procesorÃ³w. Ze wzglÄ™du na fizyczne aspekty natury proces ten, od pewnego momentu, musi uwzglÄ™dniaÄ‡ ograniczenia fizyki kwantowej. PrzyszÅ‚oÅ›Ä‡ moÅ¼e jednak wykorzystaÄ‡ inne narzÄ™dzia, ktÃ³rych moÅ¼liwoÅ›ci wykraczajÄ… poza klasyczne moce obliczeniowe. Mimo, iÅ¼ konstrukcja komputerÃ³w kwantowych to wciÄ…Å¼ etap inÅ¼ynierski, to okazuje siÄ™, Å¼e moÅ¼na juÅ¼ wskazaÄ‡ i wykorzystaÄ‡ je do tworzenia algorytmÃ³w, ktÃ³re moÅ¼na wykorzystaÄ‡ w dziedzinie uczenia maszynowego. Wykorzystanie algorytmÃ³w kwantowych pozwala zmniejszyÄ‡ czas przetwarzania duÅ¼ej iloÅ›ci danych, a tym samym rozszerza moÅ¼liwoÅ›ci przetwarzania i modelowania danych. Przedstawione na zajÄ™ciach biblioteki - IBM qiskit czy Pennylane (python) pozwalajÄ… na prostÄ… i szybkÄ… konstrukcje dowolnego algorytmu kwantowego. Algorytmy te, jak np. algorytm Grovera wykorzystaÄ‡ moÅ¼na do wielu problemÃ³w obliczeniowych uczenia maszynowego bÄ…dÅº do konstrukcji kwantowych sieci neuronowych."
  },
  {
    "objectID": "sylabus.html#efekty-ksztaÅ‚cenia",
    "href": "sylabus.html#efekty-ksztaÅ‚cenia",
    "title": "Sylabus",
    "section": "Efekty ksztaÅ‚cenia",
    "text": "Efekty ksztaÅ‚cenia\n\nWiedza:\n\n\nZna i rozumie koncepcje dziaÅ‚ania komputera klasycznego i kwantowego\nZna metody kwantowego uczenia maszynowego moÅ¼liwe do wykorzystania w biznesie\nRozumie potrzebÄ™ i moÅ¼liwoÅ›ci zastosowania komputerÃ³w kwantowych\n\n\nUmiejÄ™tnoÅ›ci:\n\n\nPotrafi stworzyÄ‡ proste algorytmy z wykorzystaniem kwantowych bramek logicznych\nUmie wykorzystaÄ‡ biblioteki pythonowe do generowania kodÃ³w obliczeÅ„ kwantowych\nPotrafi wykorzystaÄ‡ metody nadzorowane wykorzystywane w kwantowym uczeniu maszynowym\nPotrafi wykorzystaÄ‡ metody nienadzorowane wykorzystywane w kwantowym uczeniu maszynowym\nUmie stworzyÄ‡ prostÄ… kwantowÄ… sieÄ‡ neuronowÄ…\n\n\nKompetencje spoÅ‚eczne:\n\n\nformuÅ‚uje problem biznesowy wraz z jego informatycznym rozwiÄ…zaniem\nuzupeÅ‚niania wiedzÄ™ teoretycznÄ… jak i praktycznÄ…, w zakresie teorii, programowania, modelowania, nowych technologii informatycznych z wykorzystaniem kwantowego uczenia maszynowego"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Sylabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy: 40%\nkolokwium: 20%\nreferaty/eseje: 40%"
  },
  {
    "objectID": "sylabus.html#literatura",
    "href": "sylabus.html#literatura",
    "title": "Sylabus",
    "section": "Literatura",
    "text": "Literatura\n\nM. Schuld, F. Petruccione - Supervised Learning with Quantum Computers, Quantum Science and Technology. Springer 2018 https://doi.org/10.1007/978-3-319-96424-9\nC. Bernhardt - Obliczenia kwantowe dla kaÅ¼dego, Wydawnictwo Naukowe PWN 2020\nE. R. Johnston, N. Harrigan, M.Gimeno-Segovia - Komputer kwantowy.Programowanie, algorytmy, kod. Helion 2020\nS. Zajac - Modelowanie dla Biznesu. Analiza danych w czasie rzeczywistym. Oficyna Wydawnicza SGH. 2021\nA. K. Bishwas, A. Nani, V. Palade â€œQuantum Supervised Clustering Algorithm for Big Dataâ€ 2018- 3rd International Conference for Convergence in Technology.\nC. Ciliberto er al.Â â€œStatistical limits of supervised quantum learningâ€ Physical Review A 102. 4. 2020\nN Wiebe, A. Kapoor, K M. Svore â€œQuantum perceptron modelsâ€ NIPSâ€™16 Procedings of the 30th Internationa Conference on Neural information processing Systems. Vol. 29. 2016"
  },
  {
    "objectID": "sylabus.html#literatura-uzupeÅ‚niajÄ…ca",
    "href": "sylabus.html#literatura-uzupeÅ‚niajÄ…ca",
    "title": "Sylabus",
    "section": "Literatura uzupeÅ‚niajÄ…ca",
    "text": "Literatura uzupeÅ‚niajÄ…ca\n\nMichael A. Nielsen & Isaac L. Chuang - Quantum Computation and Quantum Information, Cambridge University Press, 2010\nK. Przanowski, S. ZajÄ…c - Modelowanie dla Biznesu. Metody Machine learning, Modele portfela consumer finance, modelek rekurencyjne analizy przeÅ¼ycia, modele scoringowe. Oficyna Wydawnicza SGH. 2020\nE. FrÄ…tczak - Modelowanie dla Biznesu. Regresja Logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring. Oficyna Wydawnicza SGH. 2019"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 232530-D\nSemestr zimowy 2025/2026,\nSGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje znajdziesz w sylabusie.\nCiekawe ksiÄ…Å¼ki i strony internetowe zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki. JeÅ›li chciaÅ‚(a)byÅ› coÅ› dodaÄ‡ przeÅ›lij informacje przez MS teams.",
    "crumbs": [
      "Sylabus",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Informacje ogÃ³lne",
    "section": "",
    "text": "Kod: 232530-D\nSemestr zimowy 2025/2026,\nSGH SzkoÅ‚a GÅ‚Ã³wna Handlowa w Warszawie\nPodstawowe informacje znajdziesz w sylabusie.\nCiekawe ksiÄ…Å¼ki i strony internetowe zamieszczone zostaÅ‚y w zakÅ‚adce ksiÄ…Å¼ki. JeÅ›li chciaÅ‚(a)byÅ› coÅ› dodaÄ‡ przeÅ›lij informacje przez MS teams.",
    "crumbs": [
      "Sylabus",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogÃ³lne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykÅ‚ad\nWykÅ‚ad jest realizowany w trybie stacjonarnym.\nJest on NIEOBOWIÄ„ZKOWY i odbywa siÄ™ w sali C 3B.\n\n\nLaboratoria\nWszystkie laboratoria odbywajÄ… siÄ™ w sali C.",
    "crumbs": [
      "Sylabus",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogÃ³lne",
    "section": "Technologie",
    "text": "Technologie\nUczestniczÄ…c w zajÄ™ciach musisz opanowaÄ‡ i przynajmniej w podstawowym zakresie posÅ‚ugiwaÄ‡ siÄ™ nastÄ™pujÄ…cymi technologiami informatycznymi:\n\nAlgebra liniowa - wektory, macierze, baza, iloczyn skalarny, iloczyn tensorowy\nPython, Jupyter notebook, Jupyter lab, Colab\nAlgorytmy sieci neuronowych i uczenia maszynowego w procesie klasyfikacji binarnej",
    "crumbs": [
      "Sylabus",
      "Informacje ogÃ³lne"
    ]
  },
  {
    "objectID": "cwiczenia/zad1.html",
    "href": "cwiczenia/zad1.html",
    "title": "klasyczne i kwantowe sieci neuronowe",
    "section": "",
    "text": "# import potrzebnych bibliotek \n\nimport torch\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\ntorch.manual_seed(1234)\n\n&lt;torch._C.Generator at 0x12a37f350&gt;\n\n\nWygenruj dane:\n\nkorzystajÄ…c z metody torch.linespace() wygenruj 500 punktÃ³w danych dla zakresu (0,10) w tablicy x\nZe wzglÄ™du, iz potrzebujemy 500 wierszy przypadkÃ³w (a nie 500 zmiennych) jednowymiarowej tablicy zastosuj metodÄ™ view(-1,1)\njako wynik si wygeneruj wartoÅ›ci funckji sin(x). Do zmiennej y zastosuj drobnÄ… zmianÄ™ dodajÄ…c wartoÅ›ci losowe.\nAnalogicznie jak dla danych x pamiÄ™taj o zmianie widoku : view(-1,1)\n\nPonizszy wykres wygeneruje Ci graficznÄ… reprezentacjÄ™ danych\n\nx = torch.linspace(0,10,500).view(-1,1)\n\nsi = torch.sin(x).view(-1,1)\n\ny = si + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x, y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nKorzystajÄ…c z wartwy gÄ™stej torch.nn.Linear(), oraz funkcji aktywacji (np. torch.nn.ReLU(), torch.nn.Tanh() i inne) utwÃ³rz sieÄ‡ z kilkoma (przynajmniej jednÄ… warstwÄ… ukrytÄ…) pozwalajÄ…cÄ… wygenerowaÄ‡ model regresji. Do definicji uzyj obiektu Sequential() - sprawdÅº w dokumentacji po co taki obiekt.\n\nclass SinusEstimator(torch.nn.Module):\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int):\n        super(SinusEstimator,self).__init__()\n        self.layers = torch.nn.Sequential(\n            # struktura Twojej sieci\n            torch.nn.Linear(N_INPUT, N_OUTPUT),\n        )\n\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nPoniszy kod wytrenuje TwojÄ… sieÄ‡:\n\n############## \n# zdefiniuj obiekt modelu. \nmodel = SinusEstimator(1,1)\n###########\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\n# dodatkowa funkcja - warto zrealizowaÄ‡\nlosses = []\n\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\ndef train(X, Y, model, optimiser, epochs, lossfn, callback = None):\n    for _ in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\n\nUruchom funkcjÄ™ train() z odpowiednimi parametrami.\n\ndane: x, y\nmodel sieci: model\noptymalizator: optimiser\niloÅ›Ä‡ epok: 500 (mozesz tez przetestowac najpierw 10 a potem np. 1000)\nfunkcja straty: criterion\ncallback: nasza zdefiniowana funkcja callback\n\n\ntrain(x,y,model,optimiser,10,criterion, callback)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspradz czy inna definicja funkcji kosztu special_loss_fn usprawni wyniki sieci\n\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    return torch.mean((y-y_pred)**2)\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n\ntrain(x,y,model,optimiser,10,special_loss_fn, callback)"
  },
  {
    "objectID": "cwiczenia/zad1.html#wygeneruj-klasycznÄ…-sieÄ‡-neuronowÄ…-dla-funckji-sinx",
    "href": "cwiczenia/zad1.html#wygeneruj-klasycznÄ…-sieÄ‡-neuronowÄ…-dla-funckji-sinx",
    "title": "klasyczne i kwantowe sieci neuronowe",
    "section": "",
    "text": "# import potrzebnych bibliotek \n\nimport torch\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\ntorch.manual_seed(1234)\n\n&lt;torch._C.Generator at 0x12a37f350&gt;\n\n\nWygenruj dane:\n\nkorzystajÄ…c z metody torch.linespace() wygenruj 500 punktÃ³w danych dla zakresu (0,10) w tablicy x\nZe wzglÄ™du, iz potrzebujemy 500 wierszy przypadkÃ³w (a nie 500 zmiennych) jednowymiarowej tablicy zastosuj metodÄ™ view(-1,1)\njako wynik si wygeneruj wartoÅ›ci funckji sin(x). Do zmiennej y zastosuj drobnÄ… zmianÄ™ dodajÄ…c wartoÅ›ci losowe.\nAnalogicznie jak dla danych x pamiÄ™taj o zmianie widoku : view(-1,1)\n\nPonizszy wykres wygeneruje Ci graficznÄ… reprezentacjÄ™ danych\n\nx = torch.linspace(0,10,500).view(-1,1)\n\nsi = torch.sin(x).view(-1,1)\n\ny = si + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x, y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nKorzystajÄ…c z wartwy gÄ™stej torch.nn.Linear(), oraz funkcji aktywacji (np. torch.nn.ReLU(), torch.nn.Tanh() i inne) utwÃ³rz sieÄ‡ z kilkoma (przynajmniej jednÄ… warstwÄ… ukrytÄ…) pozwalajÄ…cÄ… wygenerowaÄ‡ model regresji. Do definicji uzyj obiektu Sequential() - sprawdÅº w dokumentacji po co taki obiekt.\n\nclass SinusEstimator(torch.nn.Module):\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int):\n        super(SinusEstimator,self).__init__()\n        self.layers = torch.nn.Sequential(\n            # struktura Twojej sieci\n            torch.nn.Linear(N_INPUT, N_OUTPUT),\n        )\n\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nPoniszy kod wytrenuje TwojÄ… sieÄ‡:\n\n############## \n# zdefiniuj obiekt modelu. \nmodel = SinusEstimator(1,1)\n###########\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\n# dodatkowa funkcja - warto zrealizowaÄ‡\nlosses = []\n\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\ndef train(X, Y, model, optimiser, epochs, lossfn, callback = None):\n    for _ in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\n\nUruchom funkcjÄ™ train() z odpowiednimi parametrami.\n\ndane: x, y\nmodel sieci: model\noptymalizator: optimiser\niloÅ›Ä‡ epok: 500 (mozesz tez przetestowac najpierw 10 a potem np. 1000)\nfunkcja straty: criterion\ncallback: nasza zdefiniowana funkcja callback\n\n\ntrain(x,y,model,optimiser,10,criterion, callback)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspradz czy inna definicja funkcji kosztu special_loss_fn usprawni wyniki sieci\n\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    return torch.mean((y-y_pred)**2)\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n\ntrain(x,y,model,optimiser,10,special_loss_fn, callback)"
  },
  {
    "objectID": "cwiczenia/zad1.html#kwantowa-sieÄ‡-neuronowa",
    "href": "cwiczenia/zad1.html#kwantowa-sieÄ‡-neuronowa",
    "title": "klasyczne i kwantowe sieci neuronowe",
    "section": "Kwantowa sieÄ‡ neuronowa",
    "text": "Kwantowa sieÄ‡ neuronowa\nZdefiniujmy nowÄ… strukturÄ™ siec - wymieniajÄ…c warstÄ™ ukrytÄ… na obwÃ³d kwantowy.\nclass QN(nn.Module):\n    '''Classical -&gt; Quantum -&gt; Classical'''\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int, Q_NODE, N_QUBITS):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            # input layer\n            nn.Linear(N_INPUT, N_QUBITS),\n            # 1st hidden layer as a quantum circuit\n            Q_NODE,\n            # output layer\n            nn.Linear(N_QUBITS, N_OUTPUT)\n        )\n        \n\n    def forward(self, x):\n        return  self.layers(x)\nJak mozesz zauwazyc po warstwie wejsciowej umiescilismy obietk Q_NODE, ktÃ³rego funkcjÄ™ podstawimy jako trzeci parametr naszej sieci.\nBez wiÄ™kszego wchodzenia w definicjÄ™ tego obiektu nasz obwÃ³d kwantowy musi pobraÄ‡ dane z warstwy poprzedniej i wypuÅ›ciÄ‡ jakieÅ› wyniki do warstwy wynikowej. OczywiÅ›cie takÄ… operacjÄ™ musi realizowaÄ‡ jakaÅ› funkcja (obiekt) w pythonie.\n\nclass QN(torch.nn.Module):\n    '''Classical -&gt; Quantum -&gt; Classical'''\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int, Q_NODE, N_QUBITS):\n        super().__init__()\n\n        self.layers = troch.nn.Sequential(\n            # input layer\n            torch.nn.Linear(N_INPUT, N_QUBITS),\n            # 1st hidden layer as a quantum circuit\n            Q_NODE,\n            # output layer\n            torch.nn.Linear(N_QUBITS, N_OUTPUT)\n        )\n        \n\n    def forward(self, x):\n        return  self.layers(x)\n\n\n # NASZ kwantowy PQC - parametryzowany obwÃ³d kwantowy dla jednej warstwy ukrytej\nimport pennylane as qml\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\n\n\nn_layers = 5\n\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\nUruchomienie sieci mozesz zrealizowac ponizszym kodem\n\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    # oblicz Å›redniÄ… z roznnicy y i y_pred podniesionej do kwadratu\n    return ...\n\n\n#########################\n#   utworz zmienna qmodel z parametrami (1,1, qlayer, n_qubits)\n#   Twoj kod\n#  \nqmodel = ...\n#####\n\nprint(qmodel)\n\nx = x.requires_grad_(True)\nx_train = x.requires_grad_(True)\n\nlearning_rate=1e-3\noptimiser = torch.optim.Adam(qmodel.parameters(), lr=learning_rate)\n\nlosses = []\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n    \n\ntrain(x_train, y, qmodel, optimiser, 500, special_loss_fn, callback)\n\nSprawdz wyniki kodem:\n\ndef plot_result(x,y,x_data,y_data,yh, title=None):\n\n    plt.figure(figsize=(8,4))\n    plt.title(title)\n    plt.plot(x,y, color=\"tab:grey\", alpha=0.6, label=\"Exact solution\")\n    plt.plot(x,yh, color=\"tab:green\", label=\"Neural network prediction\")\n    plt.scatter(x_data, y_data, alpha=0.3, label='Training data')\n    l = plt.legend(loc='best')\n\nplot_result(\n    x.detach(),\n    torch.sin(x).detach(),\n    x.detach(),\n    y.detach(),\n    qmodel(x).detach(),\n    title='Training of PINN'\n    )\n\nprint(mse(qmodel(x), torch.sin(x)))"
  },
  {
    "objectID": "cwiczenia/cw2.html",
    "href": "cwiczenia/cw2.html",
    "title": "Pennylane - wprowadzenie",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt \nfrom IPython.display import clear_output\n\n\nx = torch.linspace(0,10,500).view(-1,1)\n\nsi = torch.sin(x).view(-1,1)\n\ny = si + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x, y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\nclass QN(torch.nn.Module):\n    '''Classical -&gt; Quantum -&gt; Classical'''\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int, Q_NODE, N_QUBITS):\n        super().__init__()\n\n        self.layers = torch.nn.Sequential(\n            # input layer\n            torch.nn.Linear(N_INPUT, N_QUBITS),\n            # 1st hidden layer as a quantum circuit\n            Q_NODE,\n            # output layer\n            torch.nn.Linear(N_QUBITS, N_OUTPUT)\n        )\n        \n\n    def forward(self, x):\n        return  self.layers(x)\nimport pennylane as qml\n\nn_qubits = 3\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\nn_layers = 3\n\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\nqmodel = QN(1, 1, qlayer, n_qubits)\n\nlearning_rate=1e-3\n\noptimiser = torch.optim.Adam(qmodel.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\nlosses = []\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\ndef train(X, Y, model, optimiser, epochs, lossfn, callback = None):\n    for _ in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\ntrain(x, y, qmodel, optimiser,500, criterion, callback)",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Pennylane - wprowadzenie"
    ]
  },
  {
    "objectID": "cwiczenia/cw2.html#pennylane",
    "href": "cwiczenia/cw2.html#pennylane",
    "title": "Pennylane - wprowadzenie",
    "section": "ğŸ§  PennyLane",
    "text": "ğŸ§  PennyLane\nğŸ”— Strona oficjalna\nPennyLane to otwartoâ€‘ÅºrÃ³dÅ‚owa biblioteka Pythona opracowana przez Xanadu dla kwantowego uczenia maszynowego, obliczeÅ„ kwantowych oraz chemii kwantowej.\nZapewnia wysokopoziomowy, intuicyjny interfejs do budowania hybrydowych modeli kwantowoâ€‘klasycznych, Å‚Ä…czÄ…c obwody kwantowe z popularnymi frameworkami uczenia maszynowego, takimi jak PyTorch i TensorFlow.\nPennyLane wprowadza pojÄ™cie QNode (quantum node) â€“ funkcji kwantowych, ktÃ³re zachowujÄ… siÄ™ jak zwykÅ‚e funkcje Pythona i obsÅ‚ugujÄ… automatycznÄ… rÃ³Å¼niczkowanie (autodiff).\nUmoÅ¼liwia uruchamianie modeli zarÃ³wno na symulatorach, jak i na rzeczywistym sprzÄ™cie kwantowym (np. IBMâ€¯Q, Amazonâ€¯Braket i inne).\nDziÄ™ki PennyLane moÅ¼esz: - budowaÄ‡ wariacyjne algorytmy kwantowe,\n- trenowaÄ‡ kwantowe sieci neuronowe,\n- eksplorowaÄ‡ zaawansowane architektury kwantowego uczenia maszynowego.\nBiblioteka stanowi potÄ™Å¼ny most miÄ™dzy klasycznÄ… sztucznÄ… inteligencjÄ… a rosnÄ…cym Å›wiatem obliczeÅ„ kwantowych.\nPennyLane zawiera takÅ¼e spersonalizowanÄ… wersjÄ™ NumPy (pennylane.numpy), ktÃ³ra obsÅ‚uguje tablice Å›ledzone gradientem, co uÅ‚atwia integrowanie obwodÃ³w kwantowych w procesach optymalizacji.\n\n\n\nPennyLane",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Pennylane - wprowadzenie"
    ]
  },
  {
    "objectID": "cwiczenia/cw2.html#podstawowe-importy-bibliotek",
    "href": "cwiczenia/cw2.html#podstawowe-importy-bibliotek",
    "title": "Pennylane - wprowadzenie",
    "section": "podstawowe importy bibliotek",
    "text": "podstawowe importy bibliotek\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\n\nğŸ§ª Obwody kwantowe w PennyLane\n\nObwody kwantowe sÄ… implementowane jako funkcje kwantowe, zwane takÅ¼e QNodeâ€™ami.\nSÄ… to funkcje kwantowe zachowujÄ…ce siÄ™ jak standardowe funkcje Pythona i wspierajÄ…ce automatycznÄ… rÃ³Å¼niczkacjÄ™ przy uÅ¼yciu klasycznych narzÄ™dzi ML.\nQNodeâ€™y sÄ… uruchamiane na rÃ³Å¼nych urzÄ…dzeniach (devices), takich jak:\n\nsymulatory (np. default.qubit, lightning.qubit) oraz,\nrzeczywisty sprzÄ™t kwantowy (np. IBMâ€¯Q, Amazonâ€¯Braket, Xanadu).\n\nUrzÄ…dzenia sÄ… wymienne i okreÅ›lajÄ…, w jaki sposÃ³b dana funkcja kwantowa jest wykonywana.\n\n\n\n\nPennyLane\n\n\nğŸ”— UrzÄ…dzenia, ktÃ³re moÅ¼esz uÅ¼ywaÄ‡\nMoÅ¼emy zdefiniowaÄ‡ nasz symulator â€” w tym przypadku uÅ¼yjemy default.qubit.\nMusimy takÅ¼e okreÅ›liÄ‡, ile kubitÃ³w chcemy uÅ¼yÄ‡, korzystajÄ…c z parametru wires.\nPrzykÅ‚adowe urzÄ…dzenia\n\ndefault.qubit â€“ symulator napisany w Pythonie\n\nlightning.qubit â€“ szybszy symulator napisany w C++\n\ndefault.mixed â€“ uÅ¼ywany do symulacji mieszanych stanÃ³w kwantowych\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n\n## for example \ndev2 = qml.device(\"default.qubit\", wires=3)\n\ndev3 = qml.device(\"lightning.qubit\", wires=['q1', 'aux'])\n\n\n\n\nkubity1\n\n\n\\[\n\\ket{000} = \\ket{0}\\otimes \\ket{0} \\otimes \\ket{0}\n\\]\n\nObiekt Qnode bÄ™dziemy uÅ¼ywaÄ‡ do definicji obwodÃ³w kwantowych. Obiekt ten wspiera wiele bibliotek do obliczeÅ„ numerycznych, tzw. interfejsÃ³w: - NumPy, - PyTorch, - TensorFlow, - JAX\nDomyÅ›lnie QNodes uÅ¼ywa interfejs NumPy. DziÄ™ki niemu mamy dostÄ™p do optymalizatorÃ³w domyÅ›lnych z biblioteki Pennylane. PozostaÅ‚e interferjsy wymagajÄ… uÅ¼ycia optymalizatorÃ³w z innych pakietÃ³w.\n\ndef qc(): # quantum circuit\n    return qml.state()\n\nwires oznacza kwantowy podsystem - czyli nasz pojedynczy kubit. Liczymy od 0 nie od 1.\n\nFunkcja kwantowa moÅ¼e pobieraÄ‡ klasyczne pamaretry\nFunkcja kwantowa moÅ¼e zawieraÄ‡ klasyczny flow (przepÅ‚yw) twojego programu for czy if else.\n\nZbiÃ³r kwantowych operatorÃ³w",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Pennylane - wprowadzenie"
    ]
  },
  {
    "objectID": "cwiczenia/cw2.html#uruchomienie-obwodu-kwantowego",
    "href": "cwiczenia/cw2.html#uruchomienie-obwodu-kwantowego",
    "title": "Pennylane - wprowadzenie",
    "section": "Uruchomienie obwodu kwantowego",
    "text": "Uruchomienie obwodu kwantowego\nUruchomienie odbywa siÄ™ po wyborze device z okreÅ›leniem iloÅ›ci kubitÃ³w (wires)\n\ncirc = qml.QNode(qc, dev)\ncirc()\n\narray([1.+0.j, 0.+0.j])\n\n\n\\[\n\\ket{\\psi} = \\ket{0} = [1,0]^{T}\n\\]\n\ncirc2 = qml.QNode(qc, dev2)\ncirc2()\n\narray([1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j])\n\n\n\ncirc3 = qml.QNode(qc, dev3)\ncirc3()\n\narray([1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\ndef quantum_circuit():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\ncirc = qml.QNode(quantum_circuit, dev)\n\ncirc()\n\narray([0.70710678+0.j, 0.70710678+0.j])\n\n\n\nfrom math import sqrt\nprint(circ()[0].real, 1/sqrt(2))\nprint(circ()[0].real == 1/sqrt(2))\n\n0.7071067811865475 0.7071067811865475\nTrue\n\n\n\nqml.draw(circ)()\n\n'0: â”€â”€Hâ”€â”¤  State'\n\n\n\nqml.draw_mpl(circ)()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Pennylane - wprowadzenie"
    ]
  },
  {
    "objectID": "cwiczenia/cw2.html#obwody-z-wykorzystaniem-decoratora-qml.qnode",
    "href": "cwiczenia/cw2.html#obwody-z-wykorzystaniem-decoratora-qml.qnode",
    "title": "Pennylane - wprowadzenie",
    "section": "obwody z wykorzystaniem decoratora @qml.qnode()",
    "text": "obwody z wykorzystaniem decoratora @qml.qnode()\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.70710678+0.j, 0.70710678+0.j])\n\n\n\nimport matplotlib.pyplot as plt\n\nqml.drawer.use_style(\"pennylane_sketch\")\n\nfig, ax = qml.draw_mpl(qc)()\nplt.show()\n\nMatplotlib is building the font cache; this may take a moment.\n\n\n\n\n\n\n\n\n\n\nqml.probs()\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.probs()\n\nqc()\n\narray([0.5, 0.5])\n\n\na jaki wynik otrzymamy dla pustego obwodu?\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    return qml.probs()\n\nresults = qc()\nresults\n\narray([1., 0.])\n\n\n\nuÅ¼yj qml.sample() lub qml.counts() dla innych wariantÃ³w wynikÃ³w.\n\nIloÅ›Ä‡ wykonaÅ„ obwodu sterowana jest w QNode za pomocÄ… parametru shot, ktÃ³ry moÅ¼e byÄ‡ liczbÄ… jak rÃ³wnieÅ¼ listÄ… liczb. &gt; Uwaga w wersji biblioteki &lt;0.43 - parametr shot ustawiany jest na poziomie device.\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev, shots=5)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.sample()\n\nqc()\n\narray([[0],\n       [0],\n       [0],\n       [0],\n       [1]])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev, shots=100)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.counts()\n\nresults = qc()\n\n\nresults\n\n{np.str_('0'): np.int64(48), np.str_('1'): np.int64(52)}\n\n\n\ntype(results), results.items()\n\n(dict,\n dict_items([(np.str_('0'), np.int64(48)), (np.str_('1'), np.int64(52))]))\n\n\n\nint(results['0']), int(results['1'])\n\n(48, 52)\n\n\n\nfrom functools import partial\n\ndev = qml.device(\"default.qubit\", wires=['q1'])\n\n@partial(qml.set_shots, shots=[5, 10, 1000])\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires='q1')\n    return qml.counts()\n\nresults = qc()\n\n\nresults\n\n({np.str_('0'): np.int64(3), np.str_('1'): np.int64(2)},\n {np.str_('0'): np.int64(6), np.str_('1'): np.int64(4)},\n {np.str_('0'): np.int64(520), np.str_('1'): np.int64(480)})",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Pennylane - wprowadzenie"
    ]
  },
  {
    "objectID": "cwiczenia/cw2.html#przygotowanie-wÅ‚asnego-stanu-poczÄ…tkowego",
    "href": "cwiczenia/cw2.html#przygotowanie-wÅ‚asnego-stanu-poczÄ…tkowego",
    "title": "Pennylane - wprowadzenie",
    "section": "Przygotowanie wÅ‚asnego stanu poczÄ…tkowego",
    "text": "Przygotowanie wÅ‚asnego stanu poczÄ…tkowego\n\\[\n\\ket{\\psi}=\\ket{1} = 0 \\ket{0} + 1 \\ket{1}\n\\]\n\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.ops import StatePrep\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n\nstate1 = np.array([0,1]) # state after initialization \n\n@qml.qnode(dev)\ndef qc(state):\n    StatePrep(state, wires=0)\n    return qml.state()\n\nqc(state1).real\n\ntensor([0., 1.], requires_grad=True)\n\n\n\n@qml.qnode(dev)\ndef qc(state):\n    StatePrep(state, wires=0)\n    return qml.probs()\n\nqc(state1)\n\ntensor([0., 1.], requires_grad=True)\n\n\ndwa kubity\n\ndev = qml.device(\"default.qubit\", wires=2)\n\nstan = np.array([1/2, 1/2, 1/2, 1/2])\n\nprawd = [i**2 for i in stan]\nprint(f\"test: sum of probs {np.sum(prawd)}\")\n\n@qml.qnode(dev)\ndef qc():\n    StatePrep(stan, wires=[0,1])\n    return qml.state()\n\nqc()\n\ntest: sum of probs 1.0\n\n\ntensor([0.5+0.j, 0.5+0.j, 0.5+0.j, 0.5+0.j], requires_grad=True)\n\n\nKod naszej wartwy ukrytej w ktÃ³rej uÅ¼yliÅ›my obwodu kwantowego realizowaÅ‚ nastÄ™pujÄ…ce obiekty i funkcje:\n\nimport pennylane as qml\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\nObwody kwantowe skÅ‚adajÄ… siÄ™ z rejestrÃ³w, ktÃ³re reprezentujÄ… poszczegÃ³lne kubity.\n\nDomyÅ›lnie kubity inicjalizujemy w stanie 0.\n\nOperacje wykonywane na kubitach nazywamy bramkami.\nOperacje te moÅ¼na wykonywaÄ‡ na jednym albo i wielu kubitach na raz.\nDomyÅ›lnie bÄ™dziemy optymalizowaÄ‡ algortymy aby skÅ‚adaÅ‚y siÄ™ z jak najmniejszej iloÅ›ci bramek dziaÅ‚ajÄ…cych na duÅ¼Ä… liczbÄ™ kubitÃ³w.\nGraficznie moÅ¼na rozumieÄ‡ realizacjÄ™ algorytmu jako stosowanie bramek na poszczegÃ³lnych kubitach.\n\n\n\nkibu2\n\n\nW bibliotece PennyLane, obwody kwantowe reprezentowane sÄ… przez kwantowe funkcje, realizowane przez klasyczne funkcje w pythonie.\nSchemat kodu penny lane moÅ¼emy zapisaÄ‡ jako:\nimport pennylane as qml\n\ndef my_quantum_function(params):\n\n    # Single-qubit operations with no input parameters\n    qml.Gate1(wires=0)\n    qml.Gate2(wires=1)\n\n    # A single-qubit operation with an input parameter\n    qml.Gate3(params[0], wires=0)\n\n    # Two-qubit operation with no input parameter on wires 0 and 1\n    qml.TwoQubitGate1(wires=[0, 1])\n\n    # Two-qubit operation with an input parameter on wires 0 and 1\n    qml.TwoQubitGate2(params[1], wires=[0, 1])\n\n    # Return the result of a measurement\n    return qml.Measurement(wires=[0, 1])\nMatematycznie caÅ‚oÅ›Ä‡ moÅ¼emy zapisaÄ‡ jako:\n\nPrzykÅ‚adowo\n\n\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=2)\n#dev = qml.device(\"default.qubit\", wires=2, shots=1000)\n\n@qml.qnode(dev)\ndef circ(theta):\n    qml.Hadamard(wires = 0)\n    qml.CNOT(wires = [0,1])\n    qml.RZ(theta, wires = 0)\n    return qml.state()\n#    return qml.probs(wires = [0,1])\n\ncirc(np.pi)\n\narray([4.32978028e-17-0.70710678j, 0.00000000e+00+0.j        ,\n       0.00000000e+00+0.j        , 4.32978028e-17+0.70710678j])\n\n\n\nprint(qml.draw(circ)(np.pi))\n\n0: â”€â”€Hâ”€â•­â—â”€â”€RZ(3.14)â”€â”¤  State\n1: â”€â”€â”€â”€â•°Xâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  State\n\n\n\nqml.draw_mpl(circ, decimals=2, style=\"sketch\", level=\"device\")(np.pi)\n\n\n\n\n\n\n\n\n\nBramka X\nBramka X-gate reprezentowana jest przez macierz Pauli-X :\n\\[\nX = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n\\end{pmatrix}\n\\]\nBramka X obraca kubit w kierunku osi na sferze Blochâ€™a o \\(\\pi\\) radianÃ³w. Zmienia \\(|0\\rangle\\) na \\(|1\\rangle\\) oraz \\(|1\\rangle\\) na \\(|0\\rangle\\). Jest czÄ™sto nazywana kwantowym odpowiednikiem bramki NOT lub okreÅ›lana jako bit-flip.\n\\[ \\sigma_x \\ket{0} = \\ket{1} \\,\\,\\, \\sigma_x\\ket{1} = \\ket{0} \\]\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.X(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.+0.j, 1.+0.j])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.PauliX(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.+0.j, 1.+0.j])\n\n\n\nqml.draw_mpl(qc)()\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.PauliX(wires=0)\n    qml.X(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([1.+0.j, 0.+0.j])\n\n\n\n\n\n\n\n\n\n\n\nDowolna bramka unitarna\n\nfrom pennylane import numpy as np\n\nU = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.QubitUnitary(U, wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\ntensor([0.70710678+0.j, 0.70710678+0.j], requires_grad=True)\n\n\n\n\n\n\n\n\n\n\n\nBramka Hadamarda\nBramka Hadamarda przetwarza stan \\(|0\\rangle\\) na kombinacje liniowa (superpozycje) \\(\\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}}\\), co oznacza, Å¼e pomiar zwrÃ³ci z takim samym prawdopodobieÅ„stwem stanu 1 lub 0. Stan ten czÄ™sto oznaczany jest jako: \\(|+\\rangle\\).\n\\[\nH = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\]\n\\[ H\\ket{0} = \\frac{\\sqrt{2}}{2} (\\ket{0}+ \\ket{1})\\] \\[ H\\ket{1} = \\frac{\\sqrt{2}}{2}(\\ket{0}- \\ket{1})\\]\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.70710678+0.j, 0.70710678+0.j])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc(state):\n    if state==1:\n        qml.X(wires=0)\n    qml.Hadamard(wires=0)\n    qml.PauliX(wires=0)\n    qml.Hadamard(wires=0)\n    return qml.state()\n\nqc(0)\n\narray([1.+0.j, 0.+0.j])\n\n\n\nqc(1)\n\narray([ 0.+0.j, -1.+0.j])\n\n\n\n\nbramka SX\nBramka SX jest pierwiastkiem kwadratowym bramki X. Dwukrotne zastosowanie powinno reazlizowac bramkÄ™ X.\n\\[\nSX = \\frac{1}{2}\\begin{pmatrix}\n1+i & 1-i \\\\\n1-i & 1+i \\\\\n\\end{pmatrix}\n\\]\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.SX(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([0.5+0.5j, 0.5-0.5j])\n\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.SX(wires=0)\n    qml.SX(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([0.+0.j, 1.+0.j])\n\n\n\n\n\n\n\n\n\n\n\nZ gate\n\\[\nZ = \\begin{pmatrix}\n1 & 0 \\\\\n0 & -1 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & e^{i \\pi} \\\\\n\\end{pmatrix}\n\\]\nInne nazwy bramki: phase flip lub sign flip\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.Z(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([ 1.+0.j, -0.+0.j])\n\n\n\n\n\n\n\n\n\n\n\nRZ gate\nBramkÄ™ PauliZ moÅ¼na uogÃ³lniÄ‡ i sparametryzowaÄ‡ kÄ…tem. Dla \\(\\phi=\\pi\\) otrzymujemy bramkÄ™ \\(\\sigma_z\\).\n\\[\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & e^{i \\pi} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & e^{i \\phi} \\\\\n\\end{pmatrix}\n\\]\n\\[ R_Z(\\phi) = e^{-i \\phi \\frac{\\sigma_z}{2} }  \\]\n\\[\nRZ = \\begin{pmatrix}\ne ^{-i \\frac{\\phi}{2} } & 0 \\\\\n0 & e ^{i \\frac{\\phi}{2} } \\\\\n\\end{pmatrix} = \\cos(\\frac{\\phi}{2})I_2 - \\sin(\\frac{\\phi}{2}) i\\sigma_z\n\\]\n\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\n\n\ndef qc(phi):\n    qml.RZ(phi=phi, wires=0)\n    return qml.state()\n\nqc(np.pi/2)\n\narray([0.70710678-0.70710678j, 0.        +0.j        ])\n\n\n\nqml.draw_mpl(qc)(np.pi/2)\n\n\n\n\n\n\n\n\n\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\n\n\ndef qc(phi):\n    qml.SX(wires=0)\n    qml.RZ(phi=phi, wires=0)\n    return qml.state()\n\nqc(np.pi)\n\narray([0.5-0.5j, 0.5+0.5j])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Pennylane - wprowadzenie"
    ]
  },
  {
    "objectID": "cwiczenia/cw4.html",
    "href": "cwiczenia/cw4.html",
    "title": "Optymalizacja i kodowanie danych",
    "section": "",
    "text": "Napisz nowy obwÃ³d kwantowy, ktÃ³ry zawieraÄ‡ bÄ™dzie tylko bramkÄ™ \\(R_X\\) dla dowolnego parametru \\(\\theta\\)\noblicz i uzasadnij, Å¼e wartoÅ›Ä‡ oczekiwana dla stanu \\(\\ket{\\psi} = R_X \\, \\ket{0}\\) \\[&lt;Z&gt; = cos^2(\\theta /2)- sin^2(\\theta /2) = cos(\\theta)\\]\n\nZaÅ‚Ã³Å¼my, Å¼e nasz problem obliczeniowy sprowadza siÄ™ do wygenerowania wartoÅ›ci oczekiwanej o wartoÅ›ci 0.5.\n\\[\n\\textbf{&lt;Z&gt;} = \\bra{\\psi} \\textbf{Z} \\ket{\\psi} = 0.5\n\\]\nNapisz program znajdujÄ…cy rozwiÄ…zanie - szukajÄ…cy wagÄ™ \\(\\theta\\) dla naszego obwodu\n\nZdefiniuj funkcjÄ™ kosztu, ktÃ³rÄ… bedziemy minimalizowaÄ‡ \\((Y - y)^2\\)\nzainicjuj rozwiÄ…zanie \\(theta=0.01\\) i przypisz do tablicy array np.array(0.01, requires_grad=True)\nJako opt wybierz spadek po gradiencie : opt = qml.GradientDescentOptimizer(stepsize=0.1)\nuzyj poniÅ¼szego kodu do wygenerowania pÄ™tli obiczeÅ„\n\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\nWykorzystujÄ…c tylko elementy pennylane\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.5)**2\n\ntheta = np.array(0.01, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\nMoÅ¼emy teÅ¼ uÅ¼yÄ‡ bezpoÅ›rednio pytorch\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev, interface=\"torch\")\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\n\ndef cost_fn(theta):\n    target = 0.5\n    return (par_c(theta) - target) ** 2\n\n\nimport torch\nfrom torch.optim import Adam \n\ntheta = torch.tensor(0.01, requires_grad=True)\n\noptimizer = Adam([theta], lr=0.1)\nepochs = 100\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    loss = cost_fn(theta)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: 0.1099998876452446, koszt: 0.24399264948886004\nepoka: 10, theta: 1.0454959869384766, koszt: 2.169397961785511e-06\nepoka: 20, theta: 1.0966185331344604, koszt: 0.0018829460500888265\nepoka: 30, theta: 0.9526112079620361, koszt: 0.006329338284936267\nepoka: 40, theta: 1.111649513244629, koszt: 0.0032281231974498922\nepoka: 50, theta: 1.0076401233673096, koszt: 0.0011463367423114523\nepoka: 60, theta: 1.0690317153930664, koszt: 0.00036201344599675586\nepoka: 70, theta: 1.0343401432037354, koszt: 0.000123060304852398\nepoka: 80, theta: 1.0549986362457275, koszt: 4.584717916214815e-05\nepoka: 90, theta: 1.042211651802063, koszt: 1.859027886217772e-05\n\n\nJeszcze jeden przykÅ‚ad\n\nNapisz obwÃ³d kwantowy, ktÃ³ry zawieraÄ‡ bÄ™dzie bramkÄ™ \\(R_X\\) dla parametru \\(\\theta_1\\) oraz \\(R_Y\\) dla parametru \\(\\theta_2\\)\noblicz i uzasadnij, Å¼e wartoÅ›Ä‡ oczekiwana dla stanu \\(\\ket{\\psi} = R_Y(\\theta_2) R_X(\\theta_1) \\, \\ket{0}\\)\n\n\\[&lt;Z&gt;  = \\cos(\\theta_1) \\cos(\\theta_2)\\]\nMozliwe wartoÅ›ci Å›redniej zawierajÄ… siÄ™ w przedziale \\(-1\\), \\(1\\).\nPrzyjmij zaÅ‚ozenie, ze optymalne rozwiÄ…zanie realizowane jest dla wartoÅ›ci oczekiwanej = 0.4\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta[0], wires=0)\n    qml.RY(theta[1], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.4)**2\n\ntheta = np.array([0.01, 0.02], requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: [0.01119924 0.02239872], koszt: 0.3596238551650218\nepoka: 10, theta: [0.03468299 0.06939827], koszt: 0.35640059384126277\nepoka: 20, theta: [0.10485556 0.21069384], koszt: 0.3277736421372642\nepoka: 30, theta: [0.26595847 0.55025891], koszt: 0.17843868824086426\nepoka: 40, theta: [0.41114867 0.91214351], koszt: 0.02593550926609833\nepoka: 50, theta: [0.45600131 1.05610411], koszt: 0.0017612620807984237\nepoka: 60, theta: [0.46619699 1.09390217], koszt: 0.00010074458607215528\nepoka: 70, theta: [0.4685347  1.10295946], koszt: 5.557697121461739e-06\nepoka: 80, theta: [0.469078   1.10508776], koszt: 3.040948516747214e-07\nepoka: 90, theta: [0.46920476 1.10558565], koszt: 1.6607272093790385e-08\nOptymalizacja zakonczona dla theta=[0.46923296 1.10569646], koszt: 1.2125189676042736e-09\n\n\n\n\nCelem jest znalezienie najmnieszej wartoÅ›ci wÅ‚asnej dla Hamiltonianu \\(H = Z_0 Z_1 + Z_0\\)\nTego typu hamiltoniany opisujÄ… ukÅ‚ady fizyczne np. systemy spinowe.\n\\(Z_0 Z_1\\) - mozna interpretowaÄ‡ jako krawedz miedzy dwoma wierzchoÅ‚kami.\n\\(Z_0\\) - efekty lokalne wierzchoÅ‚ka 0\n\nimport pennylane as qml\nfrom pennylane import numpy as np \nimport random\n\ndev = qml.device(\"default.qubit\", wires=2)\n\nH = qml.PauliZ(0) @ qml.PauliZ(1) + qml.PauliZ(0)\n\n@qml.qnode(dev)\ndef circuit(params):\n    qml.RY(params[0], wires=0)\n    qml.RY(params[1], wires=1)\n    qml.CNOT(wires=[0,1])\n    return qml.expval(H)\n\ndef cost_fn(params):\n    return circuit(params)\n\ninit_param = [random.uniform(0, 2*3.1415) for _ in range(2)]\n\nparams = np.array(init_param, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.01)\n\nepochs = 500\nfor epoch in range(epochs):\n    params = opt.step(cost_fn, params)\n\n    if epoch % 50 == 0:\n        print(f\"epoka: {epoch}, theta: {params}, koszt: {cost_fn(params)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={params}, koszt: {cost_fn(params)}\")",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Optymalizacja i kodowanie danych"
    ]
  },
  {
    "objectID": "cwiczenia/cw4.html#zadanie---obwÃ³d-kwantowy-z-optymalizacjÄ…",
    "href": "cwiczenia/cw4.html#zadanie---obwÃ³d-kwantowy-z-optymalizacjÄ…",
    "title": "Optymalizacja i kodowanie danych",
    "section": "",
    "text": "Napisz nowy obwÃ³d kwantowy, ktÃ³ry zawieraÄ‡ bÄ™dzie tylko bramkÄ™ \\(R_X\\) dla dowolnego parametru \\(\\theta\\)\noblicz i uzasadnij, Å¼e wartoÅ›Ä‡ oczekiwana dla stanu \\(\\ket{\\psi} = R_X \\, \\ket{0}\\) \\[&lt;Z&gt; = cos^2(\\theta /2)- sin^2(\\theta /2) = cos(\\theta)\\]\n\nZaÅ‚Ã³Å¼my, Å¼e nasz problem obliczeniowy sprowadza siÄ™ do wygenerowania wartoÅ›ci oczekiwanej o wartoÅ›ci 0.5.\n\\[\n\\textbf{&lt;Z&gt;} = \\bra{\\psi} \\textbf{Z} \\ket{\\psi} = 0.5\n\\]\nNapisz program znajdujÄ…cy rozwiÄ…zanie - szukajÄ…cy wagÄ™ \\(\\theta\\) dla naszego obwodu\n\nZdefiniuj funkcjÄ™ kosztu, ktÃ³rÄ… bedziemy minimalizowaÄ‡ \\((Y - y)^2\\)\nzainicjuj rozwiÄ…zanie \\(theta=0.01\\) i przypisz do tablicy array np.array(0.01, requires_grad=True)\nJako opt wybierz spadek po gradiencie : opt = qml.GradientDescentOptimizer(stepsize=0.1)\nuzyj poniÅ¼szego kodu do wygenerowania pÄ™tli obiczeÅ„\n\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\nWykorzystujÄ…c tylko elementy pennylane\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.5)**2\n\ntheta = np.array(0.01, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\nMoÅ¼emy teÅ¼ uÅ¼yÄ‡ bezpoÅ›rednio pytorch\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev, interface=\"torch\")\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\n\ndef cost_fn(theta):\n    target = 0.5\n    return (par_c(theta) - target) ** 2\n\n\nimport torch\nfrom torch.optim import Adam \n\ntheta = torch.tensor(0.01, requires_grad=True)\n\noptimizer = Adam([theta], lr=0.1)\nepochs = 100\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    loss = cost_fn(theta)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: 0.1099998876452446, koszt: 0.24399264948886004\nepoka: 10, theta: 1.0454959869384766, koszt: 2.169397961785511e-06\nepoka: 20, theta: 1.0966185331344604, koszt: 0.0018829460500888265\nepoka: 30, theta: 0.9526112079620361, koszt: 0.006329338284936267\nepoka: 40, theta: 1.111649513244629, koszt: 0.0032281231974498922\nepoka: 50, theta: 1.0076401233673096, koszt: 0.0011463367423114523\nepoka: 60, theta: 1.0690317153930664, koszt: 0.00036201344599675586\nepoka: 70, theta: 1.0343401432037354, koszt: 0.000123060304852398\nepoka: 80, theta: 1.0549986362457275, koszt: 4.584717916214815e-05\nepoka: 90, theta: 1.042211651802063, koszt: 1.859027886217772e-05\n\n\nJeszcze jeden przykÅ‚ad\n\nNapisz obwÃ³d kwantowy, ktÃ³ry zawieraÄ‡ bÄ™dzie bramkÄ™ \\(R_X\\) dla parametru \\(\\theta_1\\) oraz \\(R_Y\\) dla parametru \\(\\theta_2\\)\noblicz i uzasadnij, Å¼e wartoÅ›Ä‡ oczekiwana dla stanu \\(\\ket{\\psi} = R_Y(\\theta_2) R_X(\\theta_1) \\, \\ket{0}\\)\n\n\\[&lt;Z&gt;  = \\cos(\\theta_1) \\cos(\\theta_2)\\]\nMozliwe wartoÅ›ci Å›redniej zawierajÄ… siÄ™ w przedziale \\(-1\\), \\(1\\).\nPrzyjmij zaÅ‚ozenie, ze optymalne rozwiÄ…zanie realizowane jest dla wartoÅ›ci oczekiwanej = 0.4\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta[0], wires=0)\n    qml.RY(theta[1], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.4)**2\n\ntheta = np.array([0.01, 0.02], requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: [0.01119924 0.02239872], koszt: 0.3596238551650218\nepoka: 10, theta: [0.03468299 0.06939827], koszt: 0.35640059384126277\nepoka: 20, theta: [0.10485556 0.21069384], koszt: 0.3277736421372642\nepoka: 30, theta: [0.26595847 0.55025891], koszt: 0.17843868824086426\nepoka: 40, theta: [0.41114867 0.91214351], koszt: 0.02593550926609833\nepoka: 50, theta: [0.45600131 1.05610411], koszt: 0.0017612620807984237\nepoka: 60, theta: [0.46619699 1.09390217], koszt: 0.00010074458607215528\nepoka: 70, theta: [0.4685347  1.10295946], koszt: 5.557697121461739e-06\nepoka: 80, theta: [0.469078   1.10508776], koszt: 3.040948516747214e-07\nepoka: 90, theta: [0.46920476 1.10558565], koszt: 1.6607272093790385e-08\nOptymalizacja zakonczona dla theta=[0.46923296 1.10569646], koszt: 1.2125189676042736e-09\n\n\n\n\nCelem jest znalezienie najmnieszej wartoÅ›ci wÅ‚asnej dla Hamiltonianu \\(H = Z_0 Z_1 + Z_0\\)\nTego typu hamiltoniany opisujÄ… ukÅ‚ady fizyczne np. systemy spinowe.\n\\(Z_0 Z_1\\) - mozna interpretowaÄ‡ jako krawedz miedzy dwoma wierzchoÅ‚kami.\n\\(Z_0\\) - efekty lokalne wierzchoÅ‚ka 0\n\nimport pennylane as qml\nfrom pennylane import numpy as np \nimport random\n\ndev = qml.device(\"default.qubit\", wires=2)\n\nH = qml.PauliZ(0) @ qml.PauliZ(1) + qml.PauliZ(0)\n\n@qml.qnode(dev)\ndef circuit(params):\n    qml.RY(params[0], wires=0)\n    qml.RY(params[1], wires=1)\n    qml.CNOT(wires=[0,1])\n    return qml.expval(H)\n\ndef cost_fn(params):\n    return circuit(params)\n\ninit_param = [random.uniform(0, 2*3.1415) for _ in range(2)]\n\nparams = np.array(init_param, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.01)\n\nepochs = 500\nfor epoch in range(epochs):\n    params = opt.step(cost_fn, params)\n\n    if epoch % 50 == 0:\n        print(f\"epoka: {epoch}, theta: {params}, koszt: {cost_fn(params)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={params}, koszt: {cost_fn(params)}\")",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Optymalizacja i kodowanie danych"
    ]
  },
  {
    "objectID": "cwiczenia/cw4.html#klasyczne-dane",
    "href": "cwiczenia/cw4.html#klasyczne-dane",
    "title": "Optymalizacja i kodowanie danych",
    "section": "Klasyczne dane",
    "text": "Klasyczne dane\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\n\nN = 3\nwires = range(N)\ndev = qml.device('default.qubit', wires)\n\n\n@qml.qnode(dev)\ndef basis_encoding(features):\n    qml.BasisEmbedding(features, wires)\n    return qml.probs()\n\n\\[ \\ket{111} = \\ket{1}\\otimes \\ket{1} \\otimes \\ket{1} = [0 0 0 0 0 0 0 1]^T\\]\n\nbasis_encoding([1,1,1])\n\narray([0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\nbasis_encoding(7)\n\narray([0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\nqml.draw_mpl(basis_encoding)([1,1,1])\n\n\n\n\n\n\n\n\n\nn_wires = 4 \ndev = qml.device('default.qubit', wires= n_wires)\n\n@qml.qnode(dev)\ndef circ(features):\n    for i in range(len(features)):\n        if features[i] == 1:\n            qml.X(i)\n    qml.Barrier()\n    qml.Hadamard(1)\n    qml.CNOT([1,3])\n    return qml.state()\n\n\ncirc([1,0,1,0])\n\narray([0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j,\n       0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j,\n       0.        +0.j, 0.        +0.j, 0.70710678+0.j, 0.        +0.j,\n       0.        +0.j, 0.        +0.j, 0.        +0.j, 0.70710678+0.j])\n\n\n\nqml.draw_mpl(circ, level='device', scale=0.7)([1,0,1,0])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Optymalizacja i kodowanie danych"
    ]
  },
  {
    "objectID": "cwiczenia/cw4.html#amplitude-encoding",
    "href": "cwiczenia/cw4.html#amplitude-encoding",
    "title": "Optymalizacja i kodowanie danych",
    "section": "Amplitude encoding",
    "text": "Amplitude encoding\n\nimport pennylane as qml\nN = 3\nwires = range(N)\n\ndev = qml.device(\"default.qubit\", wires)\n\n@qml.qnode(dev)\ndef circuit(features):\n    qml.AmplitudeEmbedding(features, wires)\n    return qml.state()\n\n\ncircuit([0.625,0.0,0.0,0.0,0.625,0.375,0.25,0.125])\n\narray([0.625+0.j, 0.   +0.j, 0.   +0.j, 0.   +0.j, 0.625+0.j, 0.375+0.j,\n       0.25 +0.j, 0.125+0.j])\n\n\n\nimport pennylane as qml\nN = 3\nwires = range(N)\n\ndev = qml.device(\"default.qubit\", wires)\n\n@qml.qnode(dev)\ndef circuit(f=None):\n    qml.AmplitudeEmbedding(features=f, wires=dev.wires, normalize=True, pad_with=0)\n    return qml.expval(qml.PauliZ(0)), qml.state()\n\n\nvect = [0.1, -0.3, 0.5, 0.4, 0.2]\n\n\nnorm = np.linalg.norm(vect)\nnorm_vec = np.round([i / norm for i in vect], 4)\nprint(f\"Vec: {vect}, Norm{norm_vec}\")\n\nVec: [0.1, -0.3, 0.5, 0.4, 0.2], Norm[ 0.1348 -0.4045  0.6742  0.5394  0.2697]\n\n\n\nres, state = circuit(f=norm_vec)\nres2, state2 = circuit(f=vect)\n\n\nstate.real, state2.real\n\n(array([ 0.13479815, -0.40449446,  0.67419077,  0.53939262,  0.26969631,\n         0.        ,  0.        ,  0.        ]),\n array([ 0.13483997, -0.40451992,  0.67419986,  0.53935989,  0.26967994,\n         0.        ,  0.        ,  0.        ]))\n\n\n\nqml.draw_mpl(circuit)(norm_vec)\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\n\nX = data.data\ny = data.target\n\n\nX[:3]\n\narray([[1.423e+01, 1.710e+00, 2.430e+00, 1.560e+01, 1.270e+02, 2.800e+00,\n        3.060e+00, 2.800e-01, 2.290e+00, 5.640e+00, 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, 1.120e+01, 1.000e+02, 2.650e+00,\n        2.760e+00, 2.600e-01, 1.280e+00, 4.380e+00, 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, 1.860e+01, 1.010e+02, 2.800e+00,\n        3.240e+00, 3.000e-01, 2.810e+00, 5.680e+00, 1.030e+00, 3.170e+00,\n        1.185e+03]])\n\n\n\ndef prepare_ampl(x, target_len = 16):\n    padded = np.pad(x, (0, target_len - len(x)), mode=\"constant\")\n    normed = padded / np.linalg.norm(padded)\n    return np.array(normed, requires_grad=True)\n\n\nx0 = X[0]\nfeatures = prepare_ampl(x0)\n\n\nfeatures\n\ntensor([1.32644724e-02, 1.59397384e-03, 2.26512072e-03, 1.45415157e-02,\n        1.18382852e-01, 2.61001565e-03, 2.85237424e-03, 2.61001565e-04,\n        2.13461994e-03, 5.25731723e-03, 9.69434383e-04, 3.65402190e-03,\n        9.92738094e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], requires_grad=True)\n\n\n\nn_qubits = 4\ndev = qml.device('default.qubit', wires = n_qubits)\n\n@qml.qnode(dev)\ndef amplitude_circ(x):\n    qml.AmplitudeEmbedding(features=x, wires=range(n_qubits), normalize=False)\n    return qml.state()\n\n\nstate = amplitude_circ(features)\n\n\nstate\n\ntensor([1.32644724e-02+0.j, 1.59397384e-03+0.j, 2.26512072e-03+0.j,\n        1.45415157e-02+0.j, 1.18382852e-01+0.j, 2.61001565e-03+0.j,\n        2.85237424e-03+0.j, 2.61001565e-04+0.j, 2.13461994e-03+0.j,\n        5.25731723e-03+0.j, 9.69434383e-04+0.j, 3.65402190e-03+0.j,\n        9.92738094e-01+0.j, 0.00000000e+00+0.j, 0.00000000e+00+0.j,\n        0.00000000e+00+0.j], requires_grad=True)\n\n\n\n@qml.qnode(dev)\ndef amp_circ(x):\n    qml.AmplitudeEmbedding(features=x, wires=range(n_qubits), normalize=True, pad_with=0)\n    return qml.state()\n\n\nstate2 = amp_circ(X[0])\n\n\nstate2\n\narray([1.32644724e-02+0.j, 1.59397384e-03+0.j, 2.26512072e-03+0.j,\n       1.45415157e-02+0.j, 1.18382852e-01+0.j, 2.61001565e-03+0.j,\n       2.85237424e-03+0.j, 2.61001565e-04+0.j, 2.13461994e-03+0.j,\n       5.25731723e-03+0.j, 9.69434383e-04+0.j, 3.65402190e-03+0.j,\n       9.92738094e-01+0.j, 0.00000000e+00+0.j, 0.00000000e+00+0.j,\n       0.00000000e+00+0.j])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Optymalizacja i kodowanie danych"
    ]
  },
  {
    "objectID": "cwiczenia/cw4.html#angle-encoding",
    "href": "cwiczenia/cw4.html#angle-encoding",
    "title": "Optymalizacja i kodowanie danych",
    "section": "Angle encoding",
    "text": "Angle encoding\n\\[ x \\to R_k(x) \\ket{0} = e^{-i\\,x \\frac{\\sigma_k}{2}} \\ket{0} \\]\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\nfeatures= [np.pi/3, np.pi/4]\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef circ(features):\n    qml.AngleEmbedding(features=features, rotation='Y', wires=range(2))\n    return qml.probs(wires=[0,1])\n\n\nnp.round(circ(features), 3)\n\ntensor([0.64 , 0.11 , 0.213, 0.037], requires_grad=True)\n\n\n\nqml.draw_mpl(circ)(features)\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\n\nX = data.data\ny = data.target\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, np.pi))\nX_saled = scaler.fit_transform(X)\n\n\ndev = qml.device('default.qubit', wires=13)\n\n@qml.qnode(dev)\ndef emb(x):\n    qml.AngleEmbedding(x, wires=range(len(x)), rotation='Y')\n    return qml.expval(qml.PauliZ(0))\n\n\nemb(X_saled[0])\n\nnp.float64(-0.8794737512064895)\n\n\n\n@qml.qnode(dev)\ndef emb(x):\n    qml.AngleEmbedding(x, wires=range(len(x)), rotation='Y')\n    return [qml.expval(qml.PauliZ(i)) for i in range(len(x))]\n\n\nemb(X_saled[0])\n\n[np.float64(-0.8794737512064895),\n np.float64(0.8240675736145868),\n np.float64(-0.2248601123708277),\n np.float64(0.6897237772781044),\n np.float64(-0.3668542188130566),\n np.float64(-0.39017706326055457),\n np.float64(-0.2298992328822939),\n np.float64(0.6300878435817112),\n np.float64(-0.28820944852718955),\n np.float64(0.3913341989876884),\n np.float64(0.14001614496862924),\n np.float64(-0.9957653484788057),\n np.float64(-0.1915177132878786)]",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Optymalizacja i kodowanie danych"
    ]
  },
  {
    "objectID": "cwiczenia/cw2z.html",
    "href": "cwiczenia/cw2z.html",
    "title": "Laboratorium 2",
    "section": "",
    "text": "import pennylane as qml\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\nn_layers = 5\n\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\nPodstawowe importy\nimport pennylane as qml\nimport pennylane.numpy as np\nPodstawowy obiekt QNode: skÅ‚ada siÄ™ z device i funkcji kwantowej.\nDevice - okreÅ›la na jakiej maszynie bÄ™dzie wykonywany kod - symulator lub prawdziwa maszyna. - default.qubit - lightning.qubit\nFunkcja kwantowa to pythonowa definicja obwodu.\ndev = qml.device(\"default.qubit\", wires=1)\ndef quantum_function():\n    return qml.state()\ncirc = qml.QNode(quantum_function, dev)\ncirc()\n\\[ \\ket{\\psi} = \\ket{0} = [1,0]^T \\]\ndev = qml.device(\"default.qubit\", wires=1)\n\ndef quantum_function():\n    return qml.state()\n\ncirc = qml.QNode(quantum_function, dev)\n\ncirc()\nBardziej pythonowe rozwiÄ…zanie\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef quantum_function():\n    return qml.state()\n\nquantum_function()\nMoÅ¼emy teÅ¼ zwrÃ³ciÄ‡ prawdopodobieÅ„stwa stanÃ³w bazowych\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef quantum_function():\n    return qml.probs()\n\nquantum_function()\nStan superpozycji\ndev = qml.device(\"default.qubit\", wires=1)\n\nstate = np.array([1/np.sqrt(2),1/np.sqrt(2)])\n\n@qml.qnode(dev)\ndef superposiotion():\n    qml.ops.StatePrep(state, wires=0)\n    return qml.state()\n\nsuperposiotion()\n\\[ \\ket{\\psi} = \\frac{1}{\\sqrt{2}} (\\ket{0} + \\ket{1}) \\]\n@qml.qnode(dev)\ndef superposiotion():\n    qml.ops.StatePrep(state, wires=0)\n    return qml.probs()\n\nsuperposiotion()",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 2"
    ]
  },
  {
    "objectID": "cwiczenia/cw2z.html#zadanie---obwÃ³d-kwantowy-z-optymalizacjÄ…",
    "href": "cwiczenia/cw2z.html#zadanie---obwÃ³d-kwantowy-z-optymalizacjÄ…",
    "title": "Laboratorium 2",
    "section": "Zadanie - ObwÃ³d kwantowy z optymalizacjÄ…",
    "text": "Zadanie - ObwÃ³d kwantowy z optymalizacjÄ…\n\nNapisz nowy obwÃ³d kwantowy, ktÃ³ry zawieraÄ‡ bÄ™dzie tylko bramkÄ™ \\(R_X\\) dla dowolnego parametru \\(\\theta\\)\noblicz i uzasadnij, Å¼e wartoÅ›Ä‡ oczekiwana dla stanu \\(\\ket{\\psi} = R_X \\, \\ket{0}\\) \\[&lt;Z&gt; = cos^2(\\theta /2)- sin^2(\\theta /2) = cos(\\theta)\\]\n\nZaÅ‚Ã³Å¼my, Å¼e nasz problem obliczeniowy sprowadza siÄ™ do wygenerowania wartoÅ›ci oczekiwanej o wartoÅ›ci 0.5.\n\\[\n\\textbf{&lt;Z&gt;} = \\bra{\\psi} \\textbf{Z} \\ket{\\psi} = 0.5\n\\]\nNapisz program znajdujÄ…cy rozwiÄ…zanie - szukajÄ…cy wagÄ™ \\(\\theta\\) dla naszego obwodu\n\nZdefiniuj funkcjÄ™ kosztu, ktÃ³rÄ… bedziemy minimalizowaÄ‡ \\((Y - y)^2\\)\nzainicjuj rozwiÄ…zanie \\(theta=0.01\\) i przypisz do tablicy array np.array(0.01, requires_grad=True)\nJako opt wybierz spadek po gradiencie : opt = qml.GradientDescentOptimizer(stepsize=0.1)\nuzyj poniÅ¼szego kodu do wygenerowania pÄ™tli obiczeÅ„\n\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.5)**2\n\ntheta = np.array(0.01, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev, interface=\"torch\")\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\n\ndef cost_fn(theta):\n    target = 0.5\n    return (par_c(theta) - target) ** 2\n\n\nimport torch\nfrom torch.optim import Adam \n\ntheta = torch.tensor(0.01, requires_grad=True)\n\noptimizer = Adam([theta], lr=0.1)\nepochs = 100\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    loss = cost_fn(theta)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nJeszcze jeden przykÅ‚ad\n\nNapisz obwÃ³d kwantowy, ktÃ³ry zawieraÄ‡ bÄ™dzie bramkÄ™ \\(R_X\\) dla parametru \\(\\theta_1\\) oraz \\(R_Y\\) dla parametru \\(\\theta_2\\)\noblicz i uzasadnij, Å¼e wartoÅ›Ä‡ oczekiwana dla stanu \\(\\ket{\\psi} = R_Y(\\theta_2) R_X(\\theta_1) \\, \\ket{0}\\)\n\n\\[&lt;Z&gt;  = \\cos(\\theta_1) \\cos(\\theta_2)\\]\nMozliwe wartoÅ›ci Å›redniej zawierajÄ… siÄ™ w przedziale \\(-1\\), \\(1\\).\nPrzyjmij zaÅ‚ozenie, ze optymalne rozwiÄ…zanie realizowane jest dla wartoÅ›ci oczekiwanej = 0.4\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta[0], wires=0)\n    qml.RY(theta[1], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.4)**2\n\ntheta = np.array([0.01, 0.02], requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\n\nZadanie\nCelem jest znalezienie najmnieszej wartoÅ›ci wÅ‚asnej dla Hamiltonianu \\(H = Z_0 Z_1 + Z_0\\)\nTego typu hamiltoniany opisujÄ… ukÅ‚ady fizyczne np. systemy spinowe.\n\\(Z_0 Z_1\\) - mozna interpretowaÄ‡ jako krawedz miedzy dwoma wierzchoÅ‚kami.\n\\(Z_0\\) - efekty lokalne wierzchoÅ‚ka 0\n\nimport pennylane as qml\nfrom pennylane import numpy as np \nimport random\n\ndev = qml.device(\"default.qubit\", wires=2)\n\nH = qml.PauliZ(0) @ qml.PauliZ(1) + qml.PauliZ(0)\n\n@qml.qnode(dev)\ndef circuit(params):\n    qml.RY(params[0], wires=0)\n    qml.RY(params[1], wires=1)\n    qml.CNOT(wires=[0,1])\n    return qml.expval(H)\n\ndef cost_fn(params):\n    return circuit(params)\n\ninit_param = [random.uniform(0, 2*3.1415) for _ in range(2)]\n\nparams = np.array(init_param, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.01)\n\nepochs = 500\nfor epoch in range(epochs):\n    params = opt.step(cost_fn, params)\n\n    if epoch % 50 == 0:\n        print(f\"epoka: {epoch}, theta: {params}, koszt: {cost_fn(params)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={params}, koszt: {cost_fn(params)}\")",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 2"
    ]
  },
  {
    "objectID": "cwiczenia/cw2z.html#klasyczne-dane",
    "href": "cwiczenia/cw2z.html#klasyczne-dane",
    "title": "Laboratorium 2",
    "section": "klasyczne dane",
    "text": "klasyczne dane\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\n\nN = 3\nwires = range(N)\ndev = qml.device('default.qubit', wires)\n\n\n@qml.qnode(dev)\ndef basis_encoding(features):\n    qml.BasisEmbedding(features, wires)\n    return qml.probs()\n\n\\[ \\ket{111} = \\ket{1}\\otimes \\ket{1} \\otimes \\ket{1} = [0 0 0 0 0 0 0 1]^T\\]\n\nbasis_encoding([1,1,1])\n\n\nbasis_encoding(7)\n\n\nqml.draw_mpl(basis_encoding)([1,1,1])\n\n\nn_wires = 4 \ndev = qml.device('default.qubit', wires= n_wires)\n\n@qml.qnode(dev)\ndef circ(features):\n    for i in range(len(features)):\n        if features[i] == 1:\n            qml.X(i)\n    qml.Barrier()\n    qml.Hadamard(1)\n    qml.CNOT([1,3])\n    return qml.state()\n\n\ncirc([1,0,1,0])\n\n\nqml.draw_mpl(circ, level='device', scale=0.7)([1,0,1,0])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 2"
    ]
  },
  {
    "objectID": "cwiczenia/cw2z.html#amplitude-encoding",
    "href": "cwiczenia/cw2z.html#amplitude-encoding",
    "title": "Laboratorium 2",
    "section": "Amplitude encoding",
    "text": "Amplitude encoding\n\nimport pennylane as qml\nN = 3\nwires = range(N)\n\ndev = qml.device(\"default.qubit\", wires)\n\n@qml.qnode(dev)\ndef circuit(features):\n    qml.AmplitudeEmbedding(features, wires)\n    return qml.state()\n\n\ncircuit([0.625,0.0,0.0,0.0,0.625,0.375,0.25,0.125])\n\n\nimport pennylane as qml\nN = 3\nwires = range(N)\n\ndev = qml.device(\"default.qubit\", wires)\n\n@qml.qnode(dev)\ndef circuit(f=None):\n    qml.AmplitudeEmbedding(features=f, wires=dev.wires, normalize=True, pad_with=0)\n    return qml.expval(qml.PauliZ(0)), qml.state()\n\n\nvect = [0.1, -0.3, 0.5, 0.4, 0.2]\n\n\nnorm = np.linalg.norm(vect)\nnorm_vec = np.round([i / norm for i in vect], 4)\nprint(f\"Vec: {vect}, Norm{norm_vec}\")\n\n\nres, state = circuit(f=norm_vec)\nres2, state2 = circuit(f=vect)\n\n\nstate.real, state2.real\n\n\nqml.draw_mpl(circuit)(norm_vec)\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\n\nX = data.data\ny = data.target\n\n\ndef prepare_ampl(x, target_len = 16):\n    padded = np.pad(x, (0, target_len - len(x)), mode=\"constant\")\n    normed = padded / np.linalg.norm(padded)\n    return np.array(normed, requires_grad=True)\n\n\nx0 = X[0]\nfeatures = prepare_ampl(x0)\n\n\nfeatures\n\ntensor([1.32644724e-02, 1.59397384e-03, 2.26512072e-03, 1.45415157e-02,\n        1.18382852e-01, 2.61001565e-03, 2.85237424e-03, 2.61001565e-04,\n        2.13461994e-03, 5.25731723e-03, 9.69434383e-04, 3.65402190e-03,\n        9.92738094e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], requires_grad=True)\n\n\n\nn_qubits = 4\ndev = qml.device('default.qubit', wires = n_qubits)\n\n@qml.qnode(dev)\ndef amplitude_circ(x):\n    qml.AmplitudeEmbedding(features=x, wires=range(n_qubits), normalize=False)\n    return qml.state()\n\n\nstate = amplitude_circ(features)\n\n\nstate\n\ntensor([1.32644724e-02+0.j, 1.59397384e-03+0.j, 2.26512072e-03+0.j,\n        1.45415157e-02+0.j, 1.18382852e-01+0.j, 2.61001565e-03+0.j,\n        2.85237424e-03+0.j, 2.61001565e-04+0.j, 2.13461994e-03+0.j,\n        5.25731723e-03+0.j, 9.69434383e-04+0.j, 3.65402190e-03+0.j,\n        9.92738094e-01+0.j, 0.00000000e+00+0.j, 0.00000000e+00+0.j,\n        0.00000000e+00+0.j], requires_grad=True)\n\n\n\n@qml.qnode(dev)\ndef amp_circ(x):\n    qml.AmplitudeEmbedding(features=x, wires=range(n_qubits), normalize=True, pad_with=0)\n    return qml.state()\n\n\nstate2 = amp_circ(X[0])\n\n\nstate2\n\narray([1.32644724e-02+0.j, 1.59397384e-03+0.j, 2.26512072e-03+0.j,\n       1.45415157e-02+0.j, 1.18382852e-01+0.j, 2.61001565e-03+0.j,\n       2.85237424e-03+0.j, 2.61001565e-04+0.j, 2.13461994e-03+0.j,\n       5.25731723e-03+0.j, 9.69434383e-04+0.j, 3.65402190e-03+0.j,\n       9.92738094e-01+0.j, 0.00000000e+00+0.j, 0.00000000e+00+0.j,\n       0.00000000e+00+0.j])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 2"
    ]
  },
  {
    "objectID": "cwiczenia/cw2z.html#angle-encoding",
    "href": "cwiczenia/cw2z.html#angle-encoding",
    "title": "Laboratorium 2",
    "section": "Angle encoding",
    "text": "Angle encoding\n\\[ x \\to R_k(x) \\ket{0} = e^{-i\\,x \\frac{\\sigma_k}{2}} \\ket{0} \\]\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\nfeatures= [np.pi/3, np.pi/4]\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef circ(features):\n    qml.AngleEmbedding(features=features, rotation='Y', wires=range(2))\n    return qml.probs(wires=[0,1])\n\n\nnp.round(circ(features), 3)\n\ntensor([0.64 , 0.11 , 0.213, 0.037], requires_grad=True)\n\n\n\nqml.draw_mpl(circ)(features)\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\n\nX = data.data\ny = data.target\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, np.pi))\nX_saled = scaler.fit_transform(X)\n\n\ndev = qml.device('default.qubit', wires=13)\n\n@qml.qnode(dev)\ndef emb(x):\n    qml.AngleEmbedding(x, wires=range(len(x)), rotation='Y')\n    return qml.expval(qml.PauliZ(0))\n\n\nemb(X_saled[0])\n\nnp.float64(-0.8794737512064895)\n\n\n\n@qml.qnode(dev)\ndef emb(x):\n    qml.AngleEmbedding(x, wires=range(len(x)), rotation='Y')\n    return [qml.expval(qml.PauliZ(i)) for i in range(len(x))]\n\n\nemb(X_saled[0])\n\n[np.float64(-0.8794737512064895),\n np.float64(0.8240675736145868),\n np.float64(-0.2248601123708277),\n np.float64(0.6897237772781044),\n np.float64(-0.3668542188130566),\n np.float64(-0.39017706326055457),\n np.float64(-0.2298992328822939),\n np.float64(0.6300878435817112),\n np.float64(-0.28820944852718955),\n np.float64(0.3913341989876884),\n np.float64(0.14001614496862924),\n np.float64(-0.9957653484788057),\n np.float64(-0.1915177132878786)]",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Zaoczne",
      "Laboratorium 2"
    ]
  },
  {
    "objectID": "cwiczenia/cw5.html",
    "href": "cwiczenia/cw5.html",
    "title": "Modele Support Vector Machines",
    "section": "",
    "text": "from sklearn.datasets import make_blobs\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nX, y = make_blobs(n_samples=100, centers=2, random_state=6)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nsvm_clf = Pipeline(\n    [\n        ('scaler', StandardScaler()),\n        (\"linear_svc\", LinearSVC(C=1, loss='hinge'))\n    ]\n)\n\nsvm_clf.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('linear_svc', LinearSVC(C=1, loss='hinge'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('scaler', ...), ('linear_svc', ...)]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    LinearSVC?Documentation for LinearSVC\n        \n            \n                Parameters\n                \n\n\n\n\npenaltyÂ \n'l2'\n\n\n\nlossÂ \n'hinge'\n\n\n\ndualÂ \n'auto'\n\n\n\ntolÂ \n0.0001\n\n\n\nCÂ \n1\n\n\n\nmulti_classÂ \n'ovr'\n\n\n\nfit_interceptÂ \nTrue\n\n\n\nintercept_scalingÂ \n1\n\n\n\nclass_weightÂ \nNone\n\n\n\nverboseÂ \n0\n\n\n\nrandom_stateÂ \nNone\n\n\n\nmax_iterÂ \n1000\nprint(f\"Test acc: {svm_clf.score(X_test, y_test):.2f}\")\n\nTest acc: 1.00\ndef plot_svm_pipeline(pipeline, X, y):\n    scaler = pipeline.named_steps['scaler']\n    svc = pipeline.named_steps['linear_svc']\n    X_scaled = scaler.transform(X)\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='autumn')\n    ax = plt.gca()\n\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = svc.decision_function(xy).reshape(XX.shape)\n\n    ax.contour(XX, YY, Z, color='k', levels=[-1,0,1], linestyles=['--','-','--'])\n\n    plt.title(\"SVM Decision Boundry\")\n    plt.show()\n\nplot_svm_pipeline(svm_clf, X, y)\n\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_20467/3794771256.py:17: UserWarning: The following kwargs were not used by contour: 'color'\n  ax.contour(XX, YY, Z, color='k', levels=[-1,0,1], linestyles=['--','-','--'])",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele Support Vector Machines"
    ]
  },
  {
    "objectID": "cwiczenia/cw5.html#kernel-trick",
    "href": "cwiczenia/cw5.html#kernel-trick",
    "title": "Modele Support Vector Machines",
    "section": "Kernel trick",
    "text": "Kernel trick\nDla prawdziwych danych trudno oczekiwaÄ‡ aby byÅ‚y one liniowo separowalne.\nDlatego jednym z rozwiÄ…zaÅ„ jest stworzenie odwzorowania do wyÅ¼ej wymiarowej przestrzeni tak by dane w niej byÅ‚y juÅ¼ liniowo separowalne. Obliczenie takiej transformacji dla dowolnych danych jest bardzo trudne, dlatego moÅ¼emy zastosowaÄ‡ tzw kernel trick. Potrzebujemy tylko obliczyÄ‡ iloczyn skalarny: \\[ K(x,x') = &lt;\\phi(x), \\phi(x')&gt;\\] bez jawnego wyznaczania \\(\\phi\\).\n\nx, xâ€™ wektory wejÅ›ciowe z oryginalnej przestrzeni\n\\(\\phi(x)\\) odwzorowanie do przestrzeni o wyÅ¼szym wymiarze\n\\(K(x, x')\\) funkcja jÄ…drowa - kernel function - oblicza iloczy skalarny w zadanej przestrzeni.\n\n\nLinear - \\(K(x, x') = x^{T}x'\\)\nPolynomial - \\(K(x,x') = (x^{T}x' +c)^d\\)\nRBF - \\(K(x,x') = exp(-\\gamma \\, |x-x'|^2)\\)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_moons\nfrom sklearn.svm import SVC\n\nX,y = make_moons(n_samples=200, noise=0.2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=44)\n\npoly_svm_clf = Pipeline([\n    ('polu_features', PolynomialFeatures(degree=3)),\n    ('scaler', StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=1, loss='hinge'))\n])\n\npoly_svm_clf.fit(X_train, y_train)\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\nPipeline(steps=[('polu_features', PolynomialFeatures(degree=3)),\n                ('scaler', StandardScaler()),\n                ('linear_svc', LinearSVC(C=1, loss='hinge'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('polu_features', ...), ('scaler', ...), ...]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    PolynomialFeatures?Documentation for PolynomialFeatures\n        \n            \n                Parameters\n                \n\n\n\n\ndegreeÂ \n3\n\n\n\ninteraction_onlyÂ \nFalse\n\n\n\ninclude_biasÂ \nTrue\n\n\n\norderÂ \n'C'\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    LinearSVC?Documentation for LinearSVC\n        \n            \n                Parameters\n                \n\n\n\n\npenaltyÂ \n'l2'\n\n\n\nlossÂ \n'hinge'\n\n\n\ndualÂ \n'auto'\n\n\n\ntolÂ \n0.0001\n\n\n\nCÂ \n1\n\n\n\nmulti_classÂ \n'ovr'\n\n\n\nfit_interceptÂ \nTrue\n\n\n\nintercept_scalingÂ \n1\n\n\n\nclass_weightÂ \nNone\n\n\n\nverboseÂ \n0\n\n\n\nrandom_stateÂ \nNone\n\n\n\nmax_iterÂ \n1000\n\n\n\n\n            \n        \n    \n\n\n\nprint(f\"Test acc: {poly_svm_clf.score(X_test, y_test):.2f}\")\n\nTest acc: 0.88\n\n\n\nclassical_kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n\nfor kernel in classical_kernels:\n    classical_svc = SVC(kernel=kernel, decision_function_shape='ovr', degree=3)\n    classical_svc.fit(X_train, y_train)\n    classical_score = classical_svc.score(X_test, y_test)\n\n    print(f\"{kernel} kernel classfication test score: {classical_score:.2f}\")\n\nlinear kernel classfication test score: 0.83\npoly kernel classfication test score: 0.87\nrbf kernel classfication test score: 0.97\nsigmoid kernel classfication test score: 0.70\n\n\n\nlinear_svm = SVC(kernel='linear', C=1)\nlinear_svm.fit(X_train, y_train)\ny_pred_linear = linear_svm.predict(X_test)\nacc_linear = accuracy_score(y_test, y_pred_linear)\n\n\nrbf_svm = SVC(kernel='rbf', C=1, gamma='scale')\nrbf_svm.fit(X_train, y_train)\ny_pred_rbf = rbf_svm.predict(X_test)\nacc_rbf = accuracy_score(y_test, y_pred_rbf)\n\n\ndef plot_decision_boundry(model, X, y, title):\n    h = 0.02\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()+1\n    y_min, y_max = X[:, 1].min(), X[:, 1].max()+1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()\n\n\nplot_decision_boundry(linear_svm, X_test, y_test, \"linear SVM\")\nplot_decision_boundry(rbf_svm, X_test, y_test, \"RBF SVM\")",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele Support Vector Machines"
    ]
  },
  {
    "objectID": "cwiczenia/cw5.html#idea-swap-testu",
    "href": "cwiczenia/cw5.html#idea-swap-testu",
    "title": "Modele Support Vector Machines",
    "section": "Idea swap testu",
    "text": "Idea swap testu\nSwap test sÅ‚uÅ¼y do obliczania wartoÅ›ci\n\\[ |\\langle \\psi |\\phi \\rangle |^2 \\]\nczyli kwadratu moduÅ‚u iloczynu skalarnego dwÃ³ch stanÃ³w kwantowych \\(|\\psi \\rangle\\) i \\(|\\phi \\rangle\\) .\n\nğŸ”§ ObwÃ³d swap testu\nSwap test uÅ¼ywa dodatkowego kubitu kontrolnego oraz bramki SWAP\nKontrolny kubit realizowany jest w stanie \\(|0\\rangle\\).\n\\[ \\ket{\\psi_0} = \\ket{0} \\otimes \\ket{\\psi} \\otimes \\ket{\\phi} \\]\n\n\nğŸ›ï¸ Jak to dziaÅ‚a\n\nZastosuj Hadamarda (zamiana bazy) na kontrolny (ancilla) kubit \\[ \\ket{\\psi_1} = (\\ket{0} + \\ket{1}) \\otimes \\ket{\\psi} \\otimes \\ket{\\phi} \\]\nZastosuj CSWAP (3 kubitowa bramka - controll = ancilla)\nZastosuj Hadamarda (powrÃ³t do bazy)\nPomiar ancilla kubitu.\n\nPrawdopodobieÅ„stwo, Å¼e kontrolny kubit da wynik \\(0\\), wynosi: \\[P(0)=\\frac{1+|\\langle \\psi |\\phi \\rangle |^2}{2}\\]\nPrawdopodobieÅ„stwo, Å¼e kontrolny kubit da wynik 1, wynosi: \\[P(1)=\\frac{1-|\\langle \\psi |\\phi \\rangle |^2}{2}\\]\nDziÄ™ki temu, mierzÄ…c kontrolny kubit, moÅ¼emy wyznaczyÄ‡ overlap miÄ™dzy stanami.\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\ndev_test = qml.device('default.qubit', wires=['ancilla','phi','psi'], shots=5000)\n\n@qml.qnode(dev_test)\ndef swap_test():\n    qml.Hadamard(wires='ancilla')\n    \n    qml.X(wires=['phi'])\n    qml.Hadamard(wires=['psi'])\n\n    qml.CSWAP(wires=['ancilla', 'phi', 'psi'])\n    qml.Hadamard(wires='ancilla')\n    return qml.sample(wires='ancilla')\n\nres = swap_test()\n\nprint(f\"P(0) = {np.mean(res==0)}, P(1) = {np.mean(res == 1)}\")\nprint(f\"{2*np.mean(res==0) - 1}\")\n\nP(0) = 0.7506, P(1) = 0.2494\n0.5012000000000001\n\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\n\n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef phi():\n    qml.X(wires=0)\n    return qml.state()\n\n@qml.qnode(dev)\ndef psi():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\ndef theory(phi, psi):\n    inner = np.vdot(phi, psi)\n    return float(np.abs(inner)**2)\n\n\ntheory(psi(), phi())\n\n0.4999999999999999",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele Support Vector Machines"
    ]
  },
  {
    "objectID": "cwiczenia/cw5.html#quantum-embedding",
    "href": "cwiczenia/cw5.html#quantum-embedding",
    "title": "Modele Support Vector Machines",
    "section": "Quantum Embedding",
    "text": "Quantum Embedding\nKwantowy Embedding reprezentuje klasyczne dane jako stan (wektor) w przestrzeni Hilberta. Odwzorowanie, ktÃ³re generuje embedding nazywamy quantum feature map.\nFeature map: \\(\\phi: X \\to F\\) gdzie \\(F\\) to nowa przestrzeÅ„ Hilberta stanÃ³w. \\[ x \\to \\ket{\\phi(x)} \\]\nW naszym przypadku to odwzorowanie realizujÄ… \\(U_{\\phi}(x)\\) macierze kodowania kÄ…towego. \\[ \\ket{0} \\to U_{\\phi}(x)\\ket{0} \\]\n\nX[:5], y[:5]\n\n(array([[-1.10689665e+00,  4.22928095e-02],\n        [ 9.56799641e-01,  4.56750492e-01],\n        [ 7.33516277e-01,  5.84617437e-01],\n        [ 1.11140659e+00, -3.09213987e-01],\n        [ 2.09081764e-01,  6.56679495e-04]]),\n array([0, 0, 0, 1, 1]))\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, np.pi))\nX_scaled = scaler.fit_transform(X)\n\ny_scaled = 2 * y -1 \n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n\n\nX_train[:5], y_train[:5]\n\n(tensor([[1.89947175, 1.87042604],\n         [1.3509262 , 1.17853671],\n         [1.99374881, 1.98367299],\n         [2.61730918, 0.87705533],\n         [1.34724759, 1.9366214 ]], requires_grad=False),\n tensor([-1,  1, -1,  1, -1], requires_grad=False))\n\n\n\nn_qubits = len(X_train[0])\nn_qubits\n\n2\n\n\nRozwaÅ¼my model kwantowy w postaci: \\[\nf(x) = \\braket{\\phi(x) | M | \\phi{x} }\n\\]\nModel moÅ¼e byÄ‡ realizowany jako wariacyjny obwÃ³d kwantowy.\nZamiast jednak trenowaÄ‡ parametry dla takiego obwodu moÅ¼emy wykorzystaÄ‡ kwantowy kernel ktÃ³ry realizuje siÄ™ przez SWAP test.\nZamiast SWAP testu moÅ¼emy wykorzystaÄ‡ inny obwÃ³d SzczegÃ³Å‚y tutaj\n\nfrom pennylane.templates import AngleEmbedding\n\ndev_kernel = qml.device('lightning.qubit', wires= n_qubits)\n\nprojector = np.zeros((2 ** n_qubits, 2 ** n_qubits))\nprojector[0, 0] = 1\n\n\n@qml.qnode(dev_kernel)\ndef kernel(x1, x2):\n    AngleEmbedding(x1, wires=range(n_qubits))\n    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))\n\n\nX_train = np.array(X_train, requires_grad=False)\nX_test = np.array(X_test, requires_grad=False)\n\ny_train = np.array(y_train, requires_grad=False)\ny_test = np.array(y_test, requires_grad=False)\n\n\nkernel(X_train[0], X_train[0]), kernel(X_test[0], X_test[1])\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n(array(1.), array(0.95997686))\n\n\n\ndef kernel_matrix(A, B):\n    return np.array([[kernel(a,b) for b in B] for a in A])\n\n\nsvm = SVC(kernel=kernel_matrix).fit(X_train, y_train)\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n\npredictions = svm.predict(X_test)\n\n\nprint(f\"model qsvm {accuracy_score(predictions, y_test):.4f}\")\n\nmodel qsvm 0.8600\n\n\n\nsvm.predict(X_test[:4]), y_test[:4]\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n(array([-1, -1,  1,  1]), tensor([-1, -1,  1, -1], requires_grad=False))\n\n\n\n@qml.qnode(dev_kernel)\ndef kernel2(x1, x2):\n    AngleEmbedding(x1, wires=range(n_qubits))\n    qml.CNOT([0,1])\n    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n    qml.CNOT([0,1])\n    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))\n\ndef kernel_matrix2(A, B):\n    return np.array([[kernel2(a, b) for b in B] for a in A])\n\n\nsvm2 = SVC(kernel=kernel_matrix2).fit(X_train, y_train)\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n\npredictions2 = svm2.predict(X_test)\n\n\nprint(f\"acc: {accuracy_score(predictions2, y_test):.4f}\")\n\nacc: 0.5000\n\n\n\ndef zz_feature_map(x, wires=[0,1]):\n    for i in range(len(wires)):\n        qml.Hadamard(wires=wires[i])\n\n    for i in range(len(wires)):\n        qml.RX(2 * x[i], wires=wires[i])\n\n    theta = (np.pi - x[0])* (np.pi - x[i])\n    qml.CNOT(wires=[wires[0], wires[1]])\n    qml.RZ(2 * theta, wires=wires[1])\n    qml.CNOT(wires=[wires[0], wires[1]])\n\n\nqml.draw_mpl(zz_feature_map, style=\"pennylane\", decimals=2, level='device')(X_train[0])\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/drawer/draw.py:801: UserWarning: When the input to qml.draw is not a QNode, the level argument is ignored.\n  warnings.warn(\nMatplotlib is building the font cache; this may take a moment.\n\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"lightning.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"numpy\")\ndef kernel3(x1,x2):\n    zz_feature_map(x1, wires=range(2))\n    qml.adjoint(zz_feature_map)(x2, wires=range(2))\n    return qml.expval(qml.Hermitian(projector, wires=range(2)))\n\ndef kernel_matrix3(A, B): \n    return np.array([[kernel3(a, b) for b in B] for a in A])\n\n\nsvm3 = SVC(kernel=kernel_matrix3).fit(X_train, y_train)\n\n/Users/seba/Documents/GitHub/qml2025/venv/lib/python3.13/site-packages/pennylane/devices/preprocess.py:283: UserWarning: Differentiating with respect to the input parameters of Hermitian is not supported with the adjoint differentiation method. Gradients are computed only with regards to the trainable parameters of the circuit.\n\n Mark the parameters of the measured observables as non-trainable to silence this warning.\n  warnings.warn(\n\n\n\npredictions3 = svm3.predict(X_test)\nprint(f\"acc: {accuracy_score(predictions3, y_test):.4f}\")\n\nacc: 0.7000\n\n\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport pennylane.numpy as np \n\n\ndef load_mnist(n_qubits):\n    mnist = fetch_openml('mnist_784', version=1)\n    X, y = mnist.data, mnist.target\n    mask = (y == '3') | (y == '6')\n    X_filter = X[mask]\n    y_filter = y[mask]\n\n    X_filter = X_filter[:300]\n    y_filter = y_filter[:300]\n\n    y_fiter = np.where(y_filter == '3', 0, 1)\n\n    pca = PCA(n_components=n_qubits)\n    X_reduced = pca.fit_transform(X_filter)\n    scaler = StandardScaler().fit(X_reduced)\n    X_scaled = scaler.transform(X_reduced)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_filter, test_size=0.2, random_state=44\n    )\n    return X_train, X_test, y_train, y_test\n\n\nn_qubit = 6\nX_train, X_test, y_train, y_test = load_mnist(n_qubits=n_qubits)\n\n\nX_train.shape\n\n(240, 2)\n\n\n\ndef visualize(x, labels):\n    import matplotlib.pyplot as plt \n    from sklearn.manifold import TSNE\n    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n    label2name = {0: '3', 1: '6'}\n    mnist_tsne = tsne.fit_transform(x)\n    for label in np.unique(labels):\n        indices = labels == label\n        plt.scatter(mnist_tsne[indices, 0], mnist_tsne[indices, 1])\n    plt.title(\"t-SNE for 3 and 6\")\n    plt.xlabel(\"t-SNE dim 1\")\n    plt.ylabel(\"t-SNE dim 2\")\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize(X_train, y_train)\n\n/var/folders/53/b8z3c5xs0l51w2mzflnyk6400000gn/T/ipykernel_20467/1255198376.py:13: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  plt.legend()\n\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"lightning.qubit\", wires=n_qubit)\n\n@qml.qnode(dev)\ndef kernel_mnist(x1,x2, n_qubit):\n    qml.AngleEmbedding(x1, wires=range(n_qubit))\n    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubit))\n    return qml.expval(qml.Projector([0]*n_qubit, wires=range(n_qubit)))\n\n\ndef kernel_mat(A, B):\n    return np.array([[kernel_mnist(a, b, 6) for b in B] for a in A])\n\n\nsvm = SVC(kernel=kernel_mat)\n\n\nsvm.fit(X_train, y_train)\n\nSVC(kernel=&lt;function kernel_mat at 0x16b7ac0e0&gt;)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nCÂ \n1.0\n\n\n\nkernelÂ \n&lt;function ker...t 0x16b7ac0e0&gt;\n\n\n\ndegreeÂ \n3\n\n\n\ngammaÂ \n'scale'\n\n\n\ncoef0Â \n0.0\n\n\n\nshrinkingÂ \nTrue\n\n\n\nprobabilityÂ \nFalse\n\n\n\ntolÂ \n0.001\n\n\n\ncache_sizeÂ \n200\n\n\n\nclass_weightÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\nmax_iterÂ \n-1\n\n\n\ndecision_function_shapeÂ \n'ovr'\n\n\n\nbreak_tiesÂ \nFalse\n\n\n\nrandom_stateÂ \nNone\n\n\n\n\n            \n        \n    \n\n\n\npred = svm.predict(X_test)\nprint(f\"Acc: {accuracy_score(y_test, pred)}\")\n\nAcc: 0.9166666666666666",
    "crumbs": [
      "Sylabus",
      "Ä†wiczenia Dzienne",
      "Modele Support Vector Machines"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html",
    "href": "lectures/wyklad3.html",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w",
    "section": "",
    "text": "\\[\n\\newcommand{\\bra}[1]{\\left \\langle #1 \\right \\rvert}\n\\newcommand{\\ket}[1]{\\left \\rvert #1 \\right \\rangle}\n\\newcommand{\\braket}[2]{\\left \\langle #1 \\middle \\rvert #2 \\right \\rangle}\n\\]\nMechanika Kwantowa opiera siÄ™ na algebrze liniowej.\nW ogÃ³lnoÅ›ci teoria ta posÅ‚uguje siÄ™ pojÄ™ciem nieskoÅ„czenie wymiarowej przestrzeni liniowej. Na szczÄ™Å›cie do opisu kubitÃ³w (2-dim) i ukÅ‚adÃ³w kwantowych (\\(2^{n}\\)-dim) wystarczy nam pojÄ™cie skoÅ„czenie wymiarowej przestrzeni wektorowej. Bardzo upraszcza nam to naukÄ™ o kwantowym uczeniu maszynowym, gdyÅ¼ wiele problemÃ³w matematycznych (dla fizykÃ³w) tutaj nie wystÄ™puje. Upraszcza to rÃ³wnieÅ¼ iloÅ›Ä‡ potrzebnych matematycznych pojÄ™Ä‡.\nBÄ™dziemy posÅ‚ugiwali siÄ™ notacjÄ… Diraca, jednego z twÃ³rcÃ³w mechaniki kwantowej. W ksiÄ…Å¼ce Ch. Bernhardta â€œObliczenia kwantowe dla kaÅ¼degoâ€ autor rezygnuje z liczb zespolonych, na rzecz liczb rzeczywistych. O ile podejÅ›cie takie sprawdza siÄ™ na poziomie opisu o tyle dla peÅ‚nego zrozumienia posÅ‚ugiwanie siÄ™ liczbami zespolonymi jest niezbÄ™dne.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#liczby-rzeczywiste-i-zespolone---przypomnienie",
    "href": "lectures/wyklad3.html#liczby-rzeczywiste-i-zespolone---przypomnienie",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w",
    "section": "Liczby rzeczywiste i zespolone - przypomnienie",
    "text": "Liczby rzeczywiste i zespolone - przypomnienie\nLiczby to matematyczne, abstrakcyjne pojÄ™cia wywodzÄ…ce siÄ™ z teorii mnogoÅ›ci (zbiorÃ³w). PrzykÅ‚adowo, liczbÄ™ 42 moÅ¼na zapisa w postaci dziesiÄ™tnej lub binarnej \\(42=101010_2\\). MoÅ¼emy znaleÅºÄ‡ 42 przedmioty i je przeliczyÄ‡, ale w naszym przypadku skupimy siÄ™ na abstrakcyjnym pojÄ™ciu liczby, niezaleÅ¼nie od jej reprezentacji. Liczba 42 jest liczbÄ… naturalnÄ…. ZbiÃ³r liczb naturalnych oznaczamy jako \\(\\mathbb{N}\\). Identyczne cechy abstrakcji majÄ… liczby caÅ‚kowite \\(\\mathbb{Z}\\), liczby wymierne \\(\\mathbb{Q}\\), liczby rzeczywiste \\(\\mathbb{R}\\) oraz liczby zespolone \\(\\mathbb{C}\\). Nie moÅ¼emy zobaczyÄ‡ ani dotknÄ…Ä‡ liczb, ale moÅ¼emy wykonywaÄ‡ na nich operacje matematyczne. Liczb Warto zaznaczyc,Å¼e liczby zespolone nie sÄ… bardziej abstrakcyjne niÅ¼ liczby rzeczywiste, czy naturalne.\nLiczba zespolona (we wspÃ³Å‚rzÄ™dnych KartezjaÅ„skich) skÅ‚ada siÄ™ z (dwÃ³ch liczb rzeczywistych) czÄ™Å›ci rzeczywistej i urojonej: \\[z=x + i y\\] gdzie \\(i^2=-1\\).\nCzÄ™Å›Ä‡ rzeczywista to \\(R(z)=x\\) i czÄ™Å›c urojona to \\(I(z)=y\\).\nNa przykÅ‚ad: \\[1+i\\sqrt{3}\\] \\(R(z)=1\\) i \\(I(z)=\\sqrt{3}\\).\nInaczej mÃ³wiÄ…c, liczba zespolona jest sumÄ… liczby rzeczywistej i urojonej.\nLiczy zespolone, moÅ¼na traktowac jako punkty na pÅ‚aszczyÅºnie o wspÃ³Å‚rzÄ™dnych \\(x\\) i \\(y\\).\n\nKaÅ¼dÄ… liczbÄ™ zespolonÄ… moÅ¼emy zapisaÄ‡ w postaci polarnej (wspÃ³Å‚rzÄ™dne biegunowe)\n\\[ z=r\\, e^{i \\phi} , \\] gdzie \\(r=|z|\\) to moduÅ‚ liczby zespolonej, a \\(\\phi\\) to jej argument czyli wyraÅ¼ony w radianach kÄ…t miÄ™dzy osiÄ… rzeczywistÄ… a pÃ³Å‚prostÄ… poprowadzonÄ… od Å›rodka ukÅ‚. wsp. i przechodzÄ…cÄ… przez punkt \\(z\\).\n\\[ z = r\\, e^{i \\, \\phi} = r\\, (\\cos{\\phi} + i\\, \\sin{\\phi}) \\] gdzie: \\[ r = |z| = \\sqrt{x^2 + y^2} \\] \\[ \\phi = \\arctan{\\frac{y}{x}}. \\]\nNatomiast: \\[ x = r \\cos{\\phi} \\] \\[ y = r \\sin{\\phi} \\]\nDla naszego przykÅ‚adu: \\[ 1+i\\sqrt{3} = 2 e^{i \\frac{\\pi}{3}} .\\]\n\nUdowodnij samodzielnie, Å¼e powyÅ¼sze rÃ³wnanie jest prawdziwe.\n\nLiczby zespolone moÅ¼na dodawa, mnoÅ¼yc i dzieli zgodnie z zwykÅ‚ymi reguÅ‚ami arytmetyki. Dodawanie liczb zespolonych jest Å‚atwe dla liczb w postaci kartezjaÅ„skiej. Natomiast mnoÅ¼enie liczb zespolonych upraszcza siÄ™ dla postaci biegunowej (nastÄ™puje zamiana mnoÅ¼enia na dodawanie fazy).\nLiczba sprzÄ™Å¼ona do liczby zespolonej powstaje poprzez zmianÄ™ znaku czÄ™Å›ci urojonej\n\\[ z=x + i\\, y\\,\\,\\,\\,\\] to \\[\\,\\,\\,z^*=x - i y = r  e^{-i \\phi} .\\]\nNorma liczby zespolonej \\[ z=x + i y\\,\\,\\,\\,\\] to \\[\\,\\,\\,|z|=\\sqrt{x^2 + y^2}=r .\\]\nKwadrat normy liczby zespolonej \\(z=x + i y\\,\\,\\,\\,\\) to \\(\\,\\,\\, |z|^2=x^2 + y^2=r^2\\). Warto zauwaÅ¼yc, Å¼e kaÅ¼dy kwadrat moduÅ‚u daje w wyniku nieujemnÄ… liczbÄ™ rzeczywistÄ….\nMoÅ¼na go rÃ³wnieÅ¼ zapisaÄ‡ jako \\[|z|^2=z z^* = z^* z\\]\nCzynniki fazowe to szczegÃ³lna klasa liczb zespolonych \\(z\\) dla ktÃ³rej \\(r=1\\).\nOtrzymujemy wtedy:\n\\[ z=e^{i \\phi}=\\cos{\\phi} + i\\, \\sin{\\phi} \\]\n\\[ z z^* = 1 \\]\n\nUdowodnij w kartezjaÅ„skim i polarnym ukÅ‚adzie oniesienia.\n\n\nile wynosi \\(z_1 z_2\\)\n\n\nile wynosi \\(\\frac{z_1}{z_2}\\)",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#wektory-i-przestrzenie-wektorowe",
    "href": "lectures/wyklad3.html#wektory-i-przestrzenie-wektorowe",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w",
    "section": "Wektory i przestrzenie wektorowe",
    "text": "Wektory i przestrzenie wektorowe\nNiech dany bÄ™dzie zbiÃ³r \\(\\mathbb{V}\\) oraz zbiÃ³r \\(\\mathbb{K}\\). Elementy zbioru \\(\\mathbb{V}\\) moÅ¼na ze sobÄ… dodawaÄ‡ i mnoÅ¼yÄ‡ przez elementy zbioru \\(\\mathbb{K}\\). Wraz z dodatkowymi opracjami (zdefiniowanymi poniÅ¼ej) zbiÃ³r ten bÄ™dziemy nazwywali przestrzeniÄ… wektorowÄ…. Jej elementy to wektory ket \\(\\ket{u}\\) (lub kety).\nJeÅ›li wspÃ³Å‚czynniki liczbowe wektorÃ³w bÄ™dÄ… rzeczywiste to bÄ™dziemy mÃ³wiÄ‡ o przestrzeni wektorowej rzeczywistej. Natomiast jeÅ›li liczby te bÄ™dÄ… zespolone to bÄ™dziemy mÃ³wiÄ‡ o przestrzeni wektorowej zespolonej.\nMyÅ›lÄ…c o wektorach czÄ™sto wyobraÅ¼amy je sobie jako strzaÅ‚ki w przestrzeni. Przez strzaÅ‚ki rozumiemy tutaj obiekty znajdujÄ…ce siÄ™ w zwykÅ‚ej przestrzeni i posiadajÄ…ce wielkoÅ› oraz kierunek. Wektory takie majÄ… trzy skÅ‚adowe - trzy (rzeczywiste) wspÃ³Å‚rzÄ™dne przestrzenne.\nNa tych zajÄ™ciach lepiej zapomniec o tej koncepcji. Wszystkie wektory bÄ™dÄ… reprezentowane jako abstrakcyjne elementy przestrzeni wektorowej. Warto jednak pamiÄ™taÄ‡, Å¼e wszystkie wÅ‚asnoÅ›ci (algebraiczne) wektorÃ³w sÄ… rÃ³wnieÅ¼ speÅ‚nione dla strzaÅ‚ek.\n\nAksjomaty przestrzeni stanÃ³w\nNiech \\(\\ket{v}\\) , \\(\\ket{u}\\), \\(\\ket{z}\\) bÄ™dÄ… dowolnymi wektorami, natomiast \\(\\alpha\\) i \\(\\beta\\) dowolnymi liczbami.\n\nSuma dwÃ³ch wektorÃ³w ket jest wektorem ket \\[\\ket{v} + \\ket{u} = \\ket{z}\\]\nDodawanie wektorÃ³w jest przemienne: \\[\\ket{v} + \\ket{u} = \\ket{u} + \\ket{v}\\]\nDodawanie wektorÃ³w jest Å‚Ä…czne: \\[\\ket{v} + (\\ket{u} + \\ket{z}) = (\\ket{v} + \\ket{u}) + \\ket{z}\\]\nIstnieje szczegÃ³lny (i jedyny) wektor \\(\\ket{v}\\) odwrotny do wektora \\(\\ket{u}\\): \\[\\ket{v} + \\ket{u} = 0\\]\nIstnieje szczegÃ³lny (i jedyny) wektor \\(0\\) zerowy. Dla kaÅ¼dego wektora \\(\\ket{v}\\) zachodzi: \\[\\ket{v} + 0 = 0 + \\ket{v} = \\ket{v}\\]\n1*wektor = wektor: \\[1 \\ket{v} = \\ket{v}\\]\nÅÄ…cznoÅ›Ä‡ mnoÅ¼enia przez skalar: \\[\\alpha (\\beta \\ket{v}) = (\\alpha \\beta) \\ket{v}\\]\nRozdzielnoÅ›Ä‡ mnoÅ¼enia przez skalar wzglÄ™dem dodawania wektorÃ³w: \\[\\alpha (\\ket{v} + \\ket{u}) = \\alpha \\ket{v} + \\alpha \\ket{u}\\]\nRozdzielnoÅ›Ä‡Â dodawania skalarÃ³w wzglÄ™dem mnoÅ¼enia przez wektor: \\[(\\alpha + \\beta) \\ket{v} = \\alpha \\ket{v} + \\beta \\ket{v}\\]\n\n\n\nWektory kolumnowe\nZapiszmy pionowÄ… jednokolumnowÄ… tablicÄ™ liczb: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix} \\]\nMnoÅ¼enie przez liczbÄ™: \\[ \\alpha \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix} = \\begin{bmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ .\\\\ \\alpha x_n \\end{bmatrix} \\]\nDodawanie kolumn: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ .\\\\ y_n \\end{bmatrix} = \\begin{bmatrix} x_1+y_1 \\\\ x_2+y_2 \\\\ .\\\\ x_n+y_n \\end{bmatrix}\\]\nPozwala to otrzymaÄ‡ konkretnÄ… reprezentacjÄ™ wektorÃ³w, ktÃ³re bÄ™dziemy oznaczaÄ‡ w notacji Diraca przez â€œketâ€ \\(\\ket{.}\\).\n\n\nWektory wierszowe\n\\[ \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}\\]\nAnalogicznie do poprzedniego przykÅ‚adu Å‚atwo okreÅ›liÄ‡ jak dodawaÄ‡ je ze sobÄ… i mnoÅ¼yÄ‡ przez liczbÄ™. W notacji Diraca bÄ™dziemy takie wektory oznaczali przez â€œbraâ€ \\(\\bra{.}\\).\n\n\nTranspozycja i sprzÄ™Å¼enie Hermitowskie.\nTranspozycja \\(T\\) Zamienia wektory wierszowe na kolumnowe i odwrotnie.\n\\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}^{T} = \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}\\]\noraz \\[ \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}^{T} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}\\]\nNatomiast sprzÄ™Å¼enie hermitowskie \\(\\dagger = T \\ast\\) dodatkowo do transpozycji dodaje sprzÄ™Å¼enie zespolone.\n\\[\\ket{u}^{\\dagger} = \\bra{u}\\] \\[\\bra{u}^{\\dagger} = \\ket{u}\\]\nCzyli: \\[ (\\ket{u} + \\ket{v})^{\\dagger} = \\bra{u} + \\bra{v} \\] oraz \\[ \\alpha \\ket{u} \\to \\bra{u} \\alpha^*\\]\n\\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}^{\\dagger} = \\begin{bmatrix} x_1^* \\,\\, x_2^* \\,\\, \\dots \\,\\, x_n^* \\end{bmatrix}\\]\noraz \\[ \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}^{\\dagger} = \\begin{bmatrix} x_1^* \\\\ x_2^* \\\\ .\\\\ x_n^* \\end{bmatrix}\\]\n\n\nIloczyn skalarny\nIloczynem skalarnym dwÃ³ch wektorÃ³w \\(\\ket{u}\\) i \\(\\ket{v}\\) nazywany funkcjÄ™, ktÃ³ra zwraca liczbÄ™.\n\n\\(\\braket{u}{v} = \\braket{v}{u}^{\\ast}\\)\n\\((\\alpha \\bra{u})\\ket{v} = \\alpha \\braket{u}{v}\\)\n\\((\\bra{u} + \\bra{v}) \\ket{z} = \\braket{u}{z} +\\braket{v}{z}\\)\n\\(\\braket{u}{u} &gt; 0\\)\n\\(\\braket{u}{u} = 0, gdy \\ket{u}=\\ket{0}\\)\n\nDla dwÃ³ch wektorÃ³w \\(\\ket{u}\\) i \\(\\ket{v}\\) otrzymujemy: \\[ \\ket{u} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}, \\ket{v} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ .\\\\ y_n \\end{bmatrix} \\]\n\\[ \\braket{u}{v} = x_1^{*}y_1 +x_2^{*}y_2 + \\dots + x_n^{*}y_n\\]\n\nZadanie - Udowodnij, Å¼e \\(\\braket{u}{u}\\) jest liczbÄ… rzeczywistÄ….\n\nwektor znormalizowany \\(\\braket{u}{u}=1\\)\nwektory ortogonalne \\(\\braket{u}{v}=0\\)\n\n\nKombinacja liniowa wektorÃ³w\nDla dwÃ³ch wektorÃ³w \\(\\ket{u}\\) i \\(\\ket{v}\\) oraz dwÃ³ch liczb \\(\\alpha\\), \\(\\beta\\) moÅ¼emy stworzyÄ‡ nowy wektor: \\[\\ket{z} = \\alpha \\ket{u} + \\beta \\ket{v}\\] Wektor ten nazywamy kombinacjÄ… liniowÄ… wektorÃ³w \\(\\ket{u}\\) i \\(\\ket{v}\\) o wspÃ³Å‚czynnikach \\(\\alpha\\) i \\(\\beta\\).\n\n\nBaza\nKaÅ¼da przestrzeÅ„Â wektorowa ma bazÄ™.\nDowolny wektor moÅ¼na zapisa jako kombinacjÄ™ liniowÄ… wektorÃ³w bazowych.\nInteresowac bÄ™dzie nas baza (obliczeniowa) dla ktÃ³rej:\n\\[ \\braket{e_i}{e_i}=1 \\,\\, \\braket{e_i}{e_j}=0 \\,\\, \\text{dla i} \\neq j \\] gdzie \\(i,\\,j = 1,2,\\dots, n\\).\nDowolny wektor \\(\\ket{u}\\) moÅ¼emy zapisa jako: \\[ \\ket{u} = \\braket{e_1}{u}\\ket{e_1} + \\braket{e_2}{u}\\ket{e_2} + ... + \\braket{e_n}{u}\\ket{e_n}  \\]\nWarto zauwaÅ¼yc: \\[\\braket{e_1}{u}= x_1\\] \\[\\ket{u} = \\sum_{i=1}^{n} \\ket{i}\\bra{i} \\ket{u}\\]",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#formalizm-matematyczny-obliczeÅ„-kwantowych",
    "href": "lectures/wyklad3.html#formalizm-matematyczny-obliczeÅ„-kwantowych",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w",
    "section": "Formalizm matematyczny obliczeÅ„ kwantowych",
    "text": "Formalizm matematyczny obliczeÅ„ kwantowych\nTa wiedza wystarczy do wyjaÅ›nienia notacji Diraca.\nIloczyn skalarny \\(\\braket{\\psi}{\\phi}\\) wektorÃ³w \\(\\ket{\\psi}\\) i \\(\\ket{\\phi}\\) czytamy jako braket u v.\n\nStan\nW fizyce klasycznej znajomoÅ›Ä‡ stanu ukÅ‚adu oznacza, iÅ¼ wiemy wszystko co jest potrzebne\nStanem w mechanice kwantowej nazywamy wektor:\n\\[\\ket{\\psi} = x_0 \\ket{0} + x_1 \\ket{1} + \\dots x_{n-1} \\ket{n-1}\\]\nChcemy aby wspÃ³Å‚czynniki \\(x_i\\) byÅ‚y liczbami zespolonymi a caÅ‚y wektor byÅ‚ unormowany do 1.\nLiczby \\(x_i\\) nazywamy amplitudami prawdopodobieÅ„stwa stanu kwantowego. JeÅ›li przynajmniej dwie liczby \\(x_i\\) sÄ… niezerowe, to ukÅ‚ad znajduje siÄ™ w superpozycji stanÃ³w.\n\n\nKubit\n\nElementarnym obiektem w informatyce kwantowej jest kubit, ktÃ³ry realizowany jest jako dwu wymiarowy ukÅ‚ad kwantowy. Stan kwantowy kubitu opisuje wektor w przestrzeni liniowej \\(\\mathbb{C}^2\\).\nW celu wykonywania obliczeÅ„ i opisu stanu kubitu wybierzemy tzw. bazÄ™ obliczeniowÄ…: \\[\\ket{0} = \\begin{bmatrix} 1 \\\\ 0  \\end{bmatrix} , \\ket{1} = \\begin{bmatrix} 0 \\\\ 1  \\end{bmatrix}\\]\nTo co wyrÃ³Å¼nia kubit w porÃ³wnaniu do klasycznego bitu dowolny stan \\(\\ket{\\psi}\\) moÅ¼e byÄ‡ superpozycjÄ… stanÃ³w bazowych: \\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1} = \\alpha \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} + \\beta \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\n\\] \\[\n\\bra{\\psi} = \\alpha^{*} \\bra{0} + \\beta^{*} \\bra{1} = \\alpha^{*} [1 \\,\\, 0] + \\beta^{*} [0 \\,\\, 1] =\n[\\alpha^{*}  \\, \\, \\beta^{*} ]\n\\]\ndla ktÃ³rego zachodzi warunek normalizacji: \\[\n\\braket{\\psi}{\\psi} = |\\alpha|^2 + |\\beta|^2 = 1\n\\] gdzie \\(\\alpha, \\beta \\in \\mathbb{C}\\).\n\nZADANIE - oblicz \\(\\braket{\\psi}{\\psi}\\).\n\nLiczby \\(\\alpha\\) i \\(\\beta\\) nazywamy amplitudami prawdopodobieÅ„stwa. SÄ… one reprezentowane przez liczby zespolone. Potrzeba 4 liczb rzeczywistych aby je opisaÄ‡. Ze wzglÄ™du na warunek normalizacji jednÄ… liczbÄ™ moÅ¼na obliczyc co oznacza potrzebÄ™ uÅ¼ycia juÅ¼ tylko trzech liczb rzeczywiste.\nStan kubitu moÅ¼emy zapisaÄ‡ w postaci: \\[\n\\ket{\\psi} = e^{i \\gamma}\\left( \\cos{\\frac{\\phi}{2}} \\ket{0} + e^{i \\theta} \\sin{\\frac{\\phi}{2}} \\ket{1} \\right)\n\\] gdzie \\(\\phi \\in [0, \\pi]\\), \\(\\theta \\in [0, 2\\pi]\\) i \\(\\gamma \\in [0, 2\\pi]\\) sÄ… liczbami rzeczywistymi.\nWspÃ³Å‚czynnik \\(e^{i \\gamma}\\) nazywamy fazÄ… globalnÄ…. Ze wzglÄ™du, iÅ¼ analizowaÄ‡ bÄ™dziemy kwadraty amplitud prawdopodobieÅ„stwa to faza globalna nie ma znaczenia. Dlatego moÅ¼emy napisaÄ‡: \\[\n\\ket{\\psi} = \\cos{\\frac{\\phi}{2}} \\ket{0} + e^{i \\theta} \\sin{\\frac{\\phi}{2}} \\ket{1}\n= \\begin{bmatrix} \\cos{\\frac{\\phi}{2}} \\\\ e^{i \\theta} \\sin{\\frac{\\phi}{2}} \\end{bmatrix}\n\\]\nWarto zauwaÅ¼yÄ‡, Å¼e dwa dowolne stany kubitÃ³w \\(\\ket{\\psi}\\) i \\(\\ket{\\phi}\\) rÃ³Å¼niÄ… siÄ™ o czynnik fazowy \\(e^{i \\gamma}\\) to stany te dajÄ… identyczne wyniki.\nLiczby rzeczywiste \\(\\phi\\) i \\(\\theta\\) nazywamy kÄ…tami kubitu i moÅ¼emy interpretowaÄ‡ je jako wspÃ³Å‚rzÄ™dne na sferze Blocha. Bardzo czÄ™sto bÄ™dziemy wykorzystywaÄ‡ jÄ… do wizualizacji stanÃ³w kubitÃ³w.\nStany w bazie obliczeniowej, ktÃ³rymi czÄ™sto bÄ™dziemy operowac: \\[\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\] \\[\\ket{-} = \\frac{1}{\\sqrt{2}}(\\ket{0} - \\ket{1})\\] \\[\\ket{i} =\\frac{1}{\\sqrt{2}}(\\ket{0} + i \\ket{1})\\] \\[\\ket{-i} =\\frac{1}{\\sqrt{2}}(\\ket{0} - i \\ket{1})\\]\nLub: \\[\\frac{1}{\\sqrt{2}}(\\ket{0} + e^{i\\pi/6} \\ket{1})\\] \\[\\frac{\\sqrt{3}}{2}(\\ket{0} + \\frac{1}{2} \\ket{1})\\]\n\nKubit moÅ¼e by dowolnym punktem na sferze Blocha.\n\n\n\nDwa kubity\nZÅ‚Ä…czenie ukÅ‚adu dwÃ³ch kubitÃ³w realizowane jest przez iloczyn tensorowy (iloczyn Kroneckera).\nRozwaÅ¼my dwa stany kubitÃ³w \\(\\ket{\\psi}\\), \\(\\ket{\\phi}\\)\n\\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1} = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\\, ,\\,\\,\n\\ket{\\phi} = \\gamma \\ket{0} + \\delta \\ket{1} = \\begin{bmatrix} \\gamma \\\\ \\delta \\end{bmatrix}\n\\]\nStan dwukubitowy: \\[\n\\ket{\\psi} \\otimes \\ket{\\phi} = \\begin{bmatrix} \\alpha \\gamma \\\\ \\alpha \\delta \\\\ \\beta \\gamma \\\\ \\beta \\delta \\end{bmatrix} = \\alpha \\gamma \\ket{0} \\otimes \\ket{0} + \\beta \\delta \\ket{1} \\otimes \\ket{0}  + \\alpha \\delta \\ket{0} \\otimes \\ket{1}  + \\beta \\delta \\ket{1} \\otimes \\ket{1}\n\\] co moÅ¼emy zapisa jako: \\[\n\\ket{\\psi \\phi} = \\alpha \\gamma \\ket{00} + \\beta \\delta \\ket{10}  + \\alpha \\delta \\ket{01}  + \\beta \\delta \\ket{11}\n\\] gdzie: \\[\n\\ket{00} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\, \\,\n\\ket{01} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\, \\,\n\\ket{10} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\, \\,\n\\ket{11} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\]\nPo przenumerowaniu stanÃ³w moÅ¼emy napisac: \\[\n\\ket{\\Phi} = c_0 \\ket{0} + c_1 \\ket{1}  + c_2 \\ket{2}  + c_3 \\ket{3}\n\\] dla ktÃ³rego: \\[\n|c_0|^2 + |c_1|^2 + |c_2|^2 + |c_3|^2 = 1\n\\]\n\n\nStan separowalny i splÄ…tany\nJeÅ¼eli istniejÄ… stany \\(\\ket{\\phi_1}\\) i \\(\\ket{\\phi_2}\\) takie, Å¼e \\[\\ket{\\psi} = \\ket{\\phi_1} \\otimes \\ket{\\phi_2}\\] to stan nazywamy separowalny.\nZobaczmy, czy istnieje przypadek w ktÃ³rym stan ukÅ‚adu dwÃ³ch kubitÃ³w nie da siÄ™ zaprezentowac jako iloczynu tensorowego podukÅ‚adÃ³w. Aby to sprawdzic zobaczmy czy istniejÄ… takie liczby \\(c_0, c_1, c_2, c_3\\) dla ktÃ³rych nie da siÄ™ znaleÅºc \\(\\alpha, \\beta,\\gamma, \\delta\\), ktÃ³re speÅ‚niajÄ… ukÅ‚ad rÃ³wnaÅ„: \\[c_0 = \\alpha \\gamma , \\, c_1 = \\alpha \\delta , \\, c_2 = \\beta \\gamma , \\, c_3 = \\beta \\delta \\]\nRozwaÅ¼my stan \\[\\ket{bell} = \\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{3}) = \\frac{1}{\\sqrt{2}}(\\ket{00}+\\ket{11})\\]\nZaÅ‚Ã³Å¼my, Å¼e moÅ¼emy zapisa stan bell w postaci: \\[ \\alpha \\gamma \\ket{0} + \\beta \\delta \\ket{1}  + \\alpha \\delta \\ket{2}  + \\beta \\delta \\ket{3} \\]\nAby stan bell byÅ‚ separowalny musi by speÅ‚niony ukÅ‚ad rÃ³wnaÅ„:\n\\[\\begin{eqnarray}\n\\alpha \\gamma = \\frac{1}{\\sqrt{2}} \\\\ \\alpha \\delta = 0 \\\\ \\beta \\gamma = 0 \\\\ \\beta \\delta =\\frac{1}{\\sqrt{2}}\n\\end{eqnarray}\\]\nZ warunku drugiego mamy dwie moÅ¼liwoÅ›ci: albo \\(\\alpha=0\\) lub \\(\\delta=0\\). JeÅ¼eli \\(\\alpha=0\\) to warunek pierwszy nie moÅ¼e byc speÅ‚niony. JeÅ¼eli \\(\\delta=0\\) to warunek czwarty nie moÅ¼e byc speÅ‚niony. Otrzymujemy sprzecznoÅ›c.\nProwadzi to do wniosu, Å¼e stan bell'a nie jest stanem separowalnym i jest stanem splÄ…tanym. Stany te majÄ… bardzo nieintuicyjne wÅ‚asnoÅ›ci. ZwiÄ…zany jest z nimi sÅ‚ynny paradox EPR oraz tak zwane nierÃ³wnoÅ›ci Bella.\n\nSplÄ…tane stany Bellâ€™a, wraz z zasadÄ… superpozycji bÄ™dÄ… podstawowymi kwantowymi wÅ‚asnoÅ›ciami pozwalajÄ…cymi zrealizowac przewagÄ™ obliczeÅ„ kwantowych nad obliczeniami klasycznymi.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w"
    ]
  },
  {
    "objectID": "lectures/wyklad3.html#pomiar-w-bazie-z",
    "href": "lectures/wyklad3.html#pomiar-w-bazie-z",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w",
    "section": "Pomiar w bazie Z",
    "text": "Pomiar w bazie Z\nW opisie kubitÃ³w wybraliÅ›my specyficzÄ… bazÄ™ (obliczeniowÄ…) wektorÃ³w, ktÃ³ra rozkÅ‚ada kaÅ¼dy wektor na kombinacjÄ™ wektora \\(\\ket{0}\\) i \\(\\ket{1}\\).\nZasady przestrzeni wektorowej i mechaniki kwantowej dopuszczajÄ… tworzenie kombinacji liniowej (superpozycji) dla tych dwÃ³ch stanÃ³w. \\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1}\n\\] Po pomiarze kubitu, czyli na koÅ„cu procesu obliczeniowego, ze wzglÄ™du na prawa fizyki otrzymujemy tylko i wyÅ‚Ä…cznie jeden ze stanÃ³w bazowych \\(\\ket{0}\\) lub \\(\\ket{1}\\). KaÅ¼dy nastÄ™pny pomiar (tej samej obserwabli) bÄ™dzie koÅ„czyc siÄ™ w tym samym (otrzymanym) stanie.\n\nPomiar niszczy superpozycjÄ™ kubitu i sprowadza go do jednego ze stanÃ³w bazowych.\n\nDla kubitu w superpozycji stanÃ³w bazowych jedyne co moÅ¼emy okreÅ›lic to prawdopodobieÅ„stwo otrzymania stanu \\(\\ket{0}\\) i \\(\\ket{1}\\).\n\nPrawdopodobieÅ„stwo okreÅ›lone jest jako kwadrat (moduÅ‚u) amplitudy Dla stanu \\(\\ket{0}\\) \\(P(0) = |\\alpha|^2\\) oraz dla stanu \\(\\ket{1}\\) \\(P(1)= |\\beta|^2\\).\n\nIstnieje moÅ¼liwoÅ›c pomiaru kubitÃ³w w innych bazach. Jednak w wiÄ™kszoÅ›ci przypadkÃ³w ograniczymy siÄ™ do pomiaru w bazie obliczeniowej.\n\n\n\nPrzykÅ‚ad\nRozwaÅ¼my stan \\[\\ket{\\psi} = \\frac{\\sqrt{3}}{2}\\ket{0}+\\frac{1}{2}\\ket{1}\\]\nMoÅ¼liwe wyniki pomiaru w bazie Z \\(\\{ \\ket{0},\\ket{1} \\}\\).\n\\[\n\\braket{0}{\\psi} = \\bra{0}\\left( \\frac{\\sqrt{3}}{2}\\ket{0} +\\frac{1}{2}\\ket{1}\\right) = \\frac{\\sqrt{3}}{2}\\braket{0}{0} + \\frac{1}{2}\\braket{0}{1} = \\frac{\\sqrt{3}}{2}\n\\] BiorÄ…c kwadrat apmlitudy otrzymujemy kubit w stanie \\(\\ket{0}\\) z prawdopodobieÅ„stwem \\(0.75\\). \\[\n\\braket{1}{\\psi} = \\bra{1}\\left( \\frac{\\sqrt{3}}{2}\\ket{0} +\\frac{1}{2}\\ket{1}\\right) = \\frac{\\sqrt{3}}{2}\\braket{1}{0} + \\frac{1}{2}\\braket{1}{1} = \\frac{1}{2}\n\\] BiorÄ…c kwadrat apmlitudy otrzymujemy stan \\(\\ket{1}\\) z prawdopodobieÅ„stwem \\(0.25\\).\n\\[\\ket{\\psi} = \\braket{0}{\\psi}\\ket{0} + \\braket{1}{\\psi}\\ket{1}\\]\nDowolna para liniowo niezaleÅ¼nych wektorÃ³w jednostkowych \\(\\ket{u}\\) i \\(\\ket{v}\\) pochodzÄ…ca z dwuwymiarowej przestrzeni wektorowej moÅ¼e tworzyc bazÄ™: \\[\n\\alpha \\ket{0} +\\beta \\ket{1} = \\alpha' \\ket{u} +\\beta' \\ket{v}\n\\] PrzykÅ‚adem moÅ¼e byc tzw Baza Hadamarda \\(\\ket{+}\\) i \\(\\ket{-}\\) zdefiniowana jako: \\[\n\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{1}) = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n\\] \\[\n\\ket{-} = \\frac{1}{\\sqrt{2}}(\\ket{0}-\\ket{1}) = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ - \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n\\]\n\nBardzo waÅ¼nym etapem jest wybÃ³r bazy w ktÃ³rej dokonujemy pomiaru. np. dla wektora \\(\\ket{+}\\) pomiar w bazie standardowej pozwoli otrzymac wyniki stanu \\(\\ket{0}\\) i \\(\\ket{1}\\) z prawdopodobieÅ„stwami \\(\\frac{1}{2}\\). Natomiast jeÅ›li pomiar dokonywany byÅ‚by w bazie Hadamarda to zawsze otrzymamy stan \\(\\ket{+}\\) z prawdopodobieÅ„stwem 1.",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitÃ³w"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html",
    "href": "lectures/wyklad4.html",
    "title": "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych",
    "section": "",
    "text": "ZmianÄ™ stanu kwantowego w czasie opisuje Ewolucja kwantowa.\nRozwaÅ¼my stan ukÅ‚adu w chwili \\(t=0\\).\n\\[\\ket{\\psi_{t=0}}\\] W chwili \\(t=1\\) otrzymujemy stan \\(\\ket{\\psi_{t=1}}\\) t. Å¼e: \\[\\ket{\\psi_{t=1}} = \\textbf{U} \\, \\ket{\\psi_{t=0}} \\] gdzie \\(\\textbf{U}\\) jest macierzÄ… unitarnÄ….\nPowyÅ¼sze rÃ³wnanie opisuje zachowanie wszystkich ukÅ‚adÃ³w kwantowych.\nRozwaÅ¼my stany bazowe \\(\\ket{0}\\), \\(\\ket{1}\\), ktÃ³re bÄ™dziemy chcieli zamienic w ich superpozycjÄ™. \\[\n\\textbf{U}\\ket{0} = a\\ket{0} + b\\ket{1} = \\begin{bmatrix} a \\\\ b \\end{bmatrix}\n\\] \\[\n\\textbf{U}\\ket{1} = c\\ket{0} + d\\ket{1} = \\begin{bmatrix} c \\\\ d \\end{bmatrix}\n\\]\nKorzystajÄ…c z tych rÃ³wnaÅ„ moÅ¼emy napisac: \\[\n\\textbf{U} = \\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\begin{bmatrix} c \\\\ d \\end{bmatrix}\\right) = \\begin{bmatrix} a \\, \\, b \\\\ c \\,\\, d \\end{bmatrix}\n\\]\nW informatyce macierze unitarne bÄ™dÄ… realizowaÅ‚y logiczne bramki kwantowe.\n\nDlaczego bramki kwantowe muszÄ… by unitarne?\n\nNorma stanu kwantowego wynosi zawsze 1. Jest to prawdopodobieÅ„stwo caÅ‚kowite sumy stanÃ³w bazowych. PrawdopodobieÅ„stwo to powinno by zachowane. Co oznacza, Å¼e chcemy znaleÅºc takÄ… transformacjÄ™, ktÃ³ra nie zmienia dÅ‚ugoÅ›ci (kwadratu) wektora. Taka transformacja realizowana jest przez obroty.\nWarto zwrÃ³cic uwagÄ™ na jeszcze jeden fakt. Macierz odwrotna do \\(\\textbf{U}\\) (oznaczana jako \\(\\textbf{U}^{-1}\\)) zawsze istnieje i jest ona rÃ³wna sprzÄ™Å¼eniu Hermitowskiemu macierzu \\(\\textbf{U}=\\textbf{U}^{\\dagger}\\). Dlatego ewolucja stanÃ³w kwantowych zawsze jest odwracalna. A to oznacza, Å¼e i bramki muszÄ… by operacjami odwracalnymi. \\[\\ket{\\psi_{t=0}} = \\textbf{U}^{\\dagger} \\ket{\\psi_{t=1}} \\]",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#ewolucja-kwantowa",
    "href": "lectures/wyklad4.html#ewolucja-kwantowa",
    "title": "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych",
    "section": "",
    "text": "ZmianÄ™ stanu kwantowego w czasie opisuje Ewolucja kwantowa.\nRozwaÅ¼my stan ukÅ‚adu w chwili \\(t=0\\).\n\\[\\ket{\\psi_{t=0}}\\] W chwili \\(t=1\\) otrzymujemy stan \\(\\ket{\\psi_{t=1}}\\) t. Å¼e: \\[\\ket{\\psi_{t=1}} = \\textbf{U} \\, \\ket{\\psi_{t=0}} \\] gdzie \\(\\textbf{U}\\) jest macierzÄ… unitarnÄ….\nPowyÅ¼sze rÃ³wnanie opisuje zachowanie wszystkich ukÅ‚adÃ³w kwantowych.\nRozwaÅ¼my stany bazowe \\(\\ket{0}\\), \\(\\ket{1}\\), ktÃ³re bÄ™dziemy chcieli zamienic w ich superpozycjÄ™. \\[\n\\textbf{U}\\ket{0} = a\\ket{0} + b\\ket{1} = \\begin{bmatrix} a \\\\ b \\end{bmatrix}\n\\] \\[\n\\textbf{U}\\ket{1} = c\\ket{0} + d\\ket{1} = \\begin{bmatrix} c \\\\ d \\end{bmatrix}\n\\]\nKorzystajÄ…c z tych rÃ³wnaÅ„ moÅ¼emy napisac: \\[\n\\textbf{U} = \\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\begin{bmatrix} c \\\\ d \\end{bmatrix}\\right) = \\begin{bmatrix} a \\, \\, b \\\\ c \\,\\, d \\end{bmatrix}\n\\]\nW informatyce macierze unitarne bÄ™dÄ… realizowaÅ‚y logiczne bramki kwantowe.\n\nDlaczego bramki kwantowe muszÄ… by unitarne?\n\nNorma stanu kwantowego wynosi zawsze 1. Jest to prawdopodobieÅ„stwo caÅ‚kowite sumy stanÃ³w bazowych. PrawdopodobieÅ„stwo to powinno by zachowane. Co oznacza, Å¼e chcemy znaleÅºc takÄ… transformacjÄ™, ktÃ³ra nie zmienia dÅ‚ugoÅ›ci (kwadratu) wektora. Taka transformacja realizowana jest przez obroty.\nWarto zwrÃ³cic uwagÄ™ na jeszcze jeden fakt. Macierz odwrotna do \\(\\textbf{U}\\) (oznaczana jako \\(\\textbf{U}^{-1}\\)) zawsze istnieje i jest ona rÃ³wna sprzÄ™Å¼eniu Hermitowskiemu macierzu \\(\\textbf{U}=\\textbf{U}^{\\dagger}\\). Dlatego ewolucja stanÃ³w kwantowych zawsze jest odwracalna. A to oznacza, Å¼e i bramki muszÄ… by operacjami odwracalnymi. \\[\\ket{\\psi_{t=0}} = \\textbf{U}^{\\dagger} \\ket{\\psi_{t=1}} \\]",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#bramki-jednokubitowe",
    "href": "lectures/wyklad4.html#bramki-jednokubitowe",
    "title": "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych",
    "section": "Bramki jednokubitowe",
    "text": "Bramki jednokubitowe\nSpoÅ›rÃ³d wszystkich bramek kwantowych istnieje kilka, ktÃ³re majÄ… swoje ustalone nazwy. SÄ… one czÄ™sto wykorzystywane w obliczeniach kwatnowych. RozwaÅ¼my stan \\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1}\n\\]\n\nBramka identycznoÅ›ciowa\n\\[\n\\textbf{I} = \\begin{bmatrix} 1 \\,\\, 0 \\\\ 0 \\,\\, 1 \\end{bmatrix}\n\\]\nZobaczmy jak operator ten dziaÅ‚a na stany bazowe: \\[ \\textbf{I} \\ket{0} = \\begin{bmatrix} 1 \\,\\, 0 \\\\ 0 \\,\\, 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\]\n\\[ \\textbf{I} \\ket{1} = \\begin{bmatrix} 1 \\,\\, 0 \\\\ 0 \\,\\, 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\]\nDziaÅ‚ajÄ…c na stan \\(\\ket{\\psi}\\) otrzymujemy: \\[\n\\textbf{I} \\ket{\\psi} = \\begin{bmatrix} 1 \\,\\, 0 \\\\ 0 \\,\\, 1 \\end{bmatrix} \\ket{\\psi} =  \\textbf{I} \\left( \\alpha \\ket{0} + \\beta \\ket{1} \\right) = \\alpha \\ket{0} + \\beta \\ket{1}\n\\]\n\n\nBramka negacji X (NOT)\n\\[\n\\textbf{X} = \\begin{bmatrix} 0 \\,\\, 1 \\\\ 1 \\,\\, 0 \\end{bmatrix}\n\\]\n\\[\n\\textbf{X} \\ket{0} = \\begin{bmatrix} 0 \\,\\, 1 \\\\ 1\\,\\, 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\ket{1}\n\\]\n\\[\n\\textbf{X} \\ket{1} = \\begin{bmatrix} 0 \\,\\, 1 \\\\ 1 \\,\\, 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\ket{0}\n\\]\nDziaÅ‚ajÄ…c na stan \\(\\ket{\\psi}\\) otrzymujemy: \\[\n\\textbf{X} \\ket{\\psi} = \\begin{bmatrix} 0 \\,\\, 1 \\\\ 1 \\,\\, 0 \\end{bmatrix} \\ket{\\psi} =  \\textbf{X} \\left( \\alpha \\ket{0} + \\beta \\ket{1} \\right) = \\alpha \\ket{1} + \\beta \\ket{0}\n\\]\n\n\nBramka negacji fazy Y\n\\[\n\\textbf{Y} = \\begin{bmatrix} 0 \\,\\, -i \\\\ i \\,\\,\\,\\,\\,\\,\\, 0 \\end{bmatrix}\n\\]\n\\[\n\\textbf{Y} \\ket{0} = \\begin{bmatrix} 0 \\,\\, -i \\\\ i \\,\\,\\,\\,\\,\\,\\, 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = i \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = i \\ket{1}\n\\]\n\\[\n\\textbf{Y} \\ket{1} = \\begin{bmatrix} 0 \\, -i \\\\ i \\,\\,\\,\\,\\,\\, 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = -i \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = -i \\ket{0}\n\\]\nDziaÅ‚ajÄ…c na stan \\(\\ket{\\psi}\\) otrzymujemy: \\[\n\\textbf{Y} \\ket{\\psi} = \\begin{bmatrix} 0 \\,\\, -i \\\\ i \\,\\,\\,\\,\\,\\, 0 \\end{bmatrix} \\ket{\\psi} =  \\textbf{Y} \\left( \\alpha \\ket{0} + \\beta \\ket{1} \\right) = \\alpha i \\ket{1} - \\beta i \\ket{0}\n\\]\n\n\nBramka negacji fazy i bitu Z\n\\[\n\\textbf{Z} = \\begin{bmatrix} 1 \\,\\,\\,\\,\\,\\,\\,\\,\\, 0 \\\\ 0\\,\\, -1 \\end{bmatrix}\n\\]\n\\[ \\textbf{Z} \\ket{0} = \\begin{bmatrix} 1\\,\\,\\,\\,\\,\\,\\, 0 \\\\ 0 \\, -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 1 \\ket{0} \\]\n\\[ \\textbf{Z} \\ket{1} = \\begin{bmatrix} 1 \\,\\,\\,\\,\\,\\,\\, 0 \\\\ 0 \\, -1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = -1 \\ket{1}\\]\nDziaÅ‚ajÄ…c na stan \\(\\ket{\\psi}\\) otrzymujemy: \\[\n\\textbf{Z} \\ket{\\psi} = \\begin{bmatrix} 0 \\,\\,\\,\\,\\,\\,\\, 1 \\\\ 0 \\, -1 \\end{bmatrix} \\ket{\\psi} =  \\textbf{Z} \\left( \\alpha \\ket{0} + \\beta \\ket{1} \\right) = \\alpha \\ket{0} - \\beta \\ket{1}\n\\]\n\n\nBramka Hadamarda H\n\\[\n\\textbf{H}= \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1\\,\\,\\,\\,\\,\\,\\, 1 \\\\ 1 \\, -1 \\end{bmatrix}\n\\]\n\\[\n\\textbf{H} \\ket{0} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1\\,\\,\\,\\,\\,\\,\\, 1 \\\\ 1 \\, -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\left( \\ket{0} + \\ket{1} \\right) = \\ket{+}\n\\]\n\\[\n\\textbf{H} \\ket{1} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1\\,\\,\\,\\,\\,\\,\\, 1 \\\\ 1 \\, -1 \\end{bmatrix}  \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\left( \\ket{0} - \\ket{1} \\right) = \\ket{-}\n\\]\n\n\n\nLosowy bit\nStwÃ³rzmy pierwszy kwantowy program, ktÃ³ry wykona zadanie niemoÅ¼liwe do zrealizowania na komputerze klasycznym. Jak moÅ¼na zauwaÅ¼yc zdefiniowaliÅ›my bramkÄ™ Hadamarda. Brami tej nie byÅ‚o w klasycznych bramkach realizujÄ…cych operacje na bitach.\nNa przestrzeni dziejÃ³w informatyki bardzo duÅ¼o czasu i wysiÅ‚ku poÅ›wiÄ™cono opracowaniu systemu generowania liczb pseudolosowych (ang. PRNG - Pseudo Random Number Generator), ktÃ³ry znalazÅ‚ szerokie zastosowanie. Generowane liczby traktujemy jako pseudolosowe - tzn. jeÅ›li znasz zawartoÅ›c pamiÄ™ci komputera i algorytm PRNG moÅ¼esz (przynajmniej teoretycznie) przewidzie jaka jest nastÄ™pna wartosc wygenerowanej liczby.\nZgodnie z zasadami fizyki zachwanie kubitu bÄ™dÄ…cego w superpozycji w czasie dokonania pomiaru jest idealne i nieprzewidywalne. DziÄ™ki temu juÅ¼ pojedynczy kubit pozwala wygenerowa najlepszy na Å›wiecie generator liczb losowych.\ninstrukcja\n\nPrzygotuj kubit w stanie poczÄ…tkowym \\(\\ket{0}\\).\nZastosuj bramkÄ™ Hadamarda tworzÄ…c z kubitu stan superpozycji stanÃ³w bazowych.\nWykonaj pomiar\n\nWÅ‚aÅ›nie otrzymaÅ‚eÅ› QRNG - Quantum Random Number Generator. Nie jest to tani sposÃ³b na losow rzut monetÄ…. Jednak trzeba miec swiadomoÅ›c, Å¼e tutaj nie ma wewnÄ™trznego mechanizmu, ktÃ³ry generuje losowoÅ›c - wynika ona tylko i wyÅ‚Ä…cznie z praw mechaniki kwantowej.\n\nCzy potrafisz wygenerowac losowy bajt?\n\n\n\nGra w obracanie monety\nWykorzystujÄ…c powyÅ¼ej zdefiniowane bramki moÅ¼emy zrealizowa nastÄ™pujÄ…cÄ… grÄ™:\n\nW grze bierze udziaÅ‚ dwÃ³ch graczy. Gracze dysponujÄ… monetÄ…, ktÃ³rej nie widzÄ… w trakcie gry (np. jest zamkniÄ™ta w pudeÅ‚ku). Natomiast wiedzÄ…, Å¼e poczÄ…tkowo moneta uÅ‚oÅ¼ona jest orÅ‚em do gÃ³ry (w stanie \\(\\ket{0}\\)) Gra polega na wykonaniu trzech ruchÃ³w na przemian. KaÅ¼dy ruch polega na odwrÃ³ceniu monety bÄ…dÅº pozostawieniu jej w takim stanie w jakim byÅ‚a. Gracze nie wiedzÄ… jaki ruch wykonuje przeciwnik. Po ostatnim ruchu pudeÅ‚ko zostaje otwarte i gracze sprawdzajÄ… w jakiej pozycji jest moneta. Pierwszy gracz wygrywa jeÅ›li moneta jest w pozycji orÅ‚a, a drugi jeÅ›li przeciwnie.\n\nSzansa wygranej wynosi dla kaÅ¼dego \\(50\\%\\) i jak moÅ¼na sprawdzic nie istnieje strategia wygrywajÄ…ca.\nPytanie zasadnicze - a co jeÅ›li zamienimy monetÄ™ na kubit?\nMoÅ¼liwe operacje pozostawienia kubitu w takim samym stanie - bramka I, zmiany stanu na przeciwny bramka X. Czyli pierwszy gracz ustala pierwszÄ… bramkÄ™, drugi drugÄ… i ponownie pierwszy trzeciÄ…. Otwarcie pudeÅ‚ka to pomiar stanu kubitu.\n\nPrzeanalizuj wynik dla sekwencji I X I\n\nA co jeÅ›li pierwszy gracz wie, Å¼e dziaÅ‚a na kubicie?\n\nCzy moÅ¼e sprawic on, Å¼e wygra zawsze? (skoro wie, Å¼e dziaÅ‚a na kubicie moÅ¼e uÅ¼yc innych bramek)",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#bramki-dwukubitowe",
    "href": "lectures/wyklad4.html#bramki-dwukubitowe",
    "title": "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych",
    "section": "Bramki dwukubitowe",
    "text": "Bramki dwukubitowe\nAnalogicznie do bramek jednokubitowych reprezentowanych przez macierze unitarne \\(2\\times 2\\) moÅ¼emy skonstruowac dowolnÄ… wielo-kubitowÄ… bramkÄ™. Dla n kubitÃ³w mamy \\(2^n \\times 2^n\\) unitarnÄ… macierz reprezentujÄ…cÄ… takÄ… bramkÄ™. PoniewaÅ¼ bramki wielo kubiotwe dziaÅ‚ajÄ… na raz na kilka kubitÃ³w mogÄ… sÅ‚uÅ¼yc one do otrzymywania stanÃ³w splÄ…tanych. Mamy rÃ³wnieÅ¼ moÅ¼liwoÅ›c stworzyc bramkÄ™ warunkowÄ… (kontrolowanÄ…), ktÃ³ra zmienia bit docelowy jeÅ›li kontrolny bit jest w stanie \\(\\ket{1}\\).\nW ogÃ³lnoÅ›ci taka bramka moÅ¼e zostac zapisana jako: \\[\n\\textbf{CU}= \\ket{0}\\bra{0} \\otimes \\textbf{I} + \\ket{1}\\bra{1} \\otimes \\textbf{\\textbf{U}}\n\\]\nDowolna bramka dziaÅ‚ajaca na 1 kubit moÅ¼e byc przedstawiona jako mecierz \\[\n\\textbf{U} = \\begin{bmatrix} u_{00} \\, u_{01} \\\\ u_{10}\\, u_{11} \\end{bmatrix}\n\\]\ndlatego:\n\\[\n\\textbf{CU}=  \\begin{bmatrix} 1 \\,\\, \\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\\\\n0\\,\\, \\,\\,\\, 1 \\,\\,\\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\\\\n0\\,\\,\\,\\, 0\\,\\,\\,  u_{00} \\,\\, u_{01} \\\\ 0\\,\\,\\,\\, 0\\,\\,\\, u_{10}\\, \\, u_{11} \\end{bmatrix}\n\\]\nSzczegÃ³Å‚owe dziaÅ‚anie bramki moÅ¼na zapisac jako:\n\\[\\begin{align*}\n\\textbf{CU} \\ket{0} \\otimes \\ket{0} &=&  \\ket{0} \\otimes \\ket{0} \\\\\n\\textbf{CU} \\ket{0} \\otimes \\ket{1} &=& \\ket{0}\\otimes \\ket{1} \\\\\n\\textbf{CU} \\ket{1}\\otimes \\ket{0} &=& \\ket{1}\\otimes \\textbf{U} \\ket{0} \\\\\n\\textbf{CU} \\ket{1}\\otimes \\ket{1} &=& \\ket{1}\\otimes \\textbf{U} \\ket{1} \\\\\n\\end{align*}\\]\nDla kwantowej bramki NOT \\(\\textbf{U}= X\\) \\[\n\\text{CNOT} = \\begin{bmatrix} 1 \\,\\, \\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\\\\n0\\,\\, \\,\\,\\, 1 \\,\\,\\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\\\\n0\\,\\,\\,\\,\\, 0\\,\\,\\,\\,\\,  0 \\,\\,\\,\\,\\, 1 \\\\ 0\\,\\,\\,\\,\\, 0\\,\\,\\,\\,\\, 1\\,\\,\\,\\,\\, 0 \\end{bmatrix}\n\\] Bramka ta do drugiego kubitu (targetu) stosuje bramkÄ™ X jeÅ›li pierwszy kubit jest w pozycji \\(\\ket{1}\\). W przeciwnym wypadku nie zmienia siÄ™ nic.\n\\[\\begin{align*}\n\\textbf{CNOT} \\ket{0} \\otimes \\ket{0} &=&  \\ket{0} \\otimes \\ket{0} \\\\\n\\textbf{CNOT} \\ket{0} \\otimes \\ket{1} &=& \\ket{0}\\otimes \\ket{1} \\\\\n\\textbf{CNOT} \\ket{1}\\otimes \\ket{0} &=& \\ket{1}\\otimes \\ket{1} \\\\\n\\textbf{CNOT} \\ket{1}\\otimes \\ket{1} &=& \\ket{1}\\otimes \\ket{0} \\\\\n\\end{align*}\\]\n\nRozpoczynajac od stanu \\(\\ket{0} \\otimes \\ket{0}\\) zadziaÅ‚aj na pierwszy kubit bramka Hadamarda a na tak otrzymany stan zadziaÅ‚aj CNOT. Jaki stan uzyskujemy?",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad4.html#wartoÅ›Ä‡-oczekiwana-operatora-z-w-obwodzie-kwantowym",
    "href": "lectures/wyklad4.html#wartoÅ›Ä‡-oczekiwana-operatora-z-w-obwodzie-kwantowym",
    "title": "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych",
    "section": "WartoÅ›Ä‡ oczekiwana operatora Z w obwodzie kwantowym",
    "text": "WartoÅ›Ä‡ oczekiwana operatora Z w obwodzie kwantowym\nBramka (operator) Z w bazie obliczeniowej\nTen operator mierzy rÃ³Å¼nicÄ™ miÄ™dzy prawdopodobieÅ„stwem, Å¼e kubit znajduje siÄ™ w stanie , a prawdopodobieÅ„stwem, Å¼e znajduje siÄ™ w stanie .\nOgÃ³lnie, wartoÅ›Ä‡ oczekiwana (czyli Å›redni wynik pomiaru w bazie Z) jest dana wzorem: \\[\n\\langle \\textbf{Z} \\rangle = \\bra{\\psi} \\textbf{Z} \\ket{\\psi}\n\\]\nNiech: \\[\n\\ket{\\psi} = \\alpha\\ket{0} + \\beta\\ket{1}\n\\] wtedy: \\[\n\\bra{\\psi} = \\alpha^\\bra{0} + \\beta^\\bra{1}\n\\]\nMoÅ¼emy obliczyÄ‡: \\[\n\\bra{\\psi} \\textbf{Z} \\ket{\\psi} = (\\alpha^\\bra{0} + \\beta^\\bra{1}) , \\textbf{Z} , (\\alpha\\ket{0} + \\beta\\ket{1}) = |\\alpha|^2 - |\\beta|^2\n\\]\nWyraÅ¼enie \\(|\\alpha|^2 - |\\beta|^2\\) moÅ¼na interpretowaÄ‡ jako \\(Prob(0) - Prob(1)\\)\nZatem dla kubitu w stanie : \\[\n\\langle \\textbf{Z} \\rangle = 1\n\\]\nDla kubitu w stanie : \\[\n\\langle \\textbf{Z} \\rangle = -1\n\\]\nA dla kubitu w stanie superpozycji \\(\\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\): \\[\n\\langle \\textbf{Z} \\rangle = 0\n\\]",
    "crumbs": [
      "Sylabus",
      "WykÅ‚ady",
      "Kwantowe bramki logiczne w prostych algorytmach i obwodach kwantowych"
    ]
  }
]